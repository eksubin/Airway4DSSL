{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2414\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.2.2+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 5b248f6a0dd29cb9c2a9545f980a88de16a6b753\n",
      "MONAI __file__: /home/<username>/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.22.0\n",
      "scipy version: 1.13.0\n",
      "Pillow version: 10.3.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.66.2\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.8\n",
      "pandas version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "einops version: 0.7.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    ToTensord,\n",
    ")\n",
    "\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNETR\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.normpath(\"./dataset_0_init_6.json\")\n",
    "data_dir = os.path.normpath(\"./BTCV\")\n",
    "logdir = os.path.normpath(\"./logs/fine/\")\n",
    "\n",
    "if os.path.exists(logdir) is False:\n",
    "    os.mkdir(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained = True\n",
    "pretrained_path = os.path.normpath(\"./logs/best_model_32.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "# Training Hyper-params\n",
    "lr = 1e-4\n",
    "max_iterations = 30000\n",
    "eval_num = 100\n",
    "\n",
    "# Transforms\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-175,\n",
    "            a_max=250,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        # RandCropByPosNegLabeld(\n",
    "        #     keys=[\"image\", \"label\"],\n",
    "        #     label_key=\"label\",\n",
    "        #     spatial_size=(32, 32, 32),\n",
    "        #     pos=1,\n",
    "        #     neg=1,\n",
    "        #     num_samples=4,\n",
    "        #     image_key=\"image\",\n",
    "        #     image_threshold=0,\n",
    "        # ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[2],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            prob=0.10,\n",
    "            max_k=3,\n",
    "        ),\n",
    "        RandShiftIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            offsets=0.10,\n",
    "            prob=0.50,\n",
    "        ),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=-175,\n",
    "                             a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 24/24 [00:02<00:00, 11.28it/s]\n",
      "Loading dataset: 100%|██████████| 6/6 [00:00<00:00, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([1, 241, 241, 57]), label shape: torch.Size([1, 241, 241, 57])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datalist = load_decathlon_datalist(\n",
    "    base_dir=data_dir, data_list_file_path=json_path, is_segmentation=True, data_list_key=\"training\"\n",
    ")\n",
    "\n",
    "val_files = load_decathlon_datalist(\n",
    "    base_dir=data_dir, data_list_file_path=json_path, is_segmentation=True, data_list_key=\"validation\"\n",
    ")\n",
    "\n",
    "train_ds = CacheDataset(\n",
    "    data=datalist,\n",
    "    transform=train_transforms,\n",
    "    cache_num=24,\n",
    "    cache_rate=1.0,\n",
    "    num_workers=2,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=1,\n",
    "                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms,\n",
    "                      cache_num=6, cache_rate=1.0, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=1,\n",
    "                        shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Sanity check for shapes from data loaders\n",
    "case_num = 0\n",
    "img = val_ds[case_num][\"image\"]\n",
    "label = val_ds[case_num][\"label\"]\n",
    "img_shape = img.shape\n",
    "label_shape = label.shape\n",
    "print(f\"image shape: {img_shape}, label shape: {label_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([1, 241, 241, 57]), label shape: torch.Size([1, 241, 241, 57])\n",
      "image shape: torch.Size([1, 306, 306, 57]), label shape: torch.Size([1, 306, 306, 57])\n",
      "image shape: torch.Size([1, 253, 253, 45]), label shape: torch.Size([1, 253, 253, 45])\n",
      "image shape: torch.Size([1, 300, 300, 45]), label shape: torch.Size([1, 300, 300, 45])\n",
      "image shape: torch.Size([1, 227, 198, 31]), label shape: torch.Size([1, 227, 198, 31])\n",
      "image shape: torch.Size([1, 240, 202, 31]), label shape: torch.Size([1, 240, 202, 31])\n",
      "image shape: torch.Size([1, 213, 213, 37]), label shape: torch.Size([1, 213, 213, 37])\n",
      "image shape: torch.Size([1, 213, 213, 37]), label shape: torch.Size([1, 213, 213, 37])\n",
      "image shape: torch.Size([1, 266, 266, 28]), label shape: torch.Size([1, 266, 266, 28])\n",
      "image shape: torch.Size([1, 267, 267, 46]), label shape: torch.Size([1, 267, 267, 46])\n",
      "image shape: torch.Size([1, 226, 184, 31]), label shape: torch.Size([1, 226, 184, 31])\n",
      "image shape: torch.Size([1, 241, 241, 49]), label shape: torch.Size([1, 241, 241, 49])\n",
      "image shape: torch.Size([1, 306, 306, 49]), label shape: torch.Size([1, 306, 306, 49])\n",
      "image shape: torch.Size([1, 233, 233, 28]), label shape: torch.Size([1, 233, 233, 28])\n",
      "image shape: torch.Size([1, 241, 241, 65]), label shape: torch.Size([1, 241, 241, 65])\n",
      "image shape: torch.Size([1, 233, 233, 45]), label shape: torch.Size([1, 233, 233, 45])\n",
      "image shape: torch.Size([1, 200, 162, 40]), label shape: torch.Size([1, 200, 162, 40])\n"
     ]
    }
   ],
   "source": [
    "for case_num in range(len(val_ds)):\n",
    "    img = val_ds[case_num][\"image\"]\n",
    "    label = val_ds[case_num][\"label\"]\n",
    "    img_shape = img.shape\n",
    "    label_shape = label.shape\n",
    "    print(f\"image shape: {img_shape}, label shape: {label_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([1, 253, 245, 22]), label shape: torch.Size([1, 253, 245, 22])\n"
     ]
    }
   ],
   "source": [
    "case_num = 60\n",
    "img = train_ds[case_num][\"image\"]\n",
    "label = train_ds[case_num][\"label\"]\n",
    "img_shape = img.shape\n",
    "label_shape = label.shape\n",
    "print(f\"image shape: {img_shape}, label shape: {label_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Weights from the Path logs/best_model_32.pt\n",
      "Pretrained Weights Succesfully Loaded !\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\") # current GPU cannot handle this\n",
    "\n",
    "model = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=14,\n",
    "    img_size=(32, 32, 32),\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_heads=12,\n",
    "    pos_embed=\"conv\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "\n",
    "# Load ViT backbone weights into UNETR\n",
    "if use_pretrained is True:\n",
    "    print(\"Loading Weights from the Path {}\".format(pretrained_path))\n",
    "    vit_dict = torch.load(pretrained_path)\n",
    "    vit_weights = vit_dict[\"state_dict\"]\n",
    "\n",
    "    # Remove items of vit_weights if they are not in the ViT backbone (this is used in UNETR).\n",
    "    # For example, some variables names like conv3d_transpose.weight, conv3d_transpose.bias,\n",
    "    # conv3d_transpose_1.weight and conv3d_transpose_1.bias are used to match dimensions\n",
    "    # while pretraining with ViTAutoEnc and are not a part of ViT backbone.\n",
    "    model_dict = model.vit.state_dict()\n",
    "\n",
    "    vit_weights = {k: v for k, v in vit_weights.items() if k in model_dict}\n",
    "    model_dict.update(vit_weights)\n",
    "    model.vit.load_state_dict(model_dict)\n",
    "    del model_dict, vit_weights, vit_dict\n",
    "    print(\"Pretrained Weights Succesfully Loaded !\")\n",
    "\n",
    "elif use_pretrained is False:\n",
    "    print(\"No weights were loaded, all weights being used are randomly initialized!\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "post_label = AsDiscrete(to_onehot=14)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=14)\n",
    "dice_metric = DiceMetric(include_background=True,\n",
    "                         reduction=\"mean\", get_not_nans=False)\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (8 / 30000 Steps) (loss=3.65203):  14%|█▎        | 9/66 [00:09<01:01,  1.08s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/croppad/dictionary.py\", line 997, in __call__\n    self.randomize(d.get(self.label_key), fg_indices, bg_indices, d.get(self.image_key))\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/croppad/dictionary.py\", line 979, in randomize\n    self.cropper.randomize(label=label, fg_indices=fg_indices, bg_indices=bg_indices, image=image)\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/croppad/array.py\", line 1158, in randomize\n    self.centers = generate_pos_neg_label_crop_centers(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/utils.py\", line 617, in generate_pos_neg_label_crop_centers\n    centers.append(correct_crop_centers(center, spatial_size, label_spatial_shape, allow_smaller))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/utils.py\", line 541, in correct_crop_centers\n    raise ValueError(\nValueError: The size of the proposed random crop ROI is larger than the image size, got ROI size (32, 32, 32) and label image size (253, 245, 22) respectively.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/compose.py\", line 335, in __call__\n    result = execute_compose(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/compose.py\", line 111, in execute_compose\n    data = apply_transform(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.croppad.dictionary.RandCropByPosNegLabeld object at 0x7fad3198f5d0>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/data/dataset.py\", line 112, in __getitem__\n    return self._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/data/dataset.py\", line 916, in _transform\n    return super()._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/data/dataset.py\", line 98, in _transform\n    return apply_transform(self.transform, data_i) if self.transform is not None else data_i\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.compose.Compose object at 0x7fac89c0e090>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 95\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m global_step, dice_val_best, global_step_best\n\u001b[1;32m     94\u001b[0m \u001b[39mwhile\u001b[39;00m global_step \u001b[39m<\u001b[39m max_iterations:\n\u001b[0;32m---> 95\u001b[0m     global_step, dice_val_best, global_step_best \u001b[39m=\u001b[39m train(\n\u001b[1;32m     96\u001b[0m         global_step, train_loader, dice_val_best, global_step_best)\n\u001b[1;32m     97\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\n\u001b[1;32m     98\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(logdir, \u001b[39m\"\u001b[39m\u001b[39mbest_metric_model.pth\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[1;32m    100\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain completed, best_metric: \u001b[39m\u001b[39m{\u001b[39;00mdice_val_best\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mat iteration: \u001b[39m\u001b[39m{\u001b[39;00mglobal_step_best\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(global_step, train_loader, dice_val_best, global_step_best)\u001b[0m\n\u001b[1;32m     32\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     33\u001b[0m epoch_iterator \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m     34\u001b[0m     train_loader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining (X / X Steps) (loss=X.X)\u001b[39m\u001b[39m\"\u001b[39m, dynamic_ncols\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfor\u001b[39;49;00m step, batch \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(epoch_iterator):\n\u001b[1;32m     36\u001b[0m     step \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m\n\u001b[1;32m     37\u001b[0m     x, y \u001b[39m=\u001b[39;49m (batch[\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m], batch[\u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/virtenvs/SSLUnet/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[39mfor\u001b[39;49;00m obj \u001b[39min\u001b[39;49;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[39myield\u001b[39;49;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/croppad/dictionary.py\", line 997, in __call__\n    self.randomize(d.get(self.label_key), fg_indices, bg_indices, d.get(self.image_key))\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/croppad/dictionary.py\", line 979, in randomize\n    self.cropper.randomize(label=label, fg_indices=fg_indices, bg_indices=bg_indices, image=image)\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/croppad/array.py\", line 1158, in randomize\n    self.centers = generate_pos_neg_label_crop_centers(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/utils.py\", line 617, in generate_pos_neg_label_crop_centers\n    centers.append(correct_crop_centers(center, spatial_size, label_spatial_shape, allow_smaller))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/utils.py\", line 541, in correct_crop_centers\n    raise ValueError(\nValueError: The size of the proposed random crop ROI is larger than the image size, got ROI size (32, 32, 32) and label image size (253, 245, 22) respectively.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 141, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/compose.py\", line 335, in __call__\n    result = execute_compose(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/compose.py\", line 111, in execute_compose\n    data = apply_transform(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.croppad.dictionary.RandCropByPosNegLabeld object at 0x7fad3198f5d0>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/data/dataset.py\", line 112, in __getitem__\n    return self._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/data/dataset.py\", line 916, in _transform\n    return super()._transform(index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/data/dataset.py\", line 98, in _transform\n    return apply_transform(self.transform, data_i) if self.transform is not None else data_i\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/transforms/transform.py\", line 171, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.compose.Compose object at 0x7fac89c0e090>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def validation(epoch_iterator_val):\n",
    "    model.eval()\n",
    "    dice_vals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _step, batch in enumerate(epoch_iterator_val):\n",
    "            val_inputs, val_labels = (\n",
    "                batch[\"image\"], batch[\"label\"])\n",
    "            val_outputs = sliding_window_inference(\n",
    "                val_inputs, (32, 32, 32), 4, model)\n",
    "            val_labels_list = decollate_batch(val_labels)\n",
    "            val_labels_convert = [post_label(\n",
    "                val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "            val_outputs_list = decollate_batch(val_outputs)\n",
    "            val_output_convert = [post_pred(val_pred_tensor)\n",
    "                                  for val_pred_tensor in val_outputs_list]\n",
    "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "            dice = dice_metric.aggregate().item()\n",
    "            dice_vals.append(dice)\n",
    "            epoch_iterator_val.set_description(\n",
    "                \"Validate (%d / %d Steps) (dice=%2.5f)\" % (global_step, 10.0, dice))\n",
    "\n",
    "        dice_metric.reset()\n",
    "\n",
    "    mean_dice_val = np.mean(dice_vals)\n",
    "    return mean_dice_val\n",
    "\n",
    "\n",
    "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    epoch_iterator = tqdm(\n",
    "        train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        step += 1\n",
    "        x, y = (batch[\"image\"], batch[\"label\"])\n",
    "        logit_map = model(x)\n",
    "        loss = loss_function(logit_map, y)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_iterator.set_description(\n",
    "            \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss))\n",
    "\n",
    "        if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
    "            epoch_iterator_val = tqdm(\n",
    "                val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
    "            dice_val = validation(epoch_iterator_val)\n",
    "\n",
    "            epoch_loss /= step\n",
    "            epoch_loss_values.append(epoch_loss)\n",
    "            metric_values.append(dice_val)\n",
    "            if dice_val > dice_val_best:\n",
    "                dice_val_best = dice_val\n",
    "                global_step_best = global_step\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    logdir, \"best_metric_model.pth\"))\n",
    "                print(\n",
    "                    \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val)\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            plt.figure(1, (12, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(\"Iteration Average Loss\")\n",
    "            x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
    "            y = epoch_loss_values\n",
    "            plt.xlabel(\"Iteration\")\n",
    "            plt.plot(x, y)\n",
    "            plt.grid()\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(\"Val Mean Dice\")\n",
    "            x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
    "            y = metric_values\n",
    "            plt.xlabel(\"Iteration\")\n",
    "            plt.plot(x, y)\n",
    "            plt.grid()\n",
    "            plt.savefig(os.path.join(logdir, \"btcv_finetune_quick_update.png\"))\n",
    "            plt.clf()\n",
    "            plt.close(1)\n",
    "\n",
    "        global_step += 1\n",
    "    return global_step, dice_val_best, global_step_best\n",
    "\n",
    "\n",
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best = train(\n",
    "        global_step, train_loader, dice_val_best, global_step_best)\n",
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(logdir, \"best_metric_model.pth\")))\n",
    "\n",
    "print(\n",
    "    f\"train completed, best_metric: {dice_val_best:.4f} \" f\"at iteration: {global_step_best}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSLUnet",
   "language": "python",
   "name": "sslunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
