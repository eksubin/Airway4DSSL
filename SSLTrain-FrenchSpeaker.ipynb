{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2414\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.2.2+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 5b248f6a0dd29cb9c2a9545f980a88de16a6b753\n",
      "MONAI __file__: /home/<username>/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.22.0\n",
      "scipy version: 1.13.0\n",
      "Pillow version: 10.3.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.66.2\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.8\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.7.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.14.3\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import L1Loss\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.networks.nets import ViTAutoEnc\n",
    "from monai.losses import ContrastiveLoss\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    CopyItemsd,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Spacingd,\n",
    "    OneOf,\n",
    "    ScaleIntensityRanged,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCoarseDropoutd,\n",
    "    RandCoarseShuffled,\n",
    "    ScaleIntensityd\n",
    ")\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': './Data/FrenchSpeakerDataset/NRRD_Files_N4Bias/P10_2.nrrd'} {'image': './Data/FrenchSpeakerDataset/NRRD_Files_N4Bias_Val/P1_2.nrrd'}\n"
     ]
    }
   ],
   "source": [
    "logdir_path = os.path.normpath(\"./logs/\")\n",
    "\n",
    "#Convert the train and validation images into a list with locations\n",
    "train_dir = \"./Data/FrenchSpeakerDataset/NRRD_Files_N4Bias/\"\n",
    "val_dir = \"./Data/FrenchSpeakerDataset/NRRD_Files_N4Bias_Val/\"\n",
    "\n",
    "#train image file\n",
    "timage_filenames = sorted([os.path.join(train_dir, f) for f in os.listdir(train_dir) if f.endswith(\".nrrd\")])\n",
    "\n",
    "#validation image files\n",
    "vimage_filenames = sorted([os.path.join(val_dir, f)\n",
    "                          for f in os.listdir(val_dir) if f.endswith(\".nrrd\")])\n",
    "\n",
    "# Create a list of dictionaries containing the file paths\n",
    "train_datalist = [{\"image\": img} for img in timage_filenames]\n",
    "validation_datalist = [{\"image\": img} for img in vimage_filenames]\n",
    "\n",
    "# Print the datalist to verify\n",
    "print(train_datalist[0], validation_datalist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([64, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erattakulangara/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "# Define Training Transforms\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        # Defines the image intensity\n",
    "        ScaleIntensityd(keys=[\"image\"], minv=-1.0, maxv=1.0),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "        SpatialPadd(keys=[\"image\"], spatial_size=(64, 64, 64)),\n",
    "        RandSpatialCropSamplesd(keys=[\"image\"], roi_size=(\n",
    "            64, 64, 64), random_size=False, num_samples=2),\n",
    "        CopyItemsd(keys=[\"image\"], times=2, names=[\n",
    "            \"gt_image\", \"image_2\"], allow_missing_keys=False),\n",
    "        OneOf(\n",
    "            transforms=[\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=6, spatial_size=5, dropout_holes=True, max_spatial_size=32\n",
    "                ),\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=6, spatial_size=20, dropout_holes=False, max_spatial_size=64\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        RandCoarseShuffled(keys=[\"image\"], prob=0.8, holes=10, spatial_size=8),\n",
    "        # Please note that that if image, image_2 are called via the same transform call because of the determinism\n",
    "        # they will get augmented the exact same way which is not the required case here, hence two calls are made\n",
    "        OneOf(\n",
    "            transforms=[\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image_2\"], prob=1.0, holes=6, spatial_size=5, dropout_holes=True, max_spatial_size=32\n",
    "                ),\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image_2\"], prob=1.0, holes=6, spatial_size=20, dropout_holes=False, max_spatial_size=64\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        RandCoarseShuffled(keys=[\"image_2\"], prob=0.8,\n",
    "                           holes=10, spatial_size=8),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "check_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image = check_data[\"image\"][0][0]\n",
    "print(f\"image shape: {image.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config\n",
    "\n",
    "# Define Network ViT backbone & Loss & Optimizer\n",
    "device = torch.device(\"cpu\")\n",
    "model = ViTAutoEnc(\n",
    "    in_channels=1,\n",
    "    img_size=(64, 64, 64),\n",
    "    patch_size=(16, 16, 16),\n",
    "    proj_type=\"conv\",\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Hyper-paramters for training loop\n",
    "max_epochs = 500\n",
    "val_interval = 2\n",
    "batch_size = 4\n",
    "lr = 1e-4\n",
    "epoch_loss_values = []\n",
    "step_loss_values = []\n",
    "epoch_cl_loss_values = []\n",
    "epoch_recon_loss_values = []\n",
    "val_loss_values = []\n",
    "best_val_loss = 1000.0\n",
    "\n",
    "recon_loss = L1Loss()\n",
    "contrastive_loss = ContrastiveLoss(temperature=0.05)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Define DataLoader using MONAI, CacheDataset needs to be used\n",
    "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = Dataset(data=validation_datalist, transform=train_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/500\n",
      "1/174, train_loss: 0.5378, time taken: 8.541421890258789s\n",
      "2/174, train_loss: 0.5149, time taken: 1.9713759422302246s\n",
      "3/174, train_loss: 0.4896, time taken: 1.903376579284668s\n",
      "4/174, train_loss: 0.5197, time taken: 1.8070695400238037s\n",
      "5/174, train_loss: 0.5198, time taken: 1.806295394897461s\n",
      "6/174, train_loss: 0.5107, time taken: 1.810910940170288s\n",
      "7/174, train_loss: 0.5201, time taken: 1.7745130062103271s\n",
      "8/174, train_loss: 0.5002, time taken: 1.7982418537139893s\n",
      "9/174, train_loss: 0.5044, time taken: 1.7966783046722412s\n",
      "10/174, train_loss: 0.4567, time taken: 1.8051841259002686s\n",
      "11/174, train_loss: 0.4483, time taken: 1.790161371231079s\n",
      "12/174, train_loss: 0.4449, time taken: 1.798135757446289s\n",
      "13/174, train_loss: 0.5139, time taken: 1.785261631011963s\n",
      "14/174, train_loss: 0.4508, time taken: 1.8094470500946045s\n",
      "15/174, train_loss: 0.4957, time taken: 1.875885248184204s\n",
      "16/174, train_loss: 0.5529, time taken: 1.7854502201080322s\n",
      "17/174, train_loss: 0.5137, time taken: 1.7986350059509277s\n",
      "18/174, train_loss: 0.4324, time taken: 1.8022639751434326s\n",
      "19/174, train_loss: 0.4456, time taken: 1.799088954925537s\n",
      "20/174, train_loss: 0.4325, time taken: 1.7763700485229492s\n",
      "21/174, train_loss: 0.5020, time taken: 3.9257917404174805s\n",
      "22/174, train_loss: 0.4838, time taken: 1.780585527420044s\n",
      "23/174, train_loss: 0.4903, time taken: 1.8858377933502197s\n",
      "24/174, train_loss: 0.4257, time taken: 1.8149588108062744s\n",
      "25/174, train_loss: 0.4288, time taken: 1.7065935134887695s\n",
      "26/174, train_loss: 0.4434, time taken: 1.732994794845581s\n",
      "27/174, train_loss: 0.4595, time taken: 2.1021697521209717s\n",
      "28/174, train_loss: 0.4492, time taken: 1.7802166938781738s\n",
      "29/174, train_loss: 0.4875, time taken: 1.7874362468719482s\n",
      "30/174, train_loss: 0.4759, time taken: 2.078209161758423s\n",
      "31/174, train_loss: 0.4597, time taken: 1.7348005771636963s\n",
      "32/174, train_loss: 0.4686, time taken: 1.7348623275756836s\n",
      "33/174, train_loss: 0.4355, time taken: 1.8624277114868164s\n",
      "34/174, train_loss: 0.4876, time taken: 1.9265599250793457s\n",
      "35/174, train_loss: 0.4862, time taken: 1.7889797687530518s\n",
      "36/174, train_loss: 0.4469, time taken: 2.211833953857422s\n",
      "37/174, train_loss: 0.4563, time taken: 1.7743709087371826s\n",
      "38/174, train_loss: 0.4160, time taken: 1.9092621803283691s\n",
      "39/174, train_loss: 0.4114, time taken: 1.7968263626098633s\n",
      "40/174, train_loss: 0.4570, time taken: 1.8478281497955322s\n",
      "41/174, train_loss: 0.3752, time taken: 1.7347915172576904s\n",
      "42/174, train_loss: 0.3882, time taken: 1.8602180480957031s\n",
      "43/174, train_loss: 0.3836, time taken: 1.8851618766784668s\n",
      "44/174, train_loss: 0.3765, time taken: 1.743520975112915s\n",
      "45/174, train_loss: 0.4529, time taken: 1.7293152809143066s\n",
      "46/174, train_loss: 0.3812, time taken: 1.790546178817749s\n",
      "47/174, train_loss: 0.4342, time taken: 1.90262770652771s\n",
      "48/174, train_loss: 0.3774, time taken: 1.863356590270996s\n",
      "49/174, train_loss: 0.4292, time taken: 1.8035974502563477s\n",
      "50/174, train_loss: 0.4077, time taken: 1.7865784168243408s\n",
      "51/174, train_loss: 0.4276, time taken: 1.8015522956848145s\n",
      "52/174, train_loss: 0.4422, time taken: 1.792938470840454s\n",
      "53/174, train_loss: 0.3864, time taken: 1.7904787063598633s\n",
      "54/174, train_loss: 0.4429, time taken: 1.8125293254852295s\n",
      "55/174, train_loss: 0.4376, time taken: 2.178433895111084s\n",
      "56/174, train_loss: 0.4057, time taken: 1.8838589191436768s\n",
      "57/174, train_loss: 0.4454, time taken: 1.8199191093444824s\n",
      "58/174, train_loss: 0.3795, time taken: 1.885331630706787s\n",
      "59/174, train_loss: 0.4259, time taken: 1.809126615524292s\n",
      "60/174, train_loss: 0.4061, time taken: 1.8647971153259277s\n",
      "61/174, train_loss: 0.4262, time taken: 1.800032138824463s\n",
      "62/174, train_loss: 0.3974, time taken: 1.8009235858917236s\n",
      "63/174, train_loss: 0.4018, time taken: 1.7956068515777588s\n",
      "64/174, train_loss: 0.4239, time taken: 1.7842895984649658s\n",
      "65/174, train_loss: 0.3872, time taken: 1.8038511276245117s\n",
      "66/174, train_loss: 0.4040, time taken: 1.8006348609924316s\n",
      "67/174, train_loss: 0.3820, time taken: 1.8063585758209229s\n",
      "68/174, train_loss: 0.3510, time taken: 1.7679295539855957s\n",
      "69/174, train_loss: 0.3361, time taken: 1.7971584796905518s\n",
      "70/174, train_loss: 0.3956, time taken: 2.374873399734497s\n",
      "71/174, train_loss: 0.4008, time taken: 1.9329488277435303s\n",
      "72/174, train_loss: 0.3768, time taken: 2.19358491897583s\n",
      "73/174, train_loss: 0.3591, time taken: 2.159085988998413s\n",
      "74/174, train_loss: 0.3952, time taken: 1.8237905502319336s\n",
      "75/174, train_loss: 0.3513, time taken: 2.077894926071167s\n",
      "76/174, train_loss: 0.4019, time taken: 1.888150691986084s\n",
      "77/174, train_loss: 0.3854, time taken: 1.8215792179107666s\n",
      "78/174, train_loss: 0.3496, time taken: 1.8708112239837646s\n",
      "79/174, train_loss: 0.3491, time taken: 2.0262773036956787s\n",
      "80/174, train_loss: 0.4143, time taken: 1.8868870735168457s\n",
      "81/174, train_loss: 0.3755, time taken: 2.1890735626220703s\n",
      "82/174, train_loss: 0.4023, time taken: 1.8952112197875977s\n",
      "83/174, train_loss: 0.3341, time taken: 2.192023277282715s\n",
      "84/174, train_loss: 0.4387, time taken: 2.0066869258880615s\n",
      "85/174, train_loss: 0.4109, time taken: 1.859116792678833s\n",
      "86/174, train_loss: 0.3532, time taken: 1.8143384456634521s\n",
      "87/174, train_loss: 0.4054, time taken: 1.7892487049102783s\n",
      "88/174, train_loss: 0.3739, time taken: 1.7924950122833252s\n",
      "89/174, train_loss: 0.3774, time taken: 1.7868001461029053s\n",
      "90/174, train_loss: 0.3301, time taken: 1.7206215858459473s\n",
      "91/174, train_loss: 0.4607, time taken: 1.7934072017669678s\n",
      "92/174, train_loss: 0.4963, time taken: 1.771813154220581s\n",
      "93/174, train_loss: 0.3687, time taken: 1.7190499305725098s\n",
      "94/174, train_loss: 0.3973, time taken: 1.7698554992675781s\n",
      "95/174, train_loss: 0.3641, time taken: 1.789684534072876s\n",
      "96/174, train_loss: 0.3577, time taken: 1.7137253284454346s\n",
      "97/174, train_loss: 0.3155, time taken: 1.7076473236083984s\n",
      "98/174, train_loss: 0.3480, time taken: 1.7650737762451172s\n",
      "99/174, train_loss: 0.3813, time taken: 1.7901382446289062s\n",
      "100/174, train_loss: 0.3487, time taken: 1.7375423908233643s\n",
      "101/174, train_loss: 0.3320, time taken: 1.7806956768035889s\n",
      "102/174, train_loss: 0.3577, time taken: 2.479478597640991s\n",
      "103/174, train_loss: 0.3803, time taken: 1.802537202835083s\n",
      "104/174, train_loss: 0.4020, time taken: 1.7788496017456055s\n",
      "105/174, train_loss: 0.3338, time taken: 1.7433576583862305s\n",
      "106/174, train_loss: 0.3362, time taken: 1.7179722785949707s\n",
      "107/174, train_loss: 0.3645, time taken: 1.8152124881744385s\n",
      "108/174, train_loss: 0.3999, time taken: 1.8668856620788574s\n",
      "109/174, train_loss: 0.3679, time taken: 1.7808566093444824s\n",
      "110/174, train_loss: 0.3469, time taken: 1.7113909721374512s\n",
      "111/174, train_loss: 0.3110, time taken: 1.7909061908721924s\n",
      "112/174, train_loss: 0.2914, time taken: 1.7816479206085205s\n",
      "113/174, train_loss: 0.3373, time taken: 1.7393810749053955s\n",
      "114/174, train_loss: 0.3701, time taken: 1.793776273727417s\n",
      "115/174, train_loss: 0.4269, time taken: 1.7808513641357422s\n",
      "116/174, train_loss: 0.3839, time taken: 1.8083558082580566s\n",
      "117/174, train_loss: 0.3645, time taken: 1.7977194786071777s\n",
      "118/174, train_loss: 0.3455, time taken: 1.7914910316467285s\n",
      "119/174, train_loss: 0.3105, time taken: 1.723323106765747s\n",
      "120/174, train_loss: 0.3718, time taken: 1.8052923679351807s\n",
      "121/174, train_loss: 0.3353, time taken: 1.7989897727966309s\n",
      "122/174, train_loss: 0.4065, time taken: 1.7140443325042725s\n",
      "123/174, train_loss: 0.3317, time taken: 1.81058931350708s\n",
      "124/174, train_loss: 0.3935, time taken: 1.7937281131744385s\n",
      "125/174, train_loss: 0.4120, time taken: 1.7790346145629883s\n",
      "126/174, train_loss: 0.3450, time taken: 1.784925937652588s\n",
      "127/174, train_loss: 0.3850, time taken: 1.7978541851043701s\n",
      "128/174, train_loss: 0.3853, time taken: 1.7208731174468994s\n",
      "129/174, train_loss: 0.3414, time taken: 1.8695275783538818s\n",
      "130/174, train_loss: 0.3579, time taken: 1.7887275218963623s\n",
      "131/174, train_loss: 0.3297, time taken: 1.7965595722198486s\n",
      "132/174, train_loss: 0.3345, time taken: 1.7199676036834717s\n",
      "133/174, train_loss: 0.3486, time taken: 1.791933298110962s\n",
      "134/174, train_loss: 0.3251, time taken: 3.2917563915252686s\n",
      "135/174, train_loss: 0.2973, time taken: 1.7769651412963867s\n",
      "136/174, train_loss: 0.3701, time taken: 1.7915515899658203s\n",
      "137/174, train_loss: 0.2878, time taken: 1.7192444801330566s\n",
      "138/174, train_loss: 0.3686, time taken: 2.1682064533233643s\n",
      "139/174, train_loss: 0.3543, time taken: 1.812699794769287s\n",
      "140/174, train_loss: 0.3248, time taken: 1.8137497901916504s\n",
      "141/174, train_loss: 0.3925, time taken: 1.9968183040618896s\n",
      "142/174, train_loss: 0.3914, time taken: 2.185112953186035s\n",
      "143/174, train_loss: 0.2964, time taken: 2.167698860168457s\n",
      "144/174, train_loss: 0.3232, time taken: 1.742088794708252s\n",
      "145/174, train_loss: 0.2733, time taken: 1.7200520038604736s\n",
      "146/174, train_loss: 0.4016, time taken: 1.8073925971984863s\n",
      "147/174, train_loss: 0.3609, time taken: 1.9926834106445312s\n",
      "148/174, train_loss: 0.3445, time taken: 1.8637771606445312s\n",
      "149/174, train_loss: 0.3450, time taken: 2.3101253509521484s\n",
      "150/174, train_loss: 0.2998, time taken: 1.971479892730713s\n",
      "151/174, train_loss: 0.3313, time taken: 1.8380882740020752s\n",
      "152/174, train_loss: 0.3424, time taken: 1.7997841835021973s\n",
      "153/174, train_loss: 0.3092, time taken: 1.8030078411102295s\n",
      "154/174, train_loss: 0.3291, time taken: 1.8341960906982422s\n",
      "155/174, train_loss: 0.3287, time taken: 1.920560598373413s\n",
      "156/174, train_loss: 0.3474, time taken: 1.8830020427703857s\n",
      "157/174, train_loss: 0.3712, time taken: 1.8296325206756592s\n",
      "158/174, train_loss: 0.3499, time taken: 1.7423958778381348s\n",
      "159/174, train_loss: 0.3168, time taken: 1.8559536933898926s\n",
      "160/174, train_loss: 0.3384, time taken: 1.818021297454834s\n",
      "161/174, train_loss: 0.2613, time taken: 1.8114838600158691s\n",
      "162/174, train_loss: 0.3277, time taken: 1.7409796714782715s\n",
      "163/174, train_loss: 0.3188, time taken: 1.731327772140503s\n",
      "164/174, train_loss: 0.3417, time taken: 1.805474042892456s\n",
      "165/174, train_loss: 0.2527, time taken: 1.7787814140319824s\n",
      "166/174, train_loss: 0.3165, time taken: 1.7931318283081055s\n",
      "167/174, train_loss: 0.2925, time taken: 1.784186601638794s\n",
      "168/174, train_loss: 0.3160, time taken: 1.8024120330810547s\n",
      "169/174, train_loss: 0.3315, time taken: 1.7758080959320068s\n",
      "170/174, train_loss: 0.3121, time taken: 1.72285795211792s\n",
      "171/174, train_loss: 0.3261, time taken: 1.7938060760498047s\n",
      "172/174, train_loss: 0.3219, time taken: 1.795057773590088s\n",
      "173/174, train_loss: 0.2718, time taken: 1.803102970123291s\n",
      "174/174, train_loss: 0.3793, time taken: 1.796825885772705s\n",
      "175/174, train_loss: 0.2343, time taken: 1.3966140747070312s\n",
      "epoch 1 average loss: 0.3890\n",
      "Entering Validation for epoch: 1\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 1 Validation avg loss: 0.0796, time taken: 1.2174248695373535s\n",
      "Saving new model based on validation loss 0.0796\n",
      "----------\n",
      "epoch 2/500\n",
      "1/174, train_loss: 0.3224, time taken: 8.297068119049072s\n",
      "2/174, train_loss: 0.2913, time taken: 1.9375503063201904s\n",
      "3/174, train_loss: 0.2669, time taken: 1.972902774810791s\n",
      "4/174, train_loss: 0.3194, time taken: 1.820244312286377s\n",
      "5/174, train_loss: 0.2884, time taken: 1.807673454284668s\n",
      "6/174, train_loss: 0.2829, time taken: 1.8145833015441895s\n",
      "7/174, train_loss: 0.3096, time taken: 1.8840429782867432s\n",
      "8/174, train_loss: 0.2514, time taken: 1.803642988204956s\n",
      "9/174, train_loss: 0.2822, time taken: 1.7815735340118408s\n",
      "10/174, train_loss: 0.3304, time taken: 1.817040205001831s\n",
      "11/174, train_loss: 0.2951, time taken: 1.7256815433502197s\n",
      "12/174, train_loss: 0.3189, time taken: 1.7928829193115234s\n",
      "13/174, train_loss: 0.3907, time taken: 1.7996418476104736s\n",
      "14/174, train_loss: 0.2904, time taken: 1.8064320087432861s\n",
      "15/174, train_loss: 0.3726, time taken: 1.775895595550537s\n",
      "16/174, train_loss: 0.3098, time taken: 1.813236951828003s\n",
      "17/174, train_loss: 0.2942, time taken: 1.7312572002410889s\n",
      "18/174, train_loss: 0.2987, time taken: 1.807690143585205s\n",
      "19/174, train_loss: 0.2858, time taken: 1.7453162670135498s\n",
      "20/174, train_loss: 0.2457, time taken: 1.7358310222625732s\n",
      "21/174, train_loss: 0.3285, time taken: 1.7734034061431885s\n",
      "22/174, train_loss: 0.3089, time taken: 1.8158588409423828s\n",
      "23/174, train_loss: 0.2987, time taken: 1.7986204624176025s\n",
      "24/174, train_loss: 0.2806, time taken: 1.7264540195465088s\n",
      "25/174, train_loss: 0.2965, time taken: 1.799058437347412s\n",
      "26/174, train_loss: 0.3892, time taken: 1.7930018901824951s\n",
      "27/174, train_loss: 0.3086, time taken: 1.8036916255950928s\n",
      "28/174, train_loss: 0.3018, time taken: 1.7804160118103027s\n",
      "29/174, train_loss: 0.2832, time taken: 1.7948198318481445s\n",
      "30/174, train_loss: 0.3527, time taken: 1.798816442489624s\n",
      "31/174, train_loss: 0.3049, time taken: 1.7924890518188477s\n",
      "32/174, train_loss: 0.2678, time taken: 1.8085088729858398s\n",
      "33/174, train_loss: 0.2718, time taken: 1.7791180610656738s\n",
      "34/174, train_loss: 0.3361, time taken: 1.7865486145019531s\n",
      "35/174, train_loss: 0.3059, time taken: 1.7864899635314941s\n",
      "36/174, train_loss: 0.3187, time taken: 1.7207183837890625s\n",
      "37/174, train_loss: 0.2664, time taken: 1.8032426834106445s\n",
      "38/174, train_loss: 0.3401, time taken: 1.7773070335388184s\n",
      "39/174, train_loss: 0.3342, time taken: 1.7968647480010986s\n",
      "40/174, train_loss: 0.2710, time taken: 1.8059170246124268s\n",
      "41/174, train_loss: 0.2701, time taken: 1.802492618560791s\n",
      "42/174, train_loss: 0.3505, time taken: 1.7666890621185303s\n",
      "43/174, train_loss: 0.3232, time taken: 1.802597999572754s\n",
      "44/174, train_loss: 0.3393, time taken: 1.783862829208374s\n",
      "45/174, train_loss: 0.3618, time taken: 1.8226690292358398s\n",
      "46/174, train_loss: 0.3053, time taken: 1.784012794494629s\n",
      "47/174, train_loss: 0.3169, time taken: 1.8037109375s\n",
      "48/174, train_loss: 0.2680, time taken: 1.8806188106536865s\n",
      "49/174, train_loss: 0.2768, time taken: 1.797654151916504s\n",
      "50/174, train_loss: 0.3716, time taken: 2.4139044284820557s\n",
      "51/174, train_loss: 0.3091, time taken: 1.8580877780914307s\n",
      "52/174, train_loss: 0.2453, time taken: 1.7403886318206787s\n",
      "53/174, train_loss: 0.2981, time taken: 3.2259044647216797s\n",
      "54/174, train_loss: 0.3200, time taken: 1.8762233257293701s\n",
      "55/174, train_loss: 0.3257, time taken: 1.7919812202453613s\n",
      "56/174, train_loss: 0.3498, time taken: 1.809812068939209s\n",
      "57/174, train_loss: 0.2868, time taken: 1.8069171905517578s\n",
      "58/174, train_loss: 0.2647, time taken: 1.9833977222442627s\n",
      "59/174, train_loss: 0.2824, time taken: 1.792041301727295s\n",
      "60/174, train_loss: 0.3031, time taken: 1.773761510848999s\n",
      "61/174, train_loss: 0.3080, time taken: 1.7302145957946777s\n",
      "62/174, train_loss: 0.3308, time taken: 1.8617298603057861s\n",
      "63/174, train_loss: 0.3893, time taken: 1.8316154479980469s\n",
      "64/174, train_loss: 0.3312, time taken: 1.789738655090332s\n",
      "65/174, train_loss: 0.2871, time taken: 1.7993800640106201s\n",
      "66/174, train_loss: 0.2806, time taken: 1.7937417030334473s\n",
      "67/174, train_loss: 0.2688, time taken: 1.8683691024780273s\n",
      "68/174, train_loss: 0.2944, time taken: 1.986030101776123s\n",
      "69/174, train_loss: 0.2405, time taken: 1.8945577144622803s\n",
      "70/174, train_loss: 0.2510, time taken: 2.0922412872314453s\n",
      "71/174, train_loss: 0.3483, time taken: 1.79140305519104s\n",
      "72/174, train_loss: 0.2271, time taken: 1.8249685764312744s\n",
      "73/174, train_loss: 0.2983, time taken: 1.8880486488342285s\n",
      "74/174, train_loss: 0.2891, time taken: 1.810150146484375s\n",
      "75/174, train_loss: 0.2981, time taken: 1.9738240242004395s\n",
      "76/174, train_loss: 0.3228, time taken: 1.8876736164093018s\n",
      "77/174, train_loss: 0.2937, time taken: 1.7963719367980957s\n",
      "78/174, train_loss: 0.2737, time taken: 1.9079182147979736s\n",
      "79/174, train_loss: 0.3024, time taken: 1.8037574291229248s\n",
      "80/174, train_loss: 0.3030, time taken: 1.9011754989624023s\n",
      "81/174, train_loss: 0.2747, time taken: 1.7785139083862305s\n",
      "82/174, train_loss: 0.2261, time taken: 1.777000904083252s\n",
      "83/174, train_loss: 0.2664, time taken: 1.7213592529296875s\n",
      "84/174, train_loss: 0.2930, time taken: 1.7896955013275146s\n",
      "85/174, train_loss: 0.3380, time taken: 1.7936010360717773s\n",
      "86/174, train_loss: 0.2746, time taken: 1.7858967781066895s\n",
      "87/174, train_loss: 0.3095, time taken: 1.7861638069152832s\n",
      "88/174, train_loss: 0.3894, time taken: 1.815260648727417s\n",
      "89/174, train_loss: 0.2936, time taken: 1.8762304782867432s\n",
      "90/174, train_loss: 0.3032, time taken: 1.8052144050598145s\n",
      "91/174, train_loss: 0.2818, time taken: 1.7861037254333496s\n",
      "92/174, train_loss: 0.3342, time taken: 1.890406608581543s\n",
      "93/174, train_loss: 0.2536, time taken: 1.8151099681854248s\n",
      "94/174, train_loss: 0.3110, time taken: 1.793004035949707s\n",
      "95/174, train_loss: 0.3261, time taken: 1.7904274463653564s\n",
      "96/174, train_loss: 0.3208, time taken: 1.7857882976531982s\n",
      "97/174, train_loss: 0.2587, time taken: 1.8058342933654785s\n",
      "98/174, train_loss: 0.2664, time taken: 1.800968885421753s\n",
      "99/174, train_loss: 0.2618, time taken: 1.8911404609680176s\n",
      "100/174, train_loss: 0.2647, time taken: 1.8097217082977295s\n",
      "101/174, train_loss: 0.2824, time taken: 1.7846856117248535s\n",
      "102/174, train_loss: 0.3186, time taken: 1.7829678058624268s\n",
      "103/174, train_loss: 0.2877, time taken: 1.8010437488555908s\n",
      "104/174, train_loss: 0.3438, time taken: 1.7146520614624023s\n",
      "105/174, train_loss: 0.2431, time taken: 1.857703685760498s\n",
      "106/174, train_loss: 0.3285, time taken: 1.7915973663330078s\n",
      "107/174, train_loss: 0.2422, time taken: 1.7024927139282227s\n",
      "108/174, train_loss: 0.2683, time taken: 1.7271184921264648s\n",
      "109/174, train_loss: 0.2578, time taken: 1.804861068725586s\n",
      "110/174, train_loss: 0.2638, time taken: 1.7198736667633057s\n",
      "111/174, train_loss: 0.2502, time taken: 1.7802886962890625s\n",
      "112/174, train_loss: 0.3252, time taken: 1.7979300022125244s\n",
      "113/174, train_loss: 0.3022, time taken: 1.7787623405456543s\n",
      "114/174, train_loss: 0.2691, time taken: 1.8137915134429932s\n",
      "115/174, train_loss: 0.3039, time taken: 1.7964587211608887s\n",
      "116/174, train_loss: 0.3025, time taken: 1.8034355640411377s\n",
      "117/174, train_loss: 0.2893, time taken: 1.7970824241638184s\n",
      "118/174, train_loss: 0.2639, time taken: 1.8648560047149658s\n",
      "119/174, train_loss: 0.2584, time taken: 1.7220330238342285s\n",
      "120/174, train_loss: 0.2878, time taken: 1.7900316715240479s\n",
      "121/174, train_loss: 0.2935, time taken: 1.804840087890625s\n",
      "122/174, train_loss: 0.2964, time taken: 1.791863203048706s\n",
      "123/174, train_loss: 0.3330, time taken: 1.8064842224121094s\n",
      "124/174, train_loss: 0.2820, time taken: 1.8068039417266846s\n",
      "125/174, train_loss: 0.3010, time taken: 1.806286096572876s\n",
      "126/174, train_loss: 0.2770, time taken: 1.817798137664795s\n",
      "127/174, train_loss: 0.2922, time taken: 1.797856330871582s\n",
      "128/174, train_loss: 0.2960, time taken: 1.8792452812194824s\n",
      "129/174, train_loss: 0.2510, time taken: 1.810509204864502s\n",
      "130/174, train_loss: 0.2920, time taken: 1.7858314514160156s\n",
      "131/174, train_loss: 0.2600, time taken: 1.815685749053955s\n",
      "132/174, train_loss: 0.3118, time taken: 1.7805771827697754s\n",
      "133/174, train_loss: 0.2788, time taken: 1.7940130233764648s\n",
      "134/174, train_loss: 0.2640, time taken: 1.8048110008239746s\n",
      "135/174, train_loss: 0.2541, time taken: 1.8706350326538086s\n",
      "136/174, train_loss: 0.2765, time taken: 1.8886165618896484s\n",
      "137/174, train_loss: 0.2683, time taken: 1.8327953815460205s\n",
      "138/174, train_loss: 0.2470, time taken: 1.8216466903686523s\n",
      "139/174, train_loss: 0.2783, time taken: 2.4968628883361816s\n",
      "140/174, train_loss: 0.2125, time taken: 1.7883625030517578s\n",
      "141/174, train_loss: 0.2578, time taken: 3.3902323246002197s\n",
      "142/174, train_loss: 0.3198, time taken: 1.8222150802612305s\n",
      "143/174, train_loss: 0.3016, time taken: 1.8656587600708008s\n",
      "144/174, train_loss: 0.3205, time taken: 1.8061542510986328s\n",
      "145/174, train_loss: 0.2949, time taken: 1.912323236465454s\n",
      "146/174, train_loss: 0.2439, time taken: 1.9085242748260498s\n",
      "147/174, train_loss: 0.2328, time taken: 2.0232701301574707s\n",
      "148/174, train_loss: 0.2485, time taken: 1.8903794288635254s\n",
      "149/174, train_loss: 0.3093, time taken: 1.786400318145752s\n",
      "150/174, train_loss: 0.2745, time taken: 1.8089075088500977s\n",
      "151/174, train_loss: 0.2871, time taken: 1.7946794033050537s\n",
      "152/174, train_loss: 0.2649, time taken: 1.8882074356079102s\n",
      "153/174, train_loss: 0.3017, time taken: 1.9009060859680176s\n",
      "154/174, train_loss: 0.2900, time taken: 1.9052250385284424s\n",
      "155/174, train_loss: 0.3241, time taken: 1.9699177742004395s\n",
      "156/174, train_loss: 0.2774, time taken: 1.7908246517181396s\n",
      "157/174, train_loss: 0.2477, time taken: 1.736567735671997s\n",
      "158/174, train_loss: 0.2731, time taken: 1.8596408367156982s\n",
      "159/174, train_loss: 0.2590, time taken: 1.7917640209197998s\n",
      "160/174, train_loss: 0.3135, time taken: 1.8963043689727783s\n",
      "161/174, train_loss: 0.2214, time taken: 1.8128037452697754s\n",
      "162/174, train_loss: 0.2897, time taken: 2.011103630065918s\n",
      "163/174, train_loss: 0.3054, time taken: 2.1607956886291504s\n",
      "164/174, train_loss: 0.2454, time taken: 1.799814224243164s\n",
      "165/174, train_loss: 0.2877, time taken: 1.778984546661377s\n",
      "166/174, train_loss: 0.2789, time taken: 1.7299127578735352s\n",
      "167/174, train_loss: 0.3250, time taken: 1.7715518474578857s\n",
      "168/174, train_loss: 0.2672, time taken: 1.7819979190826416s\n",
      "169/174, train_loss: 0.3206, time taken: 1.7273712158203125s\n",
      "170/174, train_loss: 0.5160, time taken: 1.7844488620758057s\n",
      "171/174, train_loss: 0.2970, time taken: 1.7010729312896729s\n",
      "172/174, train_loss: 0.3011, time taken: 1.7936570644378662s\n",
      "173/174, train_loss: 0.2561, time taken: 1.8024933338165283s\n",
      "174/174, train_loss: 0.2597, time taken: 1.7891228199005127s\n",
      "175/174, train_loss: 0.2581, time taken: 1.3884518146514893s\n",
      "epoch 2 average loss: 0.2942\n",
      "----------\n",
      "epoch 3/500\n",
      "1/174, train_loss: 0.2545, time taken: 8.469016313552856s\n",
      "2/174, train_loss: 0.2514, time taken: 2.1886744499206543s\n",
      "3/174, train_loss: 0.2943, time taken: 1.9642271995544434s\n",
      "4/174, train_loss: 0.3067, time taken: 1.8178021907806396s\n",
      "5/174, train_loss: 0.2988, time taken: 1.7985835075378418s\n",
      "6/174, train_loss: 0.2694, time taken: 1.7918076515197754s\n",
      "7/174, train_loss: 0.2943, time taken: 1.9073007106781006s\n",
      "8/174, train_loss: 0.2657, time taken: 1.8044438362121582s\n",
      "9/174, train_loss: 0.3206, time taken: 1.7381718158721924s\n",
      "10/174, train_loss: 0.2571, time taken: 1.785313606262207s\n",
      "11/174, train_loss: 0.3184, time taken: 1.7898485660552979s\n",
      "12/174, train_loss: 0.2123, time taken: 1.8855457305908203s\n",
      "13/174, train_loss: 0.3086, time taken: 1.7927095890045166s\n",
      "14/174, train_loss: 0.3066, time taken: 1.8833189010620117s\n",
      "15/174, train_loss: 0.2581, time taken: 1.82576584815979s\n",
      "16/174, train_loss: 0.2382, time taken: 1.8886277675628662s\n",
      "17/174, train_loss: 0.2286, time taken: 1.8853580951690674s\n",
      "18/174, train_loss: 0.3248, time taken: 1.8024823665618896s\n",
      "19/174, train_loss: 0.2602, time taken: 1.8696541786193848s\n",
      "20/174, train_loss: 0.3024, time taken: 1.7952890396118164s\n",
      "21/174, train_loss: 0.2458, time taken: 1.7243289947509766s\n",
      "22/174, train_loss: 0.2463, time taken: 1.8572032451629639s\n",
      "23/174, train_loss: 0.2242, time taken: 1.7171974182128906s\n",
      "24/174, train_loss: 0.3073, time taken: 1.7750654220581055s\n",
      "25/174, train_loss: 0.2494, time taken: 1.8096070289611816s\n",
      "26/174, train_loss: 0.2610, time taken: 1.824150800704956s\n",
      "27/174, train_loss: 0.2217, time taken: 1.7843513488769531s\n",
      "28/174, train_loss: 0.2207, time taken: 1.7937424182891846s\n",
      "29/174, train_loss: 0.2511, time taken: 2.470651626586914s\n",
      "30/174, train_loss: 0.2776, time taken: 1.7063920497894287s\n",
      "31/174, train_loss: 0.3037, time taken: 1.879490852355957s\n",
      "32/174, train_loss: 0.2955, time taken: 1.8157107830047607s\n",
      "33/174, train_loss: 0.2584, time taken: 1.80647611618042s\n",
      "34/174, train_loss: 0.2326, time taken: 1.792396068572998s\n",
      "35/174, train_loss: 0.2571, time taken: 1.8049156665802002s\n",
      "36/174, train_loss: 0.2850, time taken: 1.7816991806030273s\n",
      "37/174, train_loss: 0.2745, time taken: 1.811819314956665s\n",
      "38/174, train_loss: 0.3423, time taken: 1.7211062908172607s\n",
      "39/174, train_loss: 0.2892, time taken: 1.8068552017211914s\n",
      "40/174, train_loss: 0.2815, time taken: 1.8829660415649414s\n",
      "41/174, train_loss: 0.2804, time taken: 1.886275053024292s\n",
      "42/174, train_loss: 0.2529, time taken: 1.8018038272857666s\n",
      "43/174, train_loss: 0.2823, time taken: 1.7819013595581055s\n",
      "44/174, train_loss: 0.2999, time taken: 1.7991480827331543s\n",
      "45/174, train_loss: 0.2751, time taken: 1.7985780239105225s\n",
      "46/174, train_loss: 0.3316, time taken: 1.7936131954193115s\n",
      "47/174, train_loss: 0.2762, time taken: 1.7211518287658691s\n",
      "48/174, train_loss: 0.2421, time taken: 1.7174959182739258s\n",
      "49/174, train_loss: 0.2635, time taken: 1.8053765296936035s\n",
      "50/174, train_loss: 0.3261, time taken: 1.8066823482513428s\n",
      "51/174, train_loss: 0.2609, time taken: 1.7877683639526367s\n",
      "52/174, train_loss: 0.2406, time taken: 1.7993175983428955s\n",
      "53/174, train_loss: 0.2449, time taken: 1.7218258380889893s\n",
      "54/174, train_loss: 0.3147, time taken: 1.798262119293213s\n",
      "55/174, train_loss: 0.2374, time taken: 1.8002662658691406s\n",
      "56/174, train_loss: 0.2905, time taken: 1.8098995685577393s\n",
      "57/174, train_loss: 0.2655, time taken: 1.8118977546691895s\n",
      "58/174, train_loss: 0.2688, time taken: 1.7258713245391846s\n",
      "59/174, train_loss: 0.2945, time taken: 1.7989017963409424s\n",
      "60/174, train_loss: 0.2345, time taken: 1.8003995418548584s\n",
      "61/174, train_loss: 0.2415, time taken: 1.7264182567596436s\n",
      "62/174, train_loss: 0.2464, time taken: 1.792616844177246s\n",
      "63/174, train_loss: 0.2520, time taken: 1.7911958694458008s\n",
      "64/174, train_loss: 0.2540, time taken: 1.879117488861084s\n",
      "65/174, train_loss: 0.2437, time taken: 1.813286542892456s\n",
      "66/174, train_loss: 0.2770, time taken: 1.7919089794158936s\n",
      "67/174, train_loss: 0.2703, time taken: 1.8697268962860107s\n",
      "68/174, train_loss: 0.2235, time taken: 1.7993693351745605s\n",
      "69/174, train_loss: 0.2200, time taken: 1.7172136306762695s\n",
      "70/174, train_loss: 0.2878, time taken: 1.794252872467041s\n",
      "71/174, train_loss: 0.3088, time taken: 1.8653898239135742s\n",
      "72/174, train_loss: 0.2655, time taken: 1.7986741065979004s\n",
      "73/174, train_loss: 0.2137, time taken: 1.7941186428070068s\n",
      "74/174, train_loss: 0.2630, time taken: 1.7898216247558594s\n",
      "75/174, train_loss: 0.2962, time taken: 1.80930757522583s\n",
      "76/174, train_loss: 0.2316, time taken: 3.4879343509674072s\n",
      "77/174, train_loss: 0.3278, time taken: 1.8063514232635498s\n",
      "78/174, train_loss: 0.2633, time taken: 1.8140020370483398s\n",
      "79/174, train_loss: 0.2526, time taken: 1.7790212631225586s\n",
      "80/174, train_loss: 0.2737, time taken: 1.8091607093811035s\n",
      "81/174, train_loss: 0.2254, time taken: 2.054555654525757s\n",
      "82/174, train_loss: 0.2411, time taken: 1.910919427871704s\n",
      "83/174, train_loss: 0.2719, time taken: 2.0908820629119873s\n",
      "84/174, train_loss: 0.2399, time taken: 1.8135626316070557s\n",
      "85/174, train_loss: 0.3008, time taken: 1.9842169284820557s\n",
      "86/174, train_loss: 0.2422, time taken: 1.8264951705932617s\n",
      "87/174, train_loss: 0.2920, time taken: 1.898242712020874s\n",
      "88/174, train_loss: 0.2178, time taken: 1.8200557231903076s\n",
      "89/174, train_loss: 0.2834, time taken: 2.0819079875946045s\n",
      "90/174, train_loss: 0.2777, time taken: 2.114535331726074s\n",
      "91/174, train_loss: 0.2716, time taken: 1.783708095550537s\n",
      "92/174, train_loss: 0.2753, time taken: 1.8094847202301025s\n",
      "93/174, train_loss: 0.2664, time taken: 1.7998409271240234s\n",
      "94/174, train_loss: 0.2319, time taken: 1.7896690368652344s\n",
      "95/174, train_loss: 0.2906, time taken: 1.8847787380218506s\n",
      "96/174, train_loss: 0.3269, time taken: 1.7941980361938477s\n",
      "97/174, train_loss: 0.2563, time taken: 1.81455397605896s\n",
      "98/174, train_loss: 0.2693, time taken: 1.8079655170440674s\n",
      "99/174, train_loss: 0.3212, time taken: 1.8270838260650635s\n",
      "100/174, train_loss: 0.3204, time taken: 1.8707973957061768s\n",
      "101/174, train_loss: 0.2409, time taken: 1.729154109954834s\n",
      "102/174, train_loss: 0.3242, time taken: 1.7707414627075195s\n",
      "103/174, train_loss: 0.2901, time taken: 1.8220481872558594s\n",
      "104/174, train_loss: 0.3145, time taken: 1.795961856842041s\n",
      "105/174, train_loss: 0.2873, time taken: 1.8612878322601318s\n",
      "106/174, train_loss: 0.2562, time taken: 1.8003637790679932s\n",
      "107/174, train_loss: 0.2314, time taken: 1.779611587524414s\n",
      "108/174, train_loss: 0.2103, time taken: 1.729051113128662s\n",
      "109/174, train_loss: 0.2765, time taken: 1.7975993156433105s\n",
      "110/174, train_loss: 0.2443, time taken: 1.7951807975769043s\n",
      "111/174, train_loss: 0.2795, time taken: 1.7922298908233643s\n",
      "112/174, train_loss: 0.2360, time taken: 1.7992877960205078s\n",
      "113/174, train_loss: 0.2254, time taken: 1.7930302619934082s\n",
      "114/174, train_loss: 0.2793, time taken: 1.7987592220306396s\n",
      "115/174, train_loss: 0.2656, time taken: 1.7970085144042969s\n",
      "116/174, train_loss: 0.2336, time taken: 1.79447603225708s\n",
      "117/174, train_loss: 0.2194, time taken: 1.7926766872406006s\n",
      "118/174, train_loss: 0.2816, time taken: 1.8575294017791748s\n",
      "119/174, train_loss: 0.2688, time taken: 1.720989465713501s\n",
      "120/174, train_loss: 0.2653, time taken: 1.7857379913330078s\n",
      "121/174, train_loss: 0.2642, time taken: 1.7814762592315674s\n",
      "122/174, train_loss: 0.2388, time taken: 1.7975594997406006s\n",
      "123/174, train_loss: 0.2237, time taken: 1.7876713275909424s\n",
      "124/174, train_loss: 0.2492, time taken: 1.7995579242706299s\n",
      "125/174, train_loss: 0.2604, time taken: 1.8369088172912598s\n",
      "126/174, train_loss: 0.2835, time taken: 1.8003416061401367s\n",
      "127/174, train_loss: 0.2818, time taken: 1.8074309825897217s\n",
      "128/174, train_loss: 0.2505, time taken: 1.813060998916626s\n",
      "129/174, train_loss: 0.2639, time taken: 1.8012452125549316s\n",
      "130/174, train_loss: 0.2566, time taken: 1.7798576354980469s\n",
      "131/174, train_loss: 0.2178, time taken: 1.7788269519805908s\n",
      "132/174, train_loss: 0.2624, time taken: 1.7902896404266357s\n",
      "133/174, train_loss: 0.2377, time taken: 1.7351067066192627s\n",
      "134/174, train_loss: 0.2475, time taken: 1.8004045486450195s\n",
      "135/174, train_loss: 0.2680, time taken: 1.7302350997924805s\n",
      "136/174, train_loss: 0.2826, time taken: 1.7886695861816406s\n",
      "137/174, train_loss: 0.2762, time taken: 1.8003897666931152s\n",
      "138/174, train_loss: 0.2816, time taken: 1.8070063591003418s\n",
      "139/174, train_loss: 0.2622, time taken: 1.721832275390625s\n",
      "140/174, train_loss: 0.3110, time taken: 1.8030693531036377s\n",
      "141/174, train_loss: 0.2955, time taken: 1.802354335784912s\n",
      "142/174, train_loss: 0.2115, time taken: 1.9147546291351318s\n",
      "143/174, train_loss: 0.2837, time taken: 2.0236639976501465s\n",
      "144/174, train_loss: 0.3119, time taken: 1.9339473247528076s\n",
      "145/174, train_loss: 0.3277, time taken: 1.793421745300293s\n",
      "146/174, train_loss: 0.3098, time taken: 1.8670122623443604s\n",
      "147/174, train_loss: 0.2273, time taken: 2.0350306034088135s\n",
      "148/174, train_loss: 0.2842, time taken: 1.9616081714630127s\n",
      "149/174, train_loss: 0.2237, time taken: 1.8193118572235107s\n",
      "150/174, train_loss: 0.3411, time taken: 1.9787395000457764s\n",
      "151/174, train_loss: 0.2770, time taken: 1.8213090896606445s\n",
      "152/174, train_loss: 0.2588, time taken: 1.8107635974884033s\n",
      "153/174, train_loss: 0.2585, time taken: 1.8935256004333496s\n",
      "154/174, train_loss: 0.2467, time taken: 1.7219891548156738s\n",
      "155/174, train_loss: 0.3816, time taken: 1.8792164325714111s\n",
      "156/174, train_loss: 0.2867, time taken: 1.8172342777252197s\n",
      "157/174, train_loss: 0.2996, time taken: 1.9664621353149414s\n",
      "158/174, train_loss: 0.2750, time taken: 2.2911128997802734s\n",
      "159/174, train_loss: 0.2217, time taken: 1.916546106338501s\n",
      "160/174, train_loss: 0.3027, time taken: 1.8685564994812012s\n",
      "161/174, train_loss: 0.2518, time taken: 1.8197968006134033s\n",
      "162/174, train_loss: 0.2445, time taken: 1.7998476028442383s\n",
      "163/174, train_loss: 0.2222, time taken: 1.7895171642303467s\n",
      "164/174, train_loss: 0.2516, time taken: 1.7915234565734863s\n",
      "165/174, train_loss: 0.2883, time taken: 1.8712553977966309s\n",
      "166/174, train_loss: 0.3330, time taken: 1.7170779705047607s\n",
      "167/174, train_loss: 0.2329, time taken: 1.7927122116088867s\n",
      "168/174, train_loss: 0.2761, time taken: 1.7779896259307861s\n",
      "169/174, train_loss: 0.2796, time taken: 1.8167979717254639s\n",
      "170/174, train_loss: 0.3041, time taken: 1.894141674041748s\n",
      "171/174, train_loss: 0.2218, time taken: 1.8688457012176514s\n",
      "172/174, train_loss: 0.2391, time taken: 1.7313010692596436s\n",
      "173/174, train_loss: 0.5037, time taken: 1.8781466484069824s\n",
      "174/174, train_loss: 0.2763, time taken: 1.7917845249176025s\n",
      "175/174, train_loss: 0.1878, time taken: 1.3947405815124512s\n",
      "epoch 3 average loss: 0.2697\n",
      "Entering Validation for epoch: 3\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 3 Validation avg loss: 0.0542, time taken: 1.2352464199066162s\n",
      "Saving new model based on validation loss 0.0542\n",
      "----------\n",
      "epoch 4/500\n",
      "1/174, train_loss: 0.2908, time taken: 8.530287027359009s\n",
      "2/174, train_loss: 0.2454, time taken: 2.0946755409240723s\n",
      "3/174, train_loss: 0.2289, time taken: 2.0070302486419678s\n",
      "4/174, train_loss: 0.2524, time taken: 1.9860944747924805s\n",
      "5/174, train_loss: 0.2524, time taken: 1.9740149974822998s\n",
      "6/174, train_loss: 0.2444, time taken: 1.7318916320800781s\n",
      "7/174, train_loss: 0.2385, time taken: 1.978306531906128s\n",
      "8/174, train_loss: 0.3480, time taken: 1.7860527038574219s\n",
      "9/174, train_loss: 0.2415, time taken: 1.8839421272277832s\n",
      "10/174, train_loss: 0.2685, time taken: 1.8121135234832764s\n",
      "11/174, train_loss: 0.2955, time taken: 2.102496862411499s\n",
      "12/174, train_loss: 0.2571, time taken: 1.9166183471679688s\n",
      "13/174, train_loss: 0.2686, time taken: 1.8111252784729004s\n",
      "14/174, train_loss: 0.2249, time taken: 1.8136005401611328s\n",
      "15/174, train_loss: 0.2727, time taken: 2.0606284141540527s\n",
      "16/174, train_loss: 0.2268, time taken: 1.7949113845825195s\n",
      "17/174, train_loss: 0.3165, time taken: 1.7275564670562744s\n",
      "18/174, train_loss: 0.2512, time taken: 1.8821699619293213s\n",
      "19/174, train_loss: 0.2451, time taken: 1.8812060356140137s\n",
      "20/174, train_loss: 0.2065, time taken: 1.8921332359313965s\n",
      "21/174, train_loss: 0.2326, time taken: 1.8016536235809326s\n",
      "22/174, train_loss: 0.2108, time taken: 1.79569673538208s\n",
      "23/174, train_loss: 0.2248, time taken: 1.8220434188842773s\n",
      "24/174, train_loss: 0.2159, time taken: 1.8164546489715576s\n",
      "25/174, train_loss: 0.4091, time taken: 1.7042508125305176s\n",
      "26/174, train_loss: 0.2058, time taken: 1.7851028442382812s\n",
      "27/174, train_loss: 0.2445, time taken: 1.7946138381958008s\n",
      "28/174, train_loss: 0.2404, time taken: 2.394505500793457s\n",
      "29/174, train_loss: 0.2611, time taken: 1.783400058746338s\n",
      "30/174, train_loss: 0.2339, time taken: 1.7232449054718018s\n",
      "31/174, train_loss: 0.2658, time taken: 1.9756605625152588s\n",
      "32/174, train_loss: 0.2578, time taken: 1.9122939109802246s\n",
      "33/174, train_loss: 0.2314, time taken: 2.161970853805542s\n",
      "34/174, train_loss: 0.2684, time taken: 1.934300422668457s\n",
      "35/174, train_loss: 0.2684, time taken: 1.86468505859375s\n",
      "36/174, train_loss: 0.2399, time taken: 1.8830010890960693s\n",
      "37/174, train_loss: 0.2364, time taken: 2.0026028156280518s\n",
      "38/174, train_loss: 0.2118, time taken: 1.7875323295593262s\n",
      "39/174, train_loss: 0.3059, time taken: 1.8273100852966309s\n",
      "40/174, train_loss: 0.2132, time taken: 2.0082757472991943s\n",
      "41/174, train_loss: 0.3111, time taken: 1.8112678527832031s\n",
      "42/174, train_loss: 0.2378, time taken: 1.8289353847503662s\n",
      "43/174, train_loss: 0.3099, time taken: 1.829707145690918s\n",
      "44/174, train_loss: 0.2604, time taken: 2.198765277862549s\n",
      "45/174, train_loss: 0.2290, time taken: 2.165011405944824s\n",
      "46/174, train_loss: 0.2740, time taken: 1.8202002048492432s\n",
      "47/174, train_loss: 0.2445, time taken: 1.793332576751709s\n",
      "48/174, train_loss: 0.2559, time taken: 1.7770054340362549s\n",
      "49/174, train_loss: 0.2635, time taken: 1.8028297424316406s\n",
      "50/174, train_loss: 0.2126, time taken: 1.8086912631988525s\n",
      "51/174, train_loss: 0.3123, time taken: 1.8624262809753418s\n",
      "52/174, train_loss: 0.2368, time taken: 1.7231836318969727s\n",
      "53/174, train_loss: 0.2484, time taken: 1.8755674362182617s\n",
      "54/174, train_loss: 0.2473, time taken: 1.8093156814575195s\n",
      "55/174, train_loss: 0.2609, time taken: 1.8889529705047607s\n",
      "56/174, train_loss: 0.2649, time taken: 1.8697125911712646s\n",
      "57/174, train_loss: 0.2361, time taken: 2.5927977561950684s\n",
      "58/174, train_loss: 0.2765, time taken: 1.7396819591522217s\n",
      "59/174, train_loss: 0.2708, time taken: 3.4622974395751953s\n",
      "60/174, train_loss: 0.2427, time taken: 1.7213857173919678s\n",
      "61/174, train_loss: 0.2555, time taken: 1.8810522556304932s\n",
      "62/174, train_loss: 0.2317, time taken: 1.7817175388336182s\n",
      "63/174, train_loss: 0.2442, time taken: 2.0917930603027344s\n",
      "64/174, train_loss: 0.2741, time taken: 1.9913311004638672s\n",
      "65/174, train_loss: 0.2615, time taken: 1.9216234683990479s\n",
      "66/174, train_loss: 0.2142, time taken: 1.8670318126678467s\n",
      "67/174, train_loss: 0.2026, time taken: 2.0964748859405518s\n",
      "68/174, train_loss: 0.3160, time taken: 2.089559555053711s\n",
      "69/174, train_loss: 0.2078, time taken: 1.89772629737854s\n",
      "70/174, train_loss: 0.2602, time taken: 1.9061977863311768s\n",
      "71/174, train_loss: 0.2371, time taken: 1.8204991817474365s\n",
      "72/174, train_loss: 0.2298, time taken: 1.8193776607513428s\n",
      "73/174, train_loss: 0.2467, time taken: 1.8059742450714111s\n",
      "74/174, train_loss: 0.2165, time taken: 1.7931814193725586s\n",
      "75/174, train_loss: 0.2852, time taken: 1.8043501377105713s\n",
      "76/174, train_loss: 0.2487, time taken: 1.7326381206512451s\n",
      "77/174, train_loss: 0.2922, time taken: 1.788149118423462s\n",
      "78/174, train_loss: 0.2348, time taken: 1.7918188571929932s\n",
      "79/174, train_loss: 0.2372, time taken: 1.7818448543548584s\n",
      "80/174, train_loss: 0.2692, time taken: 1.7142674922943115s\n",
      "81/174, train_loss: 0.2008, time taken: 1.7742094993591309s\n",
      "82/174, train_loss: 0.2678, time taken: 1.8046350479125977s\n",
      "83/174, train_loss: 0.2500, time taken: 1.808112382888794s\n",
      "84/174, train_loss: 0.1999, time taken: 1.7859160900115967s\n",
      "85/174, train_loss: 0.2153, time taken: 1.7992875576019287s\n",
      "86/174, train_loss: 0.1915, time taken: 1.7997653484344482s\n",
      "87/174, train_loss: 0.2538, time taken: 1.7844500541687012s\n",
      "88/174, train_loss: 0.2697, time taken: 2.4776928424835205s\n",
      "89/174, train_loss: 0.2713, time taken: 1.7852025032043457s\n",
      "90/174, train_loss: 0.2365, time taken: 1.8000197410583496s\n",
      "91/174, train_loss: 0.2235, time taken: 3.2204790115356445s\n",
      "92/174, train_loss: 0.2215, time taken: 1.796816110610962s\n",
      "93/174, train_loss: 0.2735, time taken: 1.7970869541168213s\n",
      "94/174, train_loss: 0.2308, time taken: 1.9928076267242432s\n",
      "95/174, train_loss: 0.2435, time taken: 1.9822211265563965s\n",
      "96/174, train_loss: 0.2574, time taken: 1.9032278060913086s\n",
      "97/174, train_loss: 0.3366, time taken: 1.9816782474517822s\n",
      "98/174, train_loss: 0.2594, time taken: 1.8832693099975586s\n",
      "99/174, train_loss: 0.2687, time taken: 1.8056983947753906s\n",
      "100/174, train_loss: 0.2678, time taken: 1.9859426021575928s\n",
      "101/174, train_loss: 0.2295, time taken: 1.8055050373077393s\n",
      "102/174, train_loss: 0.3054, time taken: 1.7988452911376953s\n",
      "103/174, train_loss: 0.2859, time taken: 1.8032054901123047s\n",
      "104/174, train_loss: 0.2156, time taken: 1.9721360206604004s\n",
      "105/174, train_loss: 0.2332, time taken: 1.8248639106750488s\n",
      "106/174, train_loss: 0.2369, time taken: 1.9999854564666748s\n",
      "107/174, train_loss: 0.2497, time taken: 1.73423433303833s\n",
      "108/174, train_loss: 0.2016, time taken: 1.8675332069396973s\n",
      "109/174, train_loss: 0.2271, time taken: 1.799692153930664s\n",
      "110/174, train_loss: 0.2616, time taken: 1.880786657333374s\n",
      "111/174, train_loss: 0.2651, time taken: 1.805300235748291s\n",
      "112/174, train_loss: 0.2522, time taken: 1.8182826042175293s\n",
      "113/174, train_loss: 0.2085, time taken: 1.8031408786773682s\n",
      "114/174, train_loss: 0.2354, time taken: 1.8166308403015137s\n",
      "115/174, train_loss: 0.2363, time taken: 1.8722729682922363s\n",
      "116/174, train_loss: 0.2524, time taken: 1.7099418640136719s\n",
      "117/174, train_loss: 0.2587, time taken: 1.7946937084197998s\n",
      "118/174, train_loss: 0.2993, time taken: 1.7191340923309326s\n",
      "119/174, train_loss: 0.2265, time taken: 1.7284190654754639s\n",
      "120/174, train_loss: 0.2618, time taken: 1.7772419452667236s\n",
      "121/174, train_loss: 0.2556, time taken: 1.7899222373962402s\n",
      "122/174, train_loss: 0.2496, time taken: 1.730494737625122s\n",
      "123/174, train_loss: 0.2610, time taken: 1.851484775543213s\n",
      "124/174, train_loss: 0.2517, time taken: 1.7373604774475098s\n",
      "125/174, train_loss: 0.2436, time taken: 1.718393325805664s\n",
      "126/174, train_loss: 0.3007, time taken: 1.8090667724609375s\n",
      "127/174, train_loss: 0.2300, time taken: 2.4767050743103027s\n",
      "128/174, train_loss: 0.2331, time taken: 3.30832576751709s\n",
      "129/174, train_loss: 0.1966, time taken: 2.000286102294922s\n",
      "130/174, train_loss: 0.2677, time taken: 1.9014949798583984s\n",
      "131/174, train_loss: 0.2622, time taken: 1.853722095489502s\n",
      "132/174, train_loss: 0.2064, time taken: 1.741103172302246s\n",
      "133/174, train_loss: 0.2333, time taken: 1.7170841693878174s\n",
      "134/174, train_loss: 0.2287, time taken: 1.7937078475952148s\n",
      "135/174, train_loss: 0.2718, time taken: 1.7998886108398438s\n",
      "136/174, train_loss: 0.2714, time taken: 2.0950443744659424s\n",
      "137/174, train_loss: 0.2224, time taken: 2.0050692558288574s\n",
      "138/174, train_loss: 0.2887, time taken: 1.880728006362915s\n",
      "139/174, train_loss: 0.2252, time taken: 1.7855005264282227s\n",
      "140/174, train_loss: 0.2661, time taken: 2.018990993499756s\n",
      "141/174, train_loss: 0.2491, time taken: 1.8755927085876465s\n",
      "142/174, train_loss: 0.3139, time taken: 1.8052740097045898s\n",
      "143/174, train_loss: 0.2002, time taken: 1.9055733680725098s\n",
      "144/174, train_loss: 0.2422, time taken: 1.9652137756347656s\n",
      "145/174, train_loss: 0.2239, time taken: 1.8170263767242432s\n",
      "146/174, train_loss: 0.2525, time taken: 2.1726462841033936s\n",
      "147/174, train_loss: 0.2275, time taken: 1.8285198211669922s\n",
      "148/174, train_loss: 0.2998, time taken: 1.8442683219909668s\n",
      "149/174, train_loss: 0.2331, time taken: 1.7230806350708008s\n",
      "150/174, train_loss: 0.2201, time taken: 1.7916367053985596s\n",
      "151/174, train_loss: 0.2209, time taken: 1.7925093173980713s\n",
      "152/174, train_loss: 0.2462, time taken: 1.8039829730987549s\n",
      "153/174, train_loss: 0.2236, time taken: 1.7855379581451416s\n",
      "154/174, train_loss: 0.2270, time taken: 1.792189121246338s\n",
      "155/174, train_loss: 0.2857, time taken: 1.8732752799987793s\n",
      "156/174, train_loss: 0.2134, time taken: 1.732985258102417s\n",
      "157/174, train_loss: 0.2360, time taken: 1.8683221340179443s\n",
      "158/174, train_loss: 0.2408, time taken: 1.7157073020935059s\n",
      "159/174, train_loss: 0.3822, time taken: 1.8076753616333008s\n",
      "160/174, train_loss: 0.2508, time taken: 1.8081555366516113s\n",
      "161/174, train_loss: 0.2212, time taken: 1.8060312271118164s\n",
      "162/174, train_loss: 0.1934, time taken: 1.8053171634674072s\n",
      "163/174, train_loss: 0.1857, time taken: 1.8030412197113037s\n",
      "164/174, train_loss: 0.3003, time taken: 1.780433177947998s\n",
      "165/174, train_loss: 0.2289, time taken: 1.8100957870483398s\n",
      "166/174, train_loss: 0.2518, time taken: 1.7263398170471191s\n",
      "167/174, train_loss: 0.2237, time taken: 1.6909489631652832s\n",
      "168/174, train_loss: 0.2487, time taken: 1.8182063102722168s\n",
      "169/174, train_loss: 0.2071, time taken: 1.8333351612091064s\n",
      "170/174, train_loss: 0.2562, time taken: 1.8640360832214355s\n",
      "171/174, train_loss: 0.2566, time taken: 1.8249964714050293s\n",
      "172/174, train_loss: 0.2712, time taken: 1.8021578788757324s\n",
      "173/174, train_loss: 0.3137, time taken: 1.8202488422393799s\n",
      "174/174, train_loss: 0.2188, time taken: 1.815242052078247s\n",
      "175/174, train_loss: 0.2237, time taken: 1.5773041248321533s\n",
      "epoch 4 average loss: 0.2500\n",
      "----------\n",
      "epoch 5/500\n",
      "1/174, train_loss: 0.2360, time taken: 8.859565734863281s\n",
      "2/174, train_loss: 0.3411, time taken: 2.1200640201568604s\n",
      "3/174, train_loss: 0.2465, time taken: 1.9105570316314697s\n",
      "4/174, train_loss: 0.2419, time taken: 1.8933181762695312s\n",
      "5/174, train_loss: 0.2674, time taken: 1.8893036842346191s\n",
      "6/174, train_loss: 0.2442, time taken: 1.8963806629180908s\n",
      "7/174, train_loss: 0.2238, time taken: 1.8757498264312744s\n",
      "8/174, train_loss: 0.2219, time taken: 1.81461501121521s\n",
      "9/174, train_loss: 0.4799, time taken: 1.8828189373016357s\n",
      "10/174, train_loss: 0.3028, time taken: 1.859755516052246s\n",
      "11/174, train_loss: 0.2383, time taken: 2.4991812705993652s\n",
      "12/174, train_loss: 0.2275, time taken: 1.7937736511230469s\n",
      "13/174, train_loss: 0.2290, time taken: 1.9284100532531738s\n",
      "14/174, train_loss: 0.2987, time taken: 1.8770971298217773s\n",
      "15/174, train_loss: 0.2396, time taken: 1.7943642139434814s\n",
      "16/174, train_loss: 0.2669, time taken: 2.015768527984619s\n",
      "17/174, train_loss: 0.2820, time taken: 1.7879056930541992s\n",
      "18/174, train_loss: 0.2879, time taken: 1.8645341396331787s\n",
      "19/174, train_loss: 0.4151, time taken: 1.8164622783660889s\n",
      "20/174, train_loss: 0.3120, time taken: 1.8798537254333496s\n",
      "21/174, train_loss: 0.3264, time taken: 1.8198254108428955s\n",
      "22/174, train_loss: 0.3014, time taken: 1.982905387878418s\n",
      "23/174, train_loss: 0.2163, time taken: 1.9115681648254395s\n",
      "24/174, train_loss: 0.3520, time taken: 1.8786041736602783s\n",
      "25/174, train_loss: 0.2469, time taken: 1.8685860633850098s\n",
      "26/174, train_loss: 0.2450, time taken: 1.8062770366668701s\n",
      "27/174, train_loss: 0.2538, time taken: 2.1015031337738037s\n",
      "28/174, train_loss: 0.2140, time taken: 1.8066387176513672s\n",
      "29/174, train_loss: 0.2452, time taken: 2.0708436965942383s\n",
      "30/174, train_loss: 0.2326, time taken: 1.803626537322998s\n",
      "31/174, train_loss: 0.3075, time taken: 1.9924719333648682s\n",
      "32/174, train_loss: 0.2455, time taken: 1.7917284965515137s\n",
      "33/174, train_loss: 0.2259, time taken: 2.0100393295288086s\n",
      "34/174, train_loss: 0.2695, time taken: 1.8065009117126465s\n",
      "35/174, train_loss: 0.2952, time taken: 1.8655061721801758s\n",
      "36/174, train_loss: 0.2644, time taken: 1.7218492031097412s\n",
      "37/174, train_loss: 0.2533, time taken: 1.7943203449249268s\n",
      "38/174, train_loss: 0.2964, time taken: 1.8750476837158203s\n",
      "39/174, train_loss: 0.2638, time taken: 1.7885310649871826s\n",
      "40/174, train_loss: 0.2654, time taken: 1.8008794784545898s\n",
      "41/174, train_loss: 0.2443, time taken: 1.8152225017547607s\n",
      "42/174, train_loss: 0.2724, time taken: 1.786363124847412s\n",
      "43/174, train_loss: 0.2406, time taken: 1.7974629402160645s\n",
      "44/174, train_loss: 0.3123, time taken: 1.8050997257232666s\n",
      "45/174, train_loss: 0.2452, time taken: 1.775493860244751s\n",
      "46/174, train_loss: 0.2060, time taken: 1.8012042045593262s\n",
      "47/174, train_loss: 0.2743, time taken: 1.786531686782837s\n",
      "48/174, train_loss: 0.2517, time taken: 1.8795411586761475s\n",
      "49/174, train_loss: 0.2885, time taken: 1.7213749885559082s\n",
      "50/174, train_loss: 0.2126, time taken: 1.77604341506958s\n",
      "51/174, train_loss: 0.2664, time taken: 1.7216386795043945s\n",
      "52/174, train_loss: 0.3002, time taken: 1.8562016487121582s\n",
      "53/174, train_loss: 0.2508, time taken: 1.816624641418457s\n",
      "54/174, train_loss: 0.3179, time taken: 1.8026878833770752s\n",
      "55/174, train_loss: 0.2748, time taken: 1.7964601516723633s\n",
      "56/174, train_loss: 0.2287, time taken: 1.8646507263183594s\n",
      "57/174, train_loss: 0.2228, time taken: 1.8007378578186035s\n",
      "58/174, train_loss: 0.2457, time taken: 1.821424961090088s\n",
      "59/174, train_loss: 0.2350, time taken: 2.0871715545654297s\n",
      "60/174, train_loss: 0.3681, time taken: 1.7902100086212158s\n",
      "61/174, train_loss: 0.2634, time taken: 1.8822970390319824s\n",
      "62/174, train_loss: 0.2626, time taken: 3.298593521118164s\n",
      "63/174, train_loss: 0.2729, time taken: 1.8142433166503906s\n",
      "64/174, train_loss: 0.2102, time taken: 1.8921308517456055s\n",
      "65/174, train_loss: 0.3260, time taken: 1.8026316165924072s\n",
      "66/174, train_loss: 0.2945, time taken: 1.8663935661315918s\n",
      "67/174, train_loss: 0.3002, time taken: 1.882354736328125s\n",
      "68/174, train_loss: 0.2388, time taken: 2.1240501403808594s\n",
      "69/174, train_loss: 0.2260, time taken: 1.8591647148132324s\n",
      "70/174, train_loss: 0.2097, time taken: 1.8807637691497803s\n",
      "71/174, train_loss: 0.2204, time taken: 1.9233341217041016s\n",
      "72/174, train_loss: 0.3106, time taken: 1.8882043361663818s\n",
      "73/174, train_loss: 0.2291, time taken: 1.9698824882507324s\n",
      "74/174, train_loss: 0.2616, time taken: 1.806882381439209s\n",
      "75/174, train_loss: 0.2667, time taken: 1.9122283458709717s\n",
      "76/174, train_loss: 0.2644, time taken: 1.811556100845337s\n",
      "77/174, train_loss: 0.2542, time taken: 1.787811517715454s\n",
      "78/174, train_loss: 0.2278, time taken: 2.2735469341278076s\n",
      "79/174, train_loss: 0.2073, time taken: 1.8136587142944336s\n",
      "80/174, train_loss: 0.3050, time taken: 2.071049690246582s\n",
      "81/174, train_loss: 0.2837, time taken: 1.8006477355957031s\n",
      "82/174, train_loss: 0.3076, time taken: 1.8001453876495361s\n",
      "83/174, train_loss: 0.2494, time taken: 1.800762414932251s\n",
      "84/174, train_loss: 0.2614, time taken: 1.772029161453247s\n",
      "85/174, train_loss: 0.2674, time taken: 1.7924444675445557s\n",
      "86/174, train_loss: 0.2131, time taken: 1.7946851253509521s\n",
      "87/174, train_loss: 0.2484, time taken: 1.731046438217163s\n",
      "88/174, train_loss: 0.2473, time taken: 1.7906773090362549s\n",
      "89/174, train_loss: 0.2316, time taken: 1.7995550632476807s\n",
      "90/174, train_loss: 0.2757, time taken: 1.8009636402130127s\n",
      "91/174, train_loss: 0.2670, time taken: 1.7276606559753418s\n",
      "92/174, train_loss: 0.2720, time taken: 1.7920265197753906s\n",
      "93/174, train_loss: 0.2301, time taken: 1.8061647415161133s\n",
      "94/174, train_loss: 0.2219, time taken: 1.7866427898406982s\n",
      "95/174, train_loss: 0.2492, time taken: 1.812732458114624s\n",
      "96/174, train_loss: 0.3075, time taken: 1.799067735671997s\n",
      "97/174, train_loss: 0.2601, time taken: 1.8260982036590576s\n",
      "98/174, train_loss: 0.2499, time taken: 3.286862850189209s\n",
      "99/174, train_loss: 0.2990, time taken: 1.7883477210998535s\n",
      "100/174, train_loss: 0.2963, time taken: 1.8267664909362793s\n",
      "101/174, train_loss: 0.2955, time taken: 1.7222330570220947s\n",
      "102/174, train_loss: 0.2735, time taken: 1.97505521774292s\n",
      "103/174, train_loss: 0.2081, time taken: 1.80820631980896s\n",
      "104/174, train_loss: 0.2891, time taken: 1.791107177734375s\n",
      "105/174, train_loss: 0.2570, time taken: 2.0884616374969482s\n",
      "106/174, train_loss: 0.2822, time taken: 1.8050274848937988s\n",
      "107/174, train_loss: 0.2341, time taken: 1.780803918838501s\n",
      "108/174, train_loss: 0.2034, time taken: 1.9139182567596436s\n",
      "109/174, train_loss: 0.2188, time taken: 1.7892892360687256s\n",
      "110/174, train_loss: 0.2779, time taken: 1.783846139907837s\n",
      "111/174, train_loss: 0.2524, time taken: 1.8255743980407715s\n",
      "112/174, train_loss: 0.2472, time taken: 1.799590826034546s\n",
      "113/174, train_loss: 0.2159, time taken: 1.8744161128997803s\n",
      "114/174, train_loss: 0.3025, time taken: 2.091585636138916s\n",
      "115/174, train_loss: 0.2127, time taken: 2.192786455154419s\n",
      "116/174, train_loss: 0.2783, time taken: 1.8216516971588135s\n",
      "117/174, train_loss: 0.2484, time taken: 1.861198902130127s\n",
      "118/174, train_loss: 0.2355, time taken: 2.095249891281128s\n",
      "119/174, train_loss: 0.2952, time taken: 2.0308284759521484s\n",
      "120/174, train_loss: 0.2682, time taken: 1.7374181747436523s\n",
      "121/174, train_loss: 0.2358, time taken: 1.7813091278076172s\n",
      "122/174, train_loss: 0.2412, time taken: 1.7993836402893066s\n",
      "123/174, train_loss: 0.1737, time taken: 1.783627986907959s\n",
      "124/174, train_loss: 0.2110, time taken: 1.887207269668579s\n",
      "125/174, train_loss: 0.2643, time taken: 1.8001224994659424s\n",
      "126/174, train_loss: 0.2319, time taken: 1.7947838306427002s\n",
      "127/174, train_loss: 0.2218, time taken: 1.887162685394287s\n",
      "128/174, train_loss: 0.2762, time taken: 1.79746413230896s\n",
      "129/174, train_loss: 0.1905, time taken: 1.7862763404846191s\n",
      "130/174, train_loss: 0.2858, time taken: 1.7153117656707764s\n",
      "131/174, train_loss: 0.2263, time taken: 1.7846767902374268s\n",
      "132/174, train_loss: 0.2706, time taken: 1.7099220752716064s\n",
      "133/174, train_loss: 0.2287, time taken: 1.7856652736663818s\n",
      "134/174, train_loss: 0.2611, time taken: 1.718644380569458s\n",
      "135/174, train_loss: 0.2612, time taken: 1.773235559463501s\n",
      "136/174, train_loss: 0.2826, time taken: 1.7952702045440674s\n",
      "137/174, train_loss: 0.2390, time taken: 1.71921706199646s\n",
      "138/174, train_loss: 0.2440, time taken: 2.4761619567871094s\n",
      "139/174, train_loss: 0.2396, time taken: 1.704714059829712s\n",
      "140/174, train_loss: 0.2842, time taken: 1.7767469882965088s\n",
      "141/174, train_loss: 0.2503, time taken: 2.5115630626678467s\n",
      "142/174, train_loss: 0.2403, time taken: 1.7808539867401123s\n",
      "143/174, train_loss: 0.2080, time taken: 1.799877643585205s\n",
      "144/174, train_loss: 0.2842, time taken: 1.8948900699615479s\n",
      "145/174, train_loss: 0.2097, time taken: 1.7769453525543213s\n",
      "146/174, train_loss: 0.2233, time taken: 1.921623945236206s\n",
      "147/174, train_loss: 0.2783, time taken: 1.8687772750854492s\n",
      "148/174, train_loss: 0.2802, time taken: 1.8083593845367432s\n",
      "149/174, train_loss: 0.2153, time taken: 2.0205442905426025s\n",
      "150/174, train_loss: 0.2317, time taken: 1.78413724899292s\n",
      "151/174, train_loss: 0.2240, time taken: 1.9884629249572754s\n",
      "152/174, train_loss: 0.2656, time taken: 1.9768357276916504s\n",
      "153/174, train_loss: 0.2043, time taken: 1.7261381149291992s\n",
      "154/174, train_loss: 0.3562, time taken: 1.796576738357544s\n",
      "155/174, train_loss: 0.2129, time taken: 1.79646897315979s\n",
      "156/174, train_loss: 0.2569, time taken: 1.8520450592041016s\n",
      "157/174, train_loss: 0.2142, time taken: 1.8133974075317383s\n",
      "158/174, train_loss: 0.2229, time taken: 1.7902235984802246s\n",
      "159/174, train_loss: 0.2856, time taken: 2.09981107711792s\n",
      "160/174, train_loss: 0.2553, time taken: 1.8038556575775146s\n",
      "161/174, train_loss: 0.2614, time taken: 1.873988389968872s\n",
      "162/174, train_loss: 0.2225, time taken: 1.8860986232757568s\n",
      "163/174, train_loss: 0.2391, time taken: 1.8938324451446533s\n",
      "164/174, train_loss: 0.2493, time taken: 1.806640386581421s\n",
      "165/174, train_loss: 0.2335, time taken: 1.809669017791748s\n",
      "166/174, train_loss: 0.2134, time taken: 1.9952716827392578s\n",
      "167/174, train_loss: 0.2533, time taken: 1.8036067485809326s\n",
      "168/174, train_loss: 0.2826, time taken: 1.8678264617919922s\n",
      "169/174, train_loss: 0.2896, time taken: 1.7963359355926514s\n",
      "170/174, train_loss: 0.2026, time taken: 1.7843050956726074s\n",
      "171/174, train_loss: 0.2029, time taken: 1.8119196891784668s\n",
      "172/174, train_loss: 0.2314, time taken: 1.7094697952270508s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_cl_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        inputs, inputs_2, gt_input = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"image_2\"].to(device),\n",
    "            batch_data[\"gt_image\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs_v1, hidden_v1 = model(inputs)\n",
    "        outputs_v2, hidden_v2 = model(inputs_2)\n",
    "\n",
    "        flat_out_v1 = outputs_v1.flatten(start_dim=1, end_dim=4)\n",
    "        flat_out_v2 = outputs_v2.flatten(start_dim=1, end_dim=4)\n",
    "\n",
    "        r_loss = recon_loss(outputs_v1, gt_input)\n",
    "        cl_loss = contrastive_loss(flat_out_v1, flat_out_v2)\n",
    "\n",
    "        # Adjust the CL loss by Recon Loss\n",
    "        total_loss = r_loss + cl_loss * r_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += total_loss.item()\n",
    "        step_loss_values.append(total_loss.item())\n",
    "\n",
    "        # CL & Recon Loss Storage of Value\n",
    "        epoch_cl_loss += cl_loss.item()\n",
    "        epoch_recon_loss += r_loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {total_loss.item():.4f}, \"\n",
    "            f\"time taken: {end_time-start_time}s\"\n",
    "        )\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_cl_loss /= step\n",
    "    epoch_recon_loss /= step\n",
    "\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    epoch_cl_loss_values.append(epoch_cl_loss)\n",
    "    epoch_recon_loss_values.append(epoch_recon_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if epoch % val_interval == 0:\n",
    "        print(\"Entering Validation for epoch: {}\".format(epoch + 1))\n",
    "        total_val_loss = 0\n",
    "        val_step = 0\n",
    "        model.eval()\n",
    "        for val_batch in val_loader:\n",
    "            val_step += 1\n",
    "            start_time = time.time()\n",
    "            inputs, gt_input = (\n",
    "                val_batch[\"image\"].to(device),\n",
    "                val_batch[\"gt_image\"].to(device),\n",
    "            )\n",
    "            print(\"Input shape: {}\".format(inputs.shape))\n",
    "            outputs, outputs_v2 = model(inputs)\n",
    "            val_loss = recon_loss(outputs, gt_input)\n",
    "            total_val_loss += val_loss.item()\n",
    "            end_time = time.time()\n",
    "\n",
    "        total_val_loss /= val_step\n",
    "        val_loss_values.append(total_val_loss)\n",
    "        print(\n",
    "            f\"epoch {epoch + 1} Validation avg loss: {total_val_loss:.4f}, \" f\"time taken: {end_time-start_time}s\")\n",
    "\n",
    "        if total_val_loss < best_val_loss:\n",
    "            print(\n",
    "                f\"Saving new model based on validation loss {total_val_loss:.4f}\")\n",
    "            best_val_loss = total_val_loss\n",
    "            checkpoint = {\"epoch\": max_epochs, \"state_dict\": model.state_dict(\n",
    "            ), \"optimizer\": optimizer.state_dict()}\n",
    "            torch.save(checkpoint, os.path.join(logdir_path, \"best_model_French_500.pt\"))\n",
    "\n",
    "        plt.figure(1, figsize=(8, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(epoch_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(val_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Validation Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(epoch_cl_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Contrastive Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(epoch_recon_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Recon Loss\")\n",
    "\n",
    "        plt.savefig(os.path.join(logdir_path, \"loss_plots.png\"))\n",
    "        plt.close(1)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSLUnet",
   "language": "python",
   "name": "sslunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
