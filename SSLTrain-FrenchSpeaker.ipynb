{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.dev2414\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.2.2+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 5b248f6a0dd29cb9c2a9545f980a88de16a6b753\n",
      "MONAI __file__: /home/<username>/virtenvs/SSLUnet/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.22.0\n",
      "scipy version: 1.13.0\n",
      "Pillow version: 10.3.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.66.2\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.8\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.7.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.14.3\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import L1Loss\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.networks.nets import ViTAutoEnc\n",
    "from monai.losses import ContrastiveLoss\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    CopyItemsd,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Spacingd,\n",
    "    OneOf,\n",
    "    ScaleIntensityRanged,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCoarseDropoutd,\n",
    "    RandCoarseShuffled,\n",
    "    ScaleIntensityd,\n",
    "    LambdaD\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': './Data/FrenchSpeakerDataset/NRRD_Files_N4Bias/P10_2.nrrd'} {'image': './Data/FrenchSpeakerDataset/NRRD_Files_N4Bias_Val/P1_2.nrrd'}\n"
     ]
    }
   ],
   "source": [
    "logdir_path = os.path.normpath(\"./logs/\")\n",
    "\n",
    "#Convert the train and validation images into a list with locations\n",
    "train_dir = \"./Data/FrenchSpeakerDataset/NRRD_Files_N4Bias/\"\n",
    "val_dir = \"./Data/FrenchSpeakerDataset/NRRD_Files_N4Bias_Val/\"\n",
    "\n",
    "#train image file\n",
    "timage_filenames = sorted([os.path.join(train_dir, f) for f in os.listdir(train_dir) if f.endswith(\".nrrd\")])\n",
    "\n",
    "#validation image files\n",
    "vimage_filenames = sorted([os.path.join(val_dir, f)\n",
    "                          for f in os.listdir(val_dir) if f.endswith(\".nrrd\")])\n",
    "\n",
    "# Create a list of dictionaries containing the file paths\n",
    "train_datalist = [{\"image\": img} for img in timage_filenames]\n",
    "validation_datalist = [{\"image\": img} for img in vimage_filenames]\n",
    "\n",
    "# Print the datalist to verify\n",
    "print(train_datalist[0], validation_datalist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Define Training Transforms\n",
    "def threshold_image(image):\n",
    "    return np.where(image < 20, 0, image)\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        # Defines the image intensity\n",
    "        ScaleIntensityd(keys=[\"image\"], minv=0, maxv=255),\n",
    "        # Defines threshold for the image\n",
    "        LambdaD(keys=\"image\", func=threshold_image),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "        SpatialPadd(keys=[\"image\"], spatial_size=(64, 64, 64)),\n",
    "        RandSpatialCropSamplesd(keys=[\"image\"], roi_size=(\n",
    "            64, 64, 64), random_size=False, num_samples=2),\n",
    "        CopyItemsd(keys=[\"image\"], times=2, names=[\n",
    "            \"gt_image\", \"image_2\"], allow_missing_keys=False),\n",
    "        OneOf(\n",
    "            transforms=[\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=6, spatial_size=5, dropout_holes=True, max_spatial_size=32\n",
    "                ),\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=6, spatial_size=20, dropout_holes=False, max_spatial_size=64\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        RandCoarseShuffled(keys=[\"image\"], prob=0.8, holes=10, spatial_size=8),\n",
    "        # Please note that that if image, image_2 are called via the same transform call because of the determinism\n",
    "        # they will get augmented the exact same way which is not the required case here, hence two calls are made\n",
    "        OneOf(\n",
    "            transforms=[\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image_2\"], prob=1.0, holes=6, spatial_size=5, dropout_holes=True, max_spatial_size=32\n",
    "                ),\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image_2\"], prob=1.0, holes=6, spatial_size=20, dropout_holes=False, max_spatial_size=64\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        RandCoarseShuffled(keys=[\"image_2\"], prob=0.8,\n",
    "                           holes=10, spatial_size=8),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "check_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image = check_data[\"image\"][0][0]\n",
    "print(f\"image shape: {image.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4de1af6710>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image[:,35,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config\n",
    "\n",
    "# Define Network ViT backbone & Loss & Optimizer\n",
    "device = torch.device(\"cpu\")\n",
    "model = ViTAutoEnc(\n",
    "    in_channels=1,\n",
    "    img_size=(64, 64, 64),\n",
    "    patch_size=(16, 16, 16),\n",
    "    proj_type=\"conv\",\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Hyper-paramters for training loop\n",
    "experiment_name = \"Train-Thresh-255-2000EP-16th\"\n",
    "max_epochs = 2000\n",
    "val_interval = 2\n",
    "batch_size = 4\n",
    "lr = 1e-4\n",
    "epoch_loss_values = []\n",
    "step_loss_values = []\n",
    "epoch_cl_loss_values = []\n",
    "epoch_recon_loss_values = []\n",
    "val_loss_values = []\n",
    "best_val_loss = 1000.0\n",
    "\n",
    "recon_loss = L1Loss()\n",
    "contrastive_loss = ContrastiveLoss(temperature=0.05)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Define DataLoader using MONAI, CacheDataset needs to be used\n",
    "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = Dataset(data=validation_datalist, transform=train_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/2000\n",
      "1/174, train_loss: 39.7546, time taken: 3.3656439781188965s\n",
      "2/174, train_loss: 42.2265, time taken: 4.150252819061279s\n",
      "3/174, train_loss: 32.7458, time taken: 6.611802577972412s\n",
      "4/174, train_loss: 34.2424, time taken: 5.789911985397339s\n",
      "5/174, train_loss: 20.6644, time taken: 2.2932488918304443s\n",
      "6/174, train_loss: 36.7667, time taken: 6.083348274230957s\n",
      "7/174, train_loss: 19.6341, time taken: 4.015890836715698s\n",
      "8/174, train_loss: 39.4123, time taken: 2.7896056175231934s\n",
      "9/174, train_loss: 22.0674, time taken: 3.4997918605804443s\n",
      "10/174, train_loss: 23.4378, time taken: 4.789405584335327s\n",
      "11/174, train_loss: 21.1531, time taken: 2.192441701889038s\n",
      "12/174, train_loss: 35.7936, time taken: 2.126340389251709s\n",
      "13/174, train_loss: 31.2293, time taken: 2.027764320373535s\n",
      "14/174, train_loss: 20.8219, time taken: 1.8021090030670166s\n",
      "15/174, train_loss: 17.8462, time taken: 1.8696725368499756s\n",
      "16/174, train_loss: 20.8244, time taken: 2.0844905376434326s\n",
      "17/174, train_loss: 18.8356, time taken: 2.2333855628967285s\n",
      "18/174, train_loss: 20.5670, time taken: 3.386409044265747s\n",
      "19/174, train_loss: 24.6361, time taken: 2.1890337467193604s\n",
      "20/174, train_loss: 38.8600, time taken: 2.4081428050994873s\n",
      "21/174, train_loss: 16.7015, time taken: 2.00177001953125s\n",
      "22/174, train_loss: 26.1921, time taken: 2.1091959476470947s\n",
      "23/174, train_loss: 24.7524, time taken: 3.1972036361694336s\n",
      "24/174, train_loss: 21.0077, time taken: 2.026019811630249s\n",
      "25/174, train_loss: 26.0599, time taken: 2.0039572715759277s\n",
      "26/174, train_loss: 15.2712, time taken: 2.073336362838745s\n",
      "27/174, train_loss: 28.5924, time taken: 1.8005321025848389s\n",
      "28/174, train_loss: 21.8484, time taken: 1.9974641799926758s\n",
      "29/174, train_loss: 20.4111, time taken: 1.9045779705047607s\n",
      "30/174, train_loss: 18.0657, time taken: 1.8870055675506592s\n",
      "31/174, train_loss: 30.5257, time taken: 1.818608045578003s\n",
      "32/174, train_loss: 21.6925, time taken: 1.8243091106414795s\n",
      "33/174, train_loss: 16.9035, time taken: 1.8770859241485596s\n",
      "34/174, train_loss: 20.8887, time taken: 1.933241367340088s\n",
      "35/174, train_loss: 19.9313, time taken: 2.0636723041534424s\n",
      "36/174, train_loss: 17.3087, time taken: 4.724643230438232s\n",
      "37/174, train_loss: 14.6606, time taken: 2.362065553665161s\n",
      "38/174, train_loss: 16.1948, time taken: 1.9283008575439453s\n",
      "39/174, train_loss: 21.8466, time taken: 2.3894217014312744s\n",
      "40/174, train_loss: 19.0011, time taken: 1.9969487190246582s\n",
      "41/174, train_loss: 25.1463, time taken: 1.972090244293213s\n",
      "42/174, train_loss: 16.9358, time taken: 1.9081008434295654s\n",
      "43/174, train_loss: 27.4042, time taken: 1.9088902473449707s\n",
      "44/174, train_loss: 17.2796, time taken: 1.9912376403808594s\n",
      "45/174, train_loss: 16.2441, time taken: 1.9116313457489014s\n",
      "46/174, train_loss: 20.1763, time taken: 1.9587137699127197s\n",
      "47/174, train_loss: 19.5143, time taken: 1.8057537078857422s\n",
      "48/174, train_loss: 23.3383, time taken: 2.1053030490875244s\n",
      "49/174, train_loss: 20.4027, time taken: 1.9916291236877441s\n",
      "50/174, train_loss: 29.0587, time taken: 1.7193527221679688s\n",
      "51/174, train_loss: 20.3478, time taken: 1.804323673248291s\n",
      "52/174, train_loss: 16.8633, time taken: 1.9215257167816162s\n",
      "53/174, train_loss: 20.1797, time taken: 2.093554735183716s\n",
      "54/174, train_loss: 18.4840, time taken: 2.394084930419922s\n",
      "55/174, train_loss: 18.6471, time taken: 2.365039110183716s\n",
      "56/174, train_loss: 20.0541, time taken: 2.123685359954834s\n",
      "57/174, train_loss: 25.4413, time taken: 1.9860501289367676s\n",
      "58/174, train_loss: 13.9322, time taken: 1.9774363040924072s\n",
      "59/174, train_loss: 21.9786, time taken: 1.9005358219146729s\n",
      "60/174, train_loss: 23.7453, time taken: 1.915902853012085s\n",
      "61/174, train_loss: 22.7089, time taken: 3.38057279586792s\n",
      "62/174, train_loss: 20.8764, time taken: 1.8205652236938477s\n",
      "63/174, train_loss: 35.4337, time taken: 1.990530014038086s\n",
      "64/174, train_loss: 18.5462, time taken: 2.1806228160858154s\n",
      "65/174, train_loss: 14.3207, time taken: 1.6941192150115967s\n",
      "66/174, train_loss: 19.0384, time taken: 2.098602533340454s\n",
      "67/174, train_loss: 27.9052, time taken: 1.7885735034942627s\n",
      "68/174, train_loss: 25.8291, time taken: 1.916485071182251s\n",
      "69/174, train_loss: 20.8807, time taken: 1.8107149600982666s\n",
      "70/174, train_loss: 16.4257, time taken: 2.0319833755493164s\n",
      "71/174, train_loss: 17.4868, time taken: 1.8640060424804688s\n",
      "72/174, train_loss: 16.9224, time taken: 1.8051002025604248s\n",
      "73/174, train_loss: 33.1877, time taken: 2.008674144744873s\n",
      "74/174, train_loss: 19.6413, time taken: 1.7949061393737793s\n",
      "75/174, train_loss: 15.8239, time taken: 2.0158748626708984s\n",
      "76/174, train_loss: 19.0753, time taken: 1.7819862365722656s\n",
      "77/174, train_loss: 17.9722, time taken: 2.0701005458831787s\n",
      "78/174, train_loss: 19.4021, time taken: 1.8355474472045898s\n",
      "79/174, train_loss: 21.3999, time taken: 1.7658987045288086s\n",
      "80/174, train_loss: 17.2593, time taken: 1.716454029083252s\n",
      "81/174, train_loss: 19.3950, time taken: 1.6958768367767334s\n",
      "82/174, train_loss: 17.7730, time taken: 2.478257656097412s\n",
      "83/174, train_loss: 25.5878, time taken: 2.3872463703155518s\n",
      "84/174, train_loss: 17.5588, time taken: 2.696878671646118s\n",
      "85/174, train_loss: 20.9687, time taken: 2.5174965858459473s\n",
      "86/174, train_loss: 18.8952, time taken: 2.7638838291168213s\n",
      "87/174, train_loss: 15.2186, time taken: 2.508788824081421s\n",
      "88/174, train_loss: 27.0471, time taken: 2.626405954360962s\n",
      "89/174, train_loss: 18.8587, time taken: 2.283149003982544s\n",
      "90/174, train_loss: 20.2992, time taken: 2.511949062347412s\n",
      "91/174, train_loss: 24.6401, time taken: 2.4191668033599854s\n",
      "92/174, train_loss: 20.2049, time taken: 2.5004777908325195s\n",
      "93/174, train_loss: 14.7846, time taken: 2.485173225402832s\n",
      "94/174, train_loss: 17.5835, time taken: 2.8089537620544434s\n",
      "95/174, train_loss: 30.4323, time taken: 2.384394645690918s\n",
      "96/174, train_loss: 20.2851, time taken: 2.416606903076172s\n",
      "97/174, train_loss: 18.9085, time taken: 2.6661131381988525s\n",
      "98/174, train_loss: 17.3173, time taken: 2.393649101257324s\n",
      "99/174, train_loss: 24.6528, time taken: 2.507436990737915s\n",
      "100/174, train_loss: 20.4996, time taken: 2.4925143718719482s\n",
      "101/174, train_loss: 20.3864, time taken: 2.427894353866577s\n",
      "102/174, train_loss: 18.3978, time taken: 2.4364213943481445s\n",
      "103/174, train_loss: 20.6219, time taken: 2.4918999671936035s\n",
      "104/174, train_loss: 17.0786, time taken: 2.4909088611602783s\n",
      "105/174, train_loss: 16.5727, time taken: 4.505302429199219s\n",
      "106/174, train_loss: 14.4719, time taken: 3.20994234085083s\n",
      "107/174, train_loss: 13.8186, time taken: 2.9122257232666016s\n",
      "108/174, train_loss: 17.9389, time taken: 2.5001583099365234s\n",
      "109/174, train_loss: 24.7008, time taken: 2.4994914531707764s\n",
      "110/174, train_loss: 19.3466, time taken: 2.8791656494140625s\n",
      "111/174, train_loss: 14.5883, time taken: 2.6025004386901855s\n",
      "112/174, train_loss: 17.4913, time taken: 2.5147571563720703s\n",
      "113/174, train_loss: 20.6551, time taken: 2.670844316482544s\n",
      "114/174, train_loss: 19.0597, time taken: 2.9198222160339355s\n",
      "115/174, train_loss: 13.2637, time taken: 3.2626001834869385s\n",
      "116/174, train_loss: 16.5419, time taken: 2.7107512950897217s\n",
      "117/174, train_loss: 21.3173, time taken: 2.688732147216797s\n",
      "118/174, train_loss: 19.8690, time taken: 2.781811237335205s\n",
      "119/174, train_loss: 16.5436, time taken: 2.4043281078338623s\n",
      "120/174, train_loss: 19.0450, time taken: 2.4946274757385254s\n",
      "121/174, train_loss: 11.6155, time taken: 2.503598690032959s\n",
      "122/174, train_loss: 28.9289, time taken: 2.421657085418701s\n",
      "123/174, train_loss: 19.8261, time taken: 2.570584774017334s\n",
      "124/174, train_loss: 18.2744, time taken: 2.585995674133301s\n",
      "125/174, train_loss: 25.7752, time taken: 2.61674427986145s\n",
      "126/174, train_loss: 18.8956, time taken: 2.5806589126586914s\n",
      "127/174, train_loss: 17.0335, time taken: 2.2222213745117188s\n",
      "128/174, train_loss: 16.9975, time taken: 2.762047290802002s\n",
      "129/174, train_loss: 13.8229, time taken: 2.4165632724761963s\n",
      "130/174, train_loss: 21.6615, time taken: 2.4888222217559814s\n",
      "131/174, train_loss: 14.6065, time taken: 4.417511701583862s\n",
      "132/174, train_loss: 18.5064, time taken: 3.1538164615631104s\n",
      "133/174, train_loss: 18.3096, time taken: 2.8242976665496826s\n",
      "134/174, train_loss: 18.6772, time taken: 2.512763023376465s\n",
      "135/174, train_loss: 12.7856, time taken: 3.0343778133392334s\n",
      "136/174, train_loss: 18.7975, time taken: 3.2738070487976074s\n",
      "137/174, train_loss: 15.9285, time taken: 2.697299003601074s\n",
      "138/174, train_loss: 19.3881, time taken: 2.7019898891448975s\n",
      "139/174, train_loss: 16.7402, time taken: 2.585019111633301s\n",
      "140/174, train_loss: 19.1248, time taken: 3.2257730960845947s\n",
      "141/174, train_loss: 21.9937, time taken: 2.700449228286743s\n",
      "142/174, train_loss: 24.8262, time taken: 2.7669100761413574s\n",
      "143/174, train_loss: 16.9773, time taken: 2.417102098464966s\n",
      "144/174, train_loss: 18.8856, time taken: 2.685070037841797s\n",
      "145/174, train_loss: 19.4355, time taken: 2.6797451972961426s\n",
      "146/174, train_loss: 15.1411, time taken: 2.5871307849884033s\n",
      "147/174, train_loss: 17.1971, time taken: 2.5355918407440186s\n",
      "148/174, train_loss: 13.7537, time taken: 2.425077199935913s\n",
      "149/174, train_loss: 18.5317, time taken: 2.49312424659729s\n",
      "150/174, train_loss: 32.6907, time taken: 2.7115848064422607s\n",
      "151/174, train_loss: 19.6199, time taken: 2.4923195838928223s\n",
      "152/174, train_loss: 22.5847, time taken: 2.966684341430664s\n",
      "153/174, train_loss: 19.5011, time taken: 2.403106927871704s\n",
      "154/174, train_loss: 20.8963, time taken: 2.6130154132843018s\n",
      "155/174, train_loss: 21.3968, time taken: 2.3068771362304688s\n",
      "156/174, train_loss: 19.1062, time taken: 2.59145450592041s\n",
      "157/174, train_loss: 19.9062, time taken: 2.368445873260498s\n",
      "158/174, train_loss: 19.6712, time taken: 2.4882760047912598s\n",
      "159/174, train_loss: 18.2330, time taken: 2.5183138847351074s\n",
      "160/174, train_loss: 24.7813, time taken: 2.5074126720428467s\n",
      "161/174, train_loss: 18.6909, time taken: 2.689423084259033s\n",
      "162/174, train_loss: 16.8020, time taken: 2.571396589279175s\n",
      "163/174, train_loss: 16.4644, time taken: 2.4330270290374756s\n",
      "164/174, train_loss: 21.1278, time taken: 2.5679943561553955s\n",
      "165/174, train_loss: 21.8340, time taken: 4.4958720207214355s\n",
      "166/174, train_loss: 21.7778, time taken: 2.919663667678833s\n",
      "167/174, train_loss: 20.0282, time taken: 2.8867807388305664s\n",
      "168/174, train_loss: 15.0089, time taken: 2.9038760662078857s\n",
      "169/174, train_loss: 14.7705, time taken: 2.5824131965637207s\n",
      "170/174, train_loss: 12.7535, time taken: 2.9116127490997314s\n",
      "171/174, train_loss: 18.5101, time taken: 2.5642318725585938s\n",
      "172/174, train_loss: 21.8750, time taken: 2.830265760421753s\n",
      "173/174, train_loss: 16.4219, time taken: 2.7035608291625977s\n",
      "174/174, train_loss: 22.2911, time taken: 2.775014877319336s\n",
      "175/174, train_loss: 10.9787, time taken: 1.991797685623169s\n",
      "epoch 1 average loss: 20.7284\n",
      "Entering Validation for epoch: 1\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 1 Validation avg loss: 11.2742, time taken: 1.552180290222168s\n",
      "Saving new model based on validation loss 11.2742\n",
      "----------\n",
      "epoch 2/2000\n",
      "1/174, train_loss: 20.6374, time taken: 9.41554856300354s\n",
      "2/174, train_loss: 18.3462, time taken: 2.9144198894500732s\n",
      "3/174, train_loss: 20.8034, time taken: 2.4753806591033936s\n",
      "4/174, train_loss: 24.1640, time taken: 2.586958885192871s\n",
      "5/174, train_loss: 17.9561, time taken: 2.4225003719329834s\n",
      "6/174, train_loss: 28.2136, time taken: 2.4978222846984863s\n",
      "7/174, train_loss: 15.8754, time taken: 2.58208966255188s\n",
      "8/174, train_loss: 24.1959, time taken: 2.7920315265655518s\n",
      "9/174, train_loss: 19.7085, time taken: 3.3079142570495605s\n",
      "10/174, train_loss: 16.0112, time taken: 2.658393383026123s\n",
      "11/174, train_loss: 17.5624, time taken: 2.8186559677124023s\n",
      "12/174, train_loss: 22.4673, time taken: 3.1765663623809814s\n",
      "13/174, train_loss: 15.7422, time taken: 3.214332103729248s\n",
      "14/174, train_loss: 15.6082, time taken: 3.2028043270111084s\n",
      "15/174, train_loss: 26.5132, time taken: 2.8837714195251465s\n",
      "16/174, train_loss: 14.6056, time taken: 2.969728946685791s\n",
      "17/174, train_loss: 22.5060, time taken: 2.5040323734283447s\n",
      "18/174, train_loss: 19.7770, time taken: 2.9243927001953125s\n",
      "19/174, train_loss: 20.1496, time taken: 3.4757564067840576s\n",
      "20/174, train_loss: 17.5320, time taken: 3.1065194606781006s\n",
      "21/174, train_loss: 20.4085, time taken: 2.782534599304199s\n",
      "22/174, train_loss: 17.4717, time taken: 2.890916109085083s\n",
      "23/174, train_loss: 15.9207, time taken: 2.6922085285186768s\n",
      "24/174, train_loss: 18.6606, time taken: 2.5401275157928467s\n",
      "25/174, train_loss: 25.0548, time taken: 2.5359466075897217s\n",
      "26/174, train_loss: 21.2166, time taken: 2.3798227310180664s\n",
      "27/174, train_loss: 17.1353, time taken: 2.4108386039733887s\n",
      "28/174, train_loss: 15.5480, time taken: 2.4918599128723145s\n",
      "29/174, train_loss: 15.4353, time taken: 2.5927722454071045s\n",
      "30/174, train_loss: 22.4196, time taken: 2.523312568664551s\n",
      "31/174, train_loss: 15.6535, time taken: 2.7134616374969482s\n",
      "32/174, train_loss: 14.1001, time taken: 2.120717763900757s\n",
      "33/174, train_loss: 15.7050, time taken: 2.3770811557769775s\n",
      "34/174, train_loss: 21.8153, time taken: 2.4033446311950684s\n",
      "35/174, train_loss: 18.2892, time taken: 2.4798293113708496s\n",
      "36/174, train_loss: 16.3756, time taken: 2.579180955886841s\n",
      "37/174, train_loss: 15.0574, time taken: 2.821255922317505s\n",
      "38/174, train_loss: 15.7749, time taken: 2.5865068435668945s\n",
      "39/174, train_loss: 15.7454, time taken: 2.7982349395751953s\n",
      "40/174, train_loss: 23.0620, time taken: 3.0015158653259277s\n",
      "41/174, train_loss: 18.2997, time taken: 3.0143954753875732s\n",
      "42/174, train_loss: 21.9949, time taken: 2.5805413722991943s\n",
      "43/174, train_loss: 23.2607, time taken: 2.7987489700317383s\n",
      "44/174, train_loss: 20.8634, time taken: 2.5777180194854736s\n",
      "45/174, train_loss: 14.9253, time taken: 2.901961088180542s\n",
      "46/174, train_loss: 19.4568, time taken: 2.723828077316284s\n",
      "47/174, train_loss: 18.0482, time taken: 2.7623138427734375s\n",
      "48/174, train_loss: 14.9356, time taken: 2.492461919784546s\n",
      "49/174, train_loss: 14.0107, time taken: 2.3975698947906494s\n",
      "50/174, train_loss: 16.6048, time taken: 2.4286088943481445s\n",
      "51/174, train_loss: 21.7396, time taken: 2.57869815826416s\n",
      "52/174, train_loss: 29.7941, time taken: 2.4014892578125s\n",
      "53/174, train_loss: 28.9436, time taken: 2.5038180351257324s\n",
      "54/174, train_loss: 18.5649, time taken: 2.4086055755615234s\n",
      "55/174, train_loss: 19.8256, time taken: 2.5906810760498047s\n",
      "56/174, train_loss: 25.2411, time taken: 4.887605428695679s\n",
      "57/174, train_loss: 15.5359, time taken: 2.799046277999878s\n",
      "58/174, train_loss: 36.0281, time taken: 2.972709894180298s\n",
      "59/174, train_loss: 16.5544, time taken: 3.0389394760131836s\n",
      "60/174, train_loss: 17.9358, time taken: 2.533843755722046s\n",
      "61/174, train_loss: 17.9688, time taken: 2.8651065826416016s\n",
      "62/174, train_loss: 21.6096, time taken: 2.50681734085083s\n",
      "63/174, train_loss: 20.4543, time taken: 3.5823700428009033s\n",
      "64/174, train_loss: 16.0076, time taken: 2.7195544242858887s\n",
      "65/174, train_loss: 20.1767, time taken: 2.7137343883514404s\n",
      "66/174, train_loss: 18.1482, time taken: 2.7609035968780518s\n",
      "67/174, train_loss: 17.7533, time taken: 2.3143746852874756s\n",
      "68/174, train_loss: 17.6963, time taken: 2.5997610092163086s\n",
      "69/174, train_loss: 25.0333, time taken: 2.805065870285034s\n",
      "70/174, train_loss: 18.6719, time taken: 2.497666835784912s\n",
      "71/174, train_loss: 17.6401, time taken: 2.4145474433898926s\n",
      "72/174, train_loss: 14.5962, time taken: 2.487154006958008s\n",
      "73/174, train_loss: 17.0870, time taken: 2.5076651573181152s\n",
      "74/174, train_loss: 15.9999, time taken: 2.6019084453582764s\n",
      "75/174, train_loss: 19.9755, time taken: 2.8620200157165527s\n",
      "76/174, train_loss: 20.8656, time taken: 2.4159696102142334s\n",
      "77/174, train_loss: 17.3482, time taken: 2.4952991008758545s\n",
      "78/174, train_loss: 19.9206, time taken: 2.586360216140747s\n",
      "79/174, train_loss: 17.2274, time taken: 2.7278800010681152s\n",
      "80/174, train_loss: 21.8437, time taken: 2.3353793621063232s\n",
      "81/174, train_loss: 20.6746, time taken: 4.503495931625366s\n",
      "82/174, train_loss: 16.2461, time taken: 2.633047342300415s\n",
      "83/174, train_loss: 17.9714, time taken: 2.716315746307373s\n",
      "84/174, train_loss: 17.2688, time taken: 3.1572914123535156s\n",
      "85/174, train_loss: 20.8627, time taken: 2.9987244606018066s\n",
      "86/174, train_loss: 15.4551, time taken: 2.7085297107696533s\n",
      "87/174, train_loss: 18.3075, time taken: 2.5988659858703613s\n",
      "88/174, train_loss: 15.2014, time taken: 2.8208107948303223s\n",
      "89/174, train_loss: 18.6122, time taken: 2.9259192943573s\n",
      "90/174, train_loss: 18.6662, time taken: 3.2133114337921143s\n",
      "91/174, train_loss: 14.9071, time taken: 3.222984552383423s\n",
      "92/174, train_loss: 13.4461, time taken: 2.599900007247925s\n",
      "93/174, train_loss: 14.3747, time taken: 2.7917966842651367s\n",
      "94/174, train_loss: 21.0720, time taken: 2.493774890899658s\n",
      "95/174, train_loss: 22.9919, time taken: 2.5037131309509277s\n",
      "96/174, train_loss: 18.0970, time taken: 2.5146658420562744s\n",
      "97/174, train_loss: 20.3500, time taken: 2.4577558040618896s\n",
      "98/174, train_loss: 17.3624, time taken: 2.7368156909942627s\n",
      "99/174, train_loss: 17.2401, time taken: 2.5107908248901367s\n",
      "100/174, train_loss: 16.9213, time taken: 2.507932424545288s\n",
      "101/174, train_loss: 17.5350, time taken: 2.5124988555908203s\n",
      "102/174, train_loss: 18.3258, time taken: 2.4418528079986572s\n",
      "103/174, train_loss: 17.3319, time taken: 2.6064364910125732s\n",
      "104/174, train_loss: 13.4467, time taken: 2.69392728805542s\n",
      "105/174, train_loss: 12.7607, time taken: 2.7281839847564697s\n",
      "106/174, train_loss: 17.0095, time taken: 2.6744725704193115s\n",
      "107/174, train_loss: 15.7674, time taken: 4.711034297943115s\n",
      "108/174, train_loss: 14.6995, time taken: 2.959946870803833s\n",
      "109/174, train_loss: 20.6813, time taken: 2.9356276988983154s\n",
      "110/174, train_loss: 15.0837, time taken: 2.50057315826416s\n",
      "111/174, train_loss: 16.4425, time taken: 2.8826663494110107s\n",
      "112/174, train_loss: 16.1845, time taken: 2.8750269412994385s\n",
      "113/174, train_loss: 16.1298, time taken: 2.8936638832092285s\n",
      "114/174, train_loss: 12.9307, time taken: 2.6041147708892822s\n",
      "115/174, train_loss: 21.8640, time taken: 2.505741596221924s\n",
      "116/174, train_loss: 17.2583, time taken: 2.9976041316986084s\n",
      "117/174, train_loss: 13.3768, time taken: 3.0792930126190186s\n",
      "118/174, train_loss: 15.6710, time taken: 2.6239473819732666s\n",
      "119/174, train_loss: 33.9617, time taken: 2.654517889022827s\n",
      "120/174, train_loss: 14.5280, time taken: 2.420480966567993s\n",
      "121/174, train_loss: 18.9285, time taken: 2.5116231441497803s\n",
      "122/174, train_loss: 20.0630, time taken: 2.469693899154663s\n",
      "123/174, train_loss: 17.8009, time taken: 2.390495538711548s\n",
      "124/174, train_loss: 20.2431, time taken: 2.4427382946014404s\n",
      "125/174, train_loss: 19.0582, time taken: 2.323294162750244s\n",
      "126/174, train_loss: 18.2863, time taken: 2.5983004570007324s\n",
      "127/174, train_loss: 21.3417, time taken: 2.5957589149475098s\n",
      "128/174, train_loss: 22.5109, time taken: 2.5183074474334717s\n",
      "129/174, train_loss: 17.6289, time taken: 2.635838508605957s\n",
      "130/174, train_loss: 16.9037, time taken: 2.5047237873077393s\n",
      "131/174, train_loss: 15.2323, time taken: 2.534846305847168s\n",
      "132/174, train_loss: 15.9278, time taken: 2.465040445327759s\n",
      "133/174, train_loss: 26.2365, time taken: 2.6952779293060303s\n",
      "134/174, train_loss: 19.2527, time taken: 2.091707944869995s\n",
      "135/174, train_loss: 17.3916, time taken: 2.7968602180480957s\n",
      "136/174, train_loss: 21.4805, time taken: 2.5050277709960938s\n",
      "137/174, train_loss: 15.3027, time taken: 2.33017635345459s\n",
      "138/174, train_loss: 18.1151, time taken: 2.6221513748168945s\n",
      "139/174, train_loss: 17.0689, time taken: 2.7868218421936035s\n",
      "140/174, train_loss: 21.1344, time taken: 2.594130516052246s\n",
      "141/174, train_loss: 16.5043, time taken: 4.530082941055298s\n",
      "142/174, train_loss: 13.3969, time taken: 2.756450653076172s\n",
      "143/174, train_loss: 25.3983, time taken: 3.0255985260009766s\n",
      "144/174, train_loss: 19.0768, time taken: 2.488556385040283s\n",
      "145/174, train_loss: 15.9659, time taken: 2.403524398803711s\n",
      "146/174, train_loss: 18.9766, time taken: 2.984330654144287s\n",
      "147/174, train_loss: 13.6952, time taken: 2.706822395324707s\n",
      "148/174, train_loss: 18.3486, time taken: 2.6143369674682617s\n",
      "149/174, train_loss: 17.6278, time taken: 2.5756943225860596s\n",
      "150/174, train_loss: 18.8185, time taken: 2.875211477279663s\n",
      "151/174, train_loss: 13.5425, time taken: 2.620763063430786s\n",
      "152/174, train_loss: 19.6555, time taken: 2.7673683166503906s\n",
      "153/174, train_loss: 17.3365, time taken: 3.000166416168213s\n",
      "154/174, train_loss: 20.8876, time taken: 2.5986063480377197s\n",
      "155/174, train_loss: 20.0296, time taken: 2.7033917903900146s\n",
      "156/174, train_loss: 15.3895, time taken: 2.598978042602539s\n",
      "157/174, train_loss: 14.5041, time taken: 2.481765031814575s\n",
      "158/174, train_loss: 19.2503, time taken: 2.31412672996521s\n",
      "159/174, train_loss: 18.0333, time taken: 2.607731342315674s\n",
      "160/174, train_loss: 22.8503, time taken: 2.4986627101898193s\n",
      "161/174, train_loss: 18.8022, time taken: 2.77323579788208s\n",
      "162/174, train_loss: 16.0838, time taken: 2.5917274951934814s\n",
      "163/174, train_loss: 21.2664, time taken: 2.7266204357147217s\n",
      "164/174, train_loss: 14.5657, time taken: 2.5163228511810303s\n",
      "165/174, train_loss: 18.7889, time taken: 2.4842529296875s\n",
      "166/174, train_loss: 31.3702, time taken: 2.4056389331817627s\n",
      "167/174, train_loss: 18.4593, time taken: 2.5257976055145264s\n",
      "168/174, train_loss: 20.8387, time taken: 2.5074028968811035s\n",
      "169/174, train_loss: 26.0937, time taken: 2.808046340942383s\n",
      "170/174, train_loss: 17.0492, time taken: 2.5821096897125244s\n",
      "171/174, train_loss: 22.0955, time taken: 2.9137120246887207s\n",
      "172/174, train_loss: 15.9071, time taken: 2.378446102142334s\n",
      "173/174, train_loss: 18.5631, time taken: 2.5330233573913574s\n",
      "174/174, train_loss: 18.0451, time taken: 4.403318405151367s\n",
      "175/174, train_loss: 27.1802, time taken: 2.517751693725586s\n",
      "epoch 2 average loss: 18.7951\n",
      "----------\n",
      "epoch 3/2000\n",
      "1/174, train_loss: 15.6000, time taken: 11.48875880241394s\n",
      "2/174, train_loss: 15.3400, time taken: 3.380936861038208s\n",
      "3/174, train_loss: 21.6435, time taken: 2.6800284385681152s\n",
      "4/174, train_loss: 14.6815, time taken: 2.4343652725219727s\n",
      "5/174, train_loss: 14.2538, time taken: 3.289241313934326s\n",
      "6/174, train_loss: 17.1169, time taken: 3.399491548538208s\n",
      "7/174, train_loss: 18.8509, time taken: 3.289581537246704s\n",
      "8/174, train_loss: 14.3436, time taken: 2.779813289642334s\n",
      "9/174, train_loss: 14.6874, time taken: 2.7010209560394287s\n",
      "10/174, train_loss: 19.2471, time taken: 2.602915048599243s\n",
      "11/174, train_loss: 17.4252, time taken: 2.404877185821533s\n",
      "12/174, train_loss: 16.9793, time taken: 2.2867422103881836s\n",
      "13/174, train_loss: 14.8733, time taken: 2.6951591968536377s\n",
      "14/174, train_loss: 17.2289, time taken: 2.4658665657043457s\n",
      "15/174, train_loss: 12.4966, time taken: 2.5246713161468506s\n",
      "16/174, train_loss: 22.4794, time taken: 2.785788059234619s\n",
      "17/174, train_loss: 20.0820, time taken: 2.5878853797912598s\n",
      "18/174, train_loss: 17.9538, time taken: 2.4028890132904053s\n",
      "19/174, train_loss: 19.2388, time taken: 2.580428123474121s\n",
      "20/174, train_loss: 13.3978, time taken: 2.7973082065582275s\n",
      "21/174, train_loss: 19.3222, time taken: 2.498382806777954s\n",
      "22/174, train_loss: 13.8153, time taken: 2.4027974605560303s\n",
      "23/174, train_loss: 14.1885, time taken: 2.3852994441986084s\n",
      "24/174, train_loss: 16.4522, time taken: 2.6186091899871826s\n",
      "25/174, train_loss: 18.2601, time taken: 3.076108694076538s\n",
      "26/174, train_loss: 19.9032, time taken: 2.5928351879119873s\n",
      "27/174, train_loss: 15.5652, time taken: 2.423672914505005s\n",
      "28/174, train_loss: 15.0894, time taken: 5.27747917175293s\n",
      "29/174, train_loss: 15.4076, time taken: 3.103053331375122s\n",
      "30/174, train_loss: 23.9151, time taken: 2.81032657623291s\n",
      "31/174, train_loss: 16.8200, time taken: 2.9057037830352783s\n",
      "32/174, train_loss: 19.5128, time taken: 2.7254111766815186s\n",
      "33/174, train_loss: 19.9392, time taken: 2.8869917392730713s\n",
      "34/174, train_loss: 16.2876, time taken: 2.9718949794769287s\n",
      "35/174, train_loss: 18.7819, time taken: 2.6305484771728516s\n",
      "36/174, train_loss: 21.6554, time taken: 2.4848358631134033s\n",
      "37/174, train_loss: 25.0494, time taken: 2.313959836959839s\n",
      "38/174, train_loss: 18.6925, time taken: 2.4734115600585938s\n",
      "39/174, train_loss: 14.7695, time taken: 2.6038293838500977s\n",
      "40/174, train_loss: 26.5998, time taken: 2.6994950771331787s\n",
      "41/174, train_loss: 17.2062, time taken: 2.5842881202697754s\n",
      "42/174, train_loss: 15.3620, time taken: 2.595010995864868s\n",
      "43/174, train_loss: 21.1157, time taken: 2.4841930866241455s\n",
      "44/174, train_loss: 17.5447, time taken: 2.4080276489257812s\n",
      "45/174, train_loss: 16.4968, time taken: 2.4043021202087402s\n",
      "46/174, train_loss: 26.4955, time taken: 2.4950599670410156s\n",
      "47/174, train_loss: 18.9780, time taken: 2.503575086593628s\n",
      "48/174, train_loss: 20.4365, time taken: 2.292576313018799s\n",
      "49/174, train_loss: 19.1976, time taken: 2.919649362564087s\n",
      "50/174, train_loss: 16.4740, time taken: 2.544201374053955s\n",
      "51/174, train_loss: 18.2266, time taken: 2.7013120651245117s\n",
      "52/174, train_loss: 21.4318, time taken: 2.799442768096924s\n",
      "53/174, train_loss: 17.6686, time taken: 3.1038713455200195s\n",
      "54/174, train_loss: 14.0633, time taken: 2.602163553237915s\n",
      "55/174, train_loss: 16.8112, time taken: 2.6151864528656006s\n",
      "56/174, train_loss: 13.4423, time taken: 2.976553201675415s\n",
      "57/174, train_loss: 18.8325, time taken: 3.1134653091430664s\n",
      "58/174, train_loss: 18.4340, time taken: 2.68683123588562s\n",
      "59/174, train_loss: 20.5759, time taken: 2.514968156814575s\n",
      "60/174, train_loss: 18.7800, time taken: 2.4126834869384766s\n",
      "61/174, train_loss: 15.6099, time taken: 2.8076565265655518s\n",
      "62/174, train_loss: 19.2500, time taken: 2.400958299636841s\n",
      "63/174, train_loss: 19.3946, time taken: 2.474135160446167s\n",
      "64/174, train_loss: 17.2998, time taken: 2.4235548973083496s\n",
      "65/174, train_loss: 17.5795, time taken: 2.504439115524292s\n",
      "66/174, train_loss: 14.3210, time taken: 2.4904258251190186s\n",
      "67/174, train_loss: 18.3387, time taken: 2.3921382427215576s\n",
      "68/174, train_loss: 16.4069, time taken: 2.9120662212371826s\n",
      "69/174, train_loss: 17.3015, time taken: 2.657437562942505s\n",
      "70/174, train_loss: 24.6129, time taken: 4.385251045227051s\n",
      "71/174, train_loss: 17.1283, time taken: 2.5051443576812744s\n",
      "72/174, train_loss: 15.0864, time taken: 2.708390712738037s\n",
      "73/174, train_loss: 15.3539, time taken: 3.085378646850586s\n",
      "74/174, train_loss: 24.0874, time taken: 2.921417474746704s\n",
      "75/174, train_loss: 18.5450, time taken: 2.5399181842803955s\n",
      "76/174, train_loss: 17.3443, time taken: 2.47586989402771s\n",
      "77/174, train_loss: 18.3910, time taken: 2.9047458171844482s\n",
      "78/174, train_loss: 19.4393, time taken: 2.5601181983947754s\n",
      "79/174, train_loss: 18.3030, time taken: 2.9197678565979004s\n",
      "80/174, train_loss: 18.5450, time taken: 2.8844220638275146s\n",
      "81/174, train_loss: 20.6795, time taken: 2.4152848720550537s\n",
      "82/174, train_loss: 17.0097, time taken: 2.4910330772399902s\n",
      "83/174, train_loss: 17.9407, time taken: 2.4066615104675293s\n",
      "84/174, train_loss: 18.0182, time taken: 2.4597830772399902s\n",
      "85/174, train_loss: 21.9042, time taken: 2.4214065074920654s\n",
      "86/174, train_loss: 18.3754, time taken: 2.486346960067749s\n",
      "87/174, train_loss: 19.4179, time taken: 2.386690855026245s\n",
      "88/174, train_loss: 18.3670, time taken: 2.420413017272949s\n",
      "89/174, train_loss: 19.7133, time taken: 2.5990116596221924s\n",
      "90/174, train_loss: 13.2051, time taken: 2.470795154571533s\n",
      "91/174, train_loss: 17.4410, time taken: 2.423232078552246s\n",
      "92/174, train_loss: 18.0135, time taken: 2.4192943572998047s\n",
      "93/174, train_loss: 18.6501, time taken: 2.4110283851623535s\n",
      "94/174, train_loss: 17.2492, time taken: 2.489773750305176s\n",
      "95/174, train_loss: 23.4029, time taken: 4.295356750488281s\n",
      "96/174, train_loss: 16.4942, time taken: 3.084404468536377s\n",
      "97/174, train_loss: 18.4113, time taken: 2.824714183807373s\n",
      "98/174, train_loss: 18.5432, time taken: 3.0670409202575684s\n",
      "99/174, train_loss: 19.1292, time taken: 2.4929349422454834s\n",
      "100/174, train_loss: 15.2377, time taken: 2.5128769874572754s\n",
      "101/174, train_loss: 22.6959, time taken: 2.802701234817505s\n",
      "102/174, train_loss: 19.6740, time taken: 2.4921936988830566s\n",
      "103/174, train_loss: 21.5856, time taken: 3.077521324157715s\n",
      "104/174, train_loss: 17.6732, time taken: 2.7048258781433105s\n",
      "105/174, train_loss: 17.4108, time taken: 3.1863765716552734s\n",
      "106/174, train_loss: 27.3559, time taken: 2.7277541160583496s\n",
      "107/174, train_loss: 22.0512, time taken: 3.086916208267212s\n",
      "108/174, train_loss: 24.6946, time taken: 2.472578525543213s\n",
      "109/174, train_loss: 16.6382, time taken: 2.435880422592163s\n",
      "110/174, train_loss: 12.6684, time taken: 2.7863008975982666s\n",
      "111/174, train_loss: 14.2855, time taken: 2.5754432678222656s\n",
      "112/174, train_loss: 15.0141, time taken: 2.614846706390381s\n",
      "113/174, train_loss: 15.0156, time taken: 2.7711479663848877s\n",
      "114/174, train_loss: 18.0615, time taken: 2.417567253112793s\n",
      "115/174, train_loss: 19.2958, time taken: 2.3999972343444824s\n",
      "116/174, train_loss: 21.5033, time taken: 2.577449083328247s\n",
      "117/174, train_loss: 20.4887, time taken: 2.6275947093963623s\n",
      "118/174, train_loss: 13.5823, time taken: 2.591510772705078s\n",
      "119/174, train_loss: 17.3889, time taken: 2.5822060108184814s\n",
      "120/174, train_loss: 15.0160, time taken: 2.5716233253479004s\n",
      "121/174, train_loss: 15.5774, time taken: 2.6251473426818848s\n",
      "122/174, train_loss: 20.4387, time taken: 2.2983269691467285s\n",
      "123/174, train_loss: 12.5599, time taken: 2.882565498352051s\n",
      "124/174, train_loss: 17.7061, time taken: 2.5285255908966064s\n",
      "125/174, train_loss: 20.6073, time taken: 2.4831159114837646s\n",
      "126/174, train_loss: 19.1993, time taken: 2.8792359828948975s\n",
      "127/174, train_loss: 14.9046, time taken: 4.011850357055664s\n",
      "128/174, train_loss: 15.6786, time taken: 2.3151979446411133s\n",
      "129/174, train_loss: 20.5231, time taken: 2.53208589553833s\n",
      "130/174, train_loss: 23.8226, time taken: 2.486578941345215s\n",
      "131/174, train_loss: 18.3806, time taken: 2.5161542892456055s\n",
      "132/174, train_loss: 15.2939, time taken: 2.7225356101989746s\n",
      "133/174, train_loss: 16.4815, time taken: 2.6896753311157227s\n",
      "134/174, train_loss: 19.4078, time taken: 2.7049591541290283s\n",
      "135/174, train_loss: 17.3373, time taken: 2.9071364402770996s\n",
      "136/174, train_loss: 19.8250, time taken: 2.680342674255371s\n",
      "137/174, train_loss: 19.2259, time taken: 2.8228280544281006s\n",
      "138/174, train_loss: 17.0549, time taken: 2.490621328353882s\n",
      "139/174, train_loss: 21.1562, time taken: 2.989914894104004s\n",
      "140/174, train_loss: 20.0581, time taken: 2.503347635269165s\n",
      "141/174, train_loss: 17.2007, time taken: 2.796499252319336s\n",
      "142/174, train_loss: 17.0053, time taken: 2.822730779647827s\n",
      "143/174, train_loss: 16.3173, time taken: 2.501493215560913s\n",
      "144/174, train_loss: 15.7842, time taken: 2.392306089401245s\n",
      "145/174, train_loss: 15.5487, time taken: 2.6171517372131348s\n",
      "146/174, train_loss: 16.6660, time taken: 2.561134099960327s\n",
      "147/174, train_loss: 19.9750, time taken: 2.3977713584899902s\n",
      "148/174, train_loss: 20.9867, time taken: 2.5163614749908447s\n",
      "149/174, train_loss: 17.0188, time taken: 2.8697264194488525s\n",
      "150/174, train_loss: 18.8060, time taken: 2.414740562438965s\n",
      "151/174, train_loss: 14.0073, time taken: 2.3877317905426025s\n",
      "152/174, train_loss: 15.0430, time taken: 2.5833420753479004s\n",
      "153/174, train_loss: 18.8829, time taken: 2.5190837383270264s\n",
      "154/174, train_loss: 20.0215, time taken: 2.506423234939575s\n",
      "155/174, train_loss: 16.0783, time taken: 2.503803014755249s\n",
      "156/174, train_loss: 17.0723, time taken: 2.509591579437256s\n",
      "157/174, train_loss: 16.0954, time taken: 4.812628984451294s\n",
      "158/174, train_loss: 12.2386, time taken: 3.2177789211273193s\n",
      "159/174, train_loss: 23.8303, time taken: 3.040616750717163s\n",
      "160/174, train_loss: 25.2510, time taken: 2.5028562545776367s\n",
      "161/174, train_loss: 20.9356, time taken: 2.4461886882781982s\n",
      "162/174, train_loss: 20.3458, time taken: 2.8943068981170654s\n",
      "163/174, train_loss: 23.1665, time taken: 2.5506367683410645s\n",
      "164/174, train_loss: 24.2994, time taken: 2.4095144271850586s\n",
      "165/174, train_loss: 25.6350, time taken: 2.785012722015381s\n",
      "166/174, train_loss: 18.5481, time taken: 3.022426128387451s\n",
      "167/174, train_loss: 16.5282, time taken: 2.8992185592651367s\n",
      "168/174, train_loss: 16.2002, time taken: 2.595958709716797s\n",
      "169/174, train_loss: 19.3897, time taken: 2.8952012062072754s\n",
      "170/174, train_loss: 17.7769, time taken: 2.3999223709106445s\n",
      "171/174, train_loss: 17.7287, time taken: 2.5052950382232666s\n",
      "172/174, train_loss: 21.4607, time taken: 2.486206531524658s\n",
      "173/174, train_loss: 15.5870, time taken: 2.4042410850524902s\n",
      "174/174, train_loss: 22.1218, time taken: 2.4862918853759766s\n",
      "175/174, train_loss: 15.5452, time taken: 2.066614866256714s\n",
      "epoch 3 average loss: 18.2371\n",
      "Entering Validation for epoch: 3\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 3 Validation avg loss: 12.2702, time taken: 1.4562344551086426s\n",
      "----------\n",
      "epoch 4/2000\n",
      "1/174, train_loss: 17.1372, time taken: 10.399022579193115s\n",
      "2/174, train_loss: 24.4496, time taken: 3.409858226776123s\n",
      "3/174, train_loss: 14.4102, time taken: 2.5092883110046387s\n",
      "4/174, train_loss: 21.6408, time taken: 2.7157695293426514s\n",
      "5/174, train_loss: 22.0084, time taken: 4.324151515960693s\n",
      "6/174, train_loss: 20.7774, time taken: 3.067322254180908s\n",
      "7/174, train_loss: 17.6772, time taken: 3.1255762577056885s\n",
      "8/174, train_loss: 13.1160, time taken: 2.6711809635162354s\n",
      "9/174, train_loss: 20.1269, time taken: 3.2935516834259033s\n",
      "10/174, train_loss: 19.4100, time taken: 3.4088029861450195s\n",
      "11/174, train_loss: 22.6898, time taken: 2.930511474609375s\n",
      "12/174, train_loss: 15.7032, time taken: 2.691847562789917s\n",
      "13/174, train_loss: 21.1245, time taken: 2.589221954345703s\n",
      "14/174, train_loss: 20.0162, time taken: 3.2572054862976074s\n",
      "15/174, train_loss: 18.9089, time taken: 3.128512144088745s\n",
      "16/174, train_loss: 15.7367, time taken: 3.359694242477417s\n",
      "17/174, train_loss: 17.0361, time taken: 2.5282623767852783s\n",
      "18/174, train_loss: 13.9690, time taken: 2.888848304748535s\n",
      "19/174, train_loss: 16.1551, time taken: 2.5095138549804688s\n",
      "20/174, train_loss: 18.2892, time taken: 2.7945151329040527s\n",
      "21/174, train_loss: 19.0586, time taken: 2.480487108230591s\n",
      "22/174, train_loss: 17.3678, time taken: 2.6077327728271484s\n",
      "23/174, train_loss: 15.5023, time taken: 2.6838295459747314s\n",
      "24/174, train_loss: 17.2754, time taken: 2.681040048599243s\n",
      "25/174, train_loss: 16.1700, time taken: 2.7330780029296875s\n",
      "26/174, train_loss: 27.1359, time taken: 2.543461561203003s\n",
      "27/174, train_loss: 18.5433, time taken: 2.4887797832489014s\n",
      "28/174, train_loss: 15.6061, time taken: 2.770047426223755s\n",
      "29/174, train_loss: 17.2926, time taken: 2.4235873222351074s\n",
      "30/174, train_loss: 20.5683, time taken: 2.4013969898223877s\n",
      "31/174, train_loss: 17.1953, time taken: 2.585782527923584s\n",
      "32/174, train_loss: 17.1877, time taken: 2.4737608432769775s\n",
      "33/174, train_loss: 26.4671, time taken: 2.4123547077178955s\n",
      "34/174, train_loss: 21.9926, time taken: 2.5779216289520264s\n",
      "35/174, train_loss: 16.3262, time taken: 2.428675651550293s\n",
      "36/174, train_loss: 16.9445, time taken: 2.7889962196350098s\n",
      "37/174, train_loss: 15.3596, time taken: 2.493231773376465s\n",
      "38/174, train_loss: 20.7610, time taken: 3.0842673778533936s\n",
      "39/174, train_loss: 17.7111, time taken: 2.704118490219116s\n",
      "40/174, train_loss: 22.9990, time taken: 2.687572717666626s\n",
      "41/174, train_loss: 12.2783, time taken: 2.896836280822754s\n",
      "42/174, train_loss: 16.3353, time taken: 2.8153533935546875s\n",
      "43/174, train_loss: 18.2845, time taken: 2.887470006942749s\n",
      "44/174, train_loss: 18.7755, time taken: 2.603764533996582s\n",
      "45/174, train_loss: 24.5042, time taken: 2.391350030899048s\n",
      "46/174, train_loss: 16.6019, time taken: 2.475381374359131s\n",
      "47/174, train_loss: 16.7084, time taken: 2.422529458999634s\n",
      "48/174, train_loss: 16.5700, time taken: 2.526160717010498s\n",
      "49/174, train_loss: 16.0801, time taken: 2.780629873275757s\n",
      "50/174, train_loss: 14.8894, time taken: 2.5124709606170654s\n",
      "51/174, train_loss: 15.5761, time taken: 2.6086275577545166s\n",
      "52/174, train_loss: 22.1600, time taken: 3.0405526161193848s\n",
      "53/174, train_loss: 19.5561, time taken: 3.0937561988830566s\n",
      "54/174, train_loss: 20.9766, time taken: 2.925541639328003s\n",
      "55/174, train_loss: 16.8306, time taken: 2.9026036262512207s\n",
      "56/174, train_loss: 16.1623, time taken: 2.704704999923706s\n",
      "57/174, train_loss: 17.9476, time taken: 2.8867082595825195s\n",
      "58/174, train_loss: 17.8758, time taken: 2.9073314666748047s\n",
      "59/174, train_loss: 23.5649, time taken: 3.2901651859283447s\n",
      "60/174, train_loss: 16.7618, time taken: 3.0611162185668945s\n",
      "61/174, train_loss: 17.8597, time taken: 2.80961537361145s\n",
      "62/174, train_loss: 15.2612, time taken: 2.7114529609680176s\n",
      "63/174, train_loss: 17.5004, time taken: 2.4653468132019043s\n",
      "64/174, train_loss: 20.3502, time taken: 2.8171300888061523s\n",
      "65/174, train_loss: 15.0202, time taken: 2.580080986022949s\n",
      "66/174, train_loss: 16.9423, time taken: 2.2959892749786377s\n",
      "67/174, train_loss: 19.3988, time taken: 2.6009790897369385s\n",
      "68/174, train_loss: 18.9752, time taken: 2.579646348953247s\n",
      "69/174, train_loss: 17.6908, time taken: 2.7283527851104736s\n",
      "70/174, train_loss: 15.9327, time taken: 2.2927379608154297s\n",
      "71/174, train_loss: 21.1634, time taken: 2.5957541465759277s\n",
      "72/174, train_loss: 19.7259, time taken: 2.57047700881958s\n",
      "73/174, train_loss: 16.3784, time taken: 2.5075185298919678s\n",
      "74/174, train_loss: 14.8137, time taken: 2.791985273361206s\n",
      "75/174, train_loss: 16.9136, time taken: 2.5833559036254883s\n",
      "76/174, train_loss: 16.1743, time taken: 2.4282233715057373s\n",
      "77/174, train_loss: 15.5061, time taken: 2.8133597373962402s\n",
      "78/174, train_loss: 18.8148, time taken: 2.6479904651641846s\n",
      "79/174, train_loss: 19.1942, time taken: 2.41172456741333s\n",
      "80/174, train_loss: 20.8545, time taken: 2.5898666381835938s\n",
      "81/174, train_loss: 20.8941, time taken: 2.711336612701416s\n",
      "82/174, train_loss: 11.0234, time taken: 3.0726609230041504s\n",
      "83/174, train_loss: 22.1593, time taken: 2.71627140045166s\n",
      "84/174, train_loss: 19.1711, time taken: 3.070276975631714s\n",
      "85/174, train_loss: 13.9354, time taken: 2.7272274494171143s\n",
      "86/174, train_loss: 23.6579, time taken: 2.684180974960327s\n",
      "87/174, train_loss: 16.8536, time taken: 2.4759013652801514s\n",
      "88/174, train_loss: 20.1288, time taken: 2.42140531539917s\n",
      "89/174, train_loss: 18.4359, time taken: 2.9919118881225586s\n",
      "90/174, train_loss: 18.9858, time taken: 2.8959624767303467s\n",
      "91/174, train_loss: 16.7575, time taken: 2.795196294784546s\n",
      "92/174, train_loss: 17.5396, time taken: 2.376965284347534s\n",
      "93/174, train_loss: 14.5662, time taken: 2.490363836288452s\n",
      "94/174, train_loss: 19.2963, time taken: 2.4157252311706543s\n",
      "95/174, train_loss: 14.8812, time taken: 2.6037747859954834s\n",
      "96/174, train_loss: 19.9926, time taken: 2.5891025066375732s\n",
      "97/174, train_loss: 16.3688, time taken: 2.575474500656128s\n",
      "98/174, train_loss: 21.8013, time taken: 2.8005597591400146s\n",
      "99/174, train_loss: 19.5069, time taken: 2.508883476257324s\n",
      "100/174, train_loss: 15.3973, time taken: 2.5000438690185547s\n",
      "101/174, train_loss: 23.9608, time taken: 2.5083439350128174s\n",
      "102/174, train_loss: 20.2380, time taken: 2.3366377353668213s\n",
      "103/174, train_loss: 18.2466, time taken: 2.555283784866333s\n",
      "104/174, train_loss: 15.3082, time taken: 2.7850685119628906s\n",
      "105/174, train_loss: 20.7795, time taken: 2.5359199047088623s\n",
      "106/174, train_loss: 18.0608, time taken: 2.5807242393493652s\n",
      "107/174, train_loss: 15.2470, time taken: 2.7862443923950195s\n",
      "108/174, train_loss: 17.3662, time taken: 2.5178492069244385s\n",
      "109/174, train_loss: 15.9368, time taken: 2.4657249450683594s\n",
      "110/174, train_loss: 13.6413, time taken: 4.52484130859375s\n",
      "111/174, train_loss: 14.0953, time taken: 2.7045021057128906s\n",
      "112/174, train_loss: 14.4297, time taken: 2.848423719406128s\n",
      "113/174, train_loss: 20.0616, time taken: 2.9252769947052s\n",
      "114/174, train_loss: 17.1902, time taken: 2.5146780014038086s\n",
      "115/174, train_loss: 14.1771, time taken: 2.6589181423187256s\n",
      "116/174, train_loss: 16.2601, time taken: 2.8297672271728516s\n",
      "117/174, train_loss: 29.2150, time taken: 2.5632829666137695s\n",
      "118/174, train_loss: 17.8390, time taken: 2.79811429977417s\n",
      "119/174, train_loss: 18.8837, time taken: 2.829599380493164s\n",
      "120/174, train_loss: 19.9470, time taken: 2.9617013931274414s\n",
      "121/174, train_loss: 12.2680, time taken: 2.9318861961364746s\n",
      "122/174, train_loss: 22.3720, time taken: 2.4898946285247803s\n",
      "123/174, train_loss: 18.5359, time taken: 2.5574450492858887s\n",
      "124/174, train_loss: 17.0536, time taken: 2.5408623218536377s\n",
      "125/174, train_loss: 17.7689, time taken: 2.713733196258545s\n",
      "126/174, train_loss: 20.4940, time taken: 2.404496192932129s\n",
      "127/174, train_loss: 20.3573, time taken: 2.6066808700561523s\n",
      "128/174, train_loss: 22.2731, time taken: 2.406669855117798s\n",
      "129/174, train_loss: 17.8779, time taken: 2.236896276473999s\n",
      "130/174, train_loss: 16.7518, time taken: 2.297555923461914s\n",
      "131/174, train_loss: 24.1162, time taken: 2.4838855266571045s\n",
      "132/174, train_loss: 17.9839, time taken: 2.7780392169952393s\n",
      "133/174, train_loss: 14.0718, time taken: 2.489103317260742s\n",
      "134/174, train_loss: 15.3373, time taken: 2.842827796936035s\n",
      "135/174, train_loss: 20.9043, time taken: 2.498884439468384s\n",
      "136/174, train_loss: 21.3999, time taken: 2.305817127227783s\n",
      "137/174, train_loss: 15.8421, time taken: 2.5264549255371094s\n",
      "138/174, train_loss: 15.7283, time taken: 2.4703516960144043s\n",
      "139/174, train_loss: 18.3357, time taken: 2.504574775695801s\n",
      "140/174, train_loss: 20.2748, time taken: 2.5854618549346924s\n",
      "141/174, train_loss: 15.0249, time taken: 2.7222840785980225s\n",
      "142/174, train_loss: 18.4933, time taken: 2.564038038253784s\n",
      "143/174, train_loss: 19.6211, time taken: 2.49646258354187s\n",
      "144/174, train_loss: 14.9193, time taken: 2.5368707180023193s\n",
      "145/174, train_loss: 15.4008, time taken: 4.211066961288452s\n",
      "146/174, train_loss: 16.1607, time taken: 3.011134147644043s\n",
      "147/174, train_loss: 18.2866, time taken: 2.912245512008667s\n",
      "148/174, train_loss: 16.7709, time taken: 3.008355140686035s\n",
      "149/174, train_loss: 15.6561, time taken: 2.8336851596832275s\n",
      "150/174, train_loss: 15.7879, time taken: 2.485813617706299s\n",
      "151/174, train_loss: 21.2235, time taken: 2.590982437133789s\n",
      "152/174, train_loss: 19.1176, time taken: 2.7901649475097656s\n",
      "153/174, train_loss: 19.8619, time taken: 2.5960612297058105s\n",
      "154/174, train_loss: 16.3472, time taken: 2.5914759635925293s\n",
      "155/174, train_loss: 18.8089, time taken: 2.776792049407959s\n",
      "156/174, train_loss: 13.2059, time taken: 2.9250247478485107s\n",
      "157/174, train_loss: 18.9632, time taken: 2.4809653759002686s\n",
      "158/174, train_loss: 16.1635, time taken: 2.904233694076538s\n",
      "159/174, train_loss: 18.1163, time taken: 2.4015557765960693s\n",
      "160/174, train_loss: 17.2605, time taken: 2.498354196548462s\n",
      "161/174, train_loss: 16.4483, time taken: 2.408064365386963s\n",
      "162/174, train_loss: 18.5680, time taken: 2.4056971073150635s\n",
      "163/174, train_loss: 13.1802, time taken: 2.394854784011841s\n",
      "164/174, train_loss: 17.9696, time taken: 2.5064778327941895s\n",
      "165/174, train_loss: 21.9054, time taken: 2.51173734664917s\n",
      "166/174, train_loss: 21.8181, time taken: 2.507744789123535s\n",
      "167/174, train_loss: 19.0199, time taken: 2.236868143081665s\n",
      "168/174, train_loss: 14.5954, time taken: 2.4822983741760254s\n",
      "169/174, train_loss: 22.1141, time taken: 2.572796106338501s\n",
      "170/174, train_loss: 23.0567, time taken: 2.412424087524414s\n",
      "171/174, train_loss: 13.7839, time taken: 2.4789822101593018s\n",
      "172/174, train_loss: 13.9545, time taken: 2.3219337463378906s\n",
      "173/174, train_loss: 15.8365, time taken: 2.604680061340332s\n",
      "174/174, train_loss: 20.2304, time taken: 2.584380626678467s\n",
      "175/174, train_loss: 14.3542, time taken: 2.2107796669006348s\n",
      "epoch 4 average loss: 18.0844\n",
      "----------\n",
      "epoch 5/2000\n",
      "1/174, train_loss: 20.5189, time taken: 10.519503355026245s\n",
      "2/174, train_loss: 14.4716, time taken: 3.891890287399292s\n",
      "3/174, train_loss: 17.2524, time taken: 2.9454894065856934s\n",
      "4/174, train_loss: 14.1595, time taken: 3.4365668296813965s\n",
      "5/174, train_loss: 19.1849, time taken: 3.184217929840088s\n",
      "6/174, train_loss: 14.1596, time taken: 2.7803196907043457s\n",
      "7/174, train_loss: 12.9339, time taken: 2.711228132247925s\n",
      "8/174, train_loss: 17.3778, time taken: 2.8770151138305664s\n",
      "9/174, train_loss: 16.0278, time taken: 3.3029468059539795s\n",
      "10/174, train_loss: 17.8036, time taken: 3.29671049118042s\n",
      "11/174, train_loss: 15.1360, time taken: 3.188032627105713s\n",
      "12/174, train_loss: 20.1436, time taken: 2.6799676418304443s\n",
      "13/174, train_loss: 19.5664, time taken: 2.7170462608337402s\n",
      "14/174, train_loss: 17.0267, time taken: 2.6871869564056396s\n",
      "15/174, train_loss: 14.8220, time taken: 2.4832863807678223s\n",
      "16/174, train_loss: 16.6685, time taken: 2.8307788372039795s\n",
      "17/174, train_loss: 16.0477, time taken: 2.4948391914367676s\n",
      "18/174, train_loss: 16.6319, time taken: 2.504345417022705s\n",
      "19/174, train_loss: 18.8744, time taken: 2.5275228023529053s\n",
      "20/174, train_loss: 14.8845, time taken: 1.9925563335418701s\n",
      "21/174, train_loss: 17.5153, time taken: 2.0722174644470215s\n",
      "22/174, train_loss: 16.2917, time taken: 1.826566457748413s\n",
      "23/174, train_loss: 17.7051, time taken: 2.0081591606140137s\n",
      "24/174, train_loss: 18.6348, time taken: 2.30134916305542s\n",
      "25/174, train_loss: 17.9147, time taken: 2.0281693935394287s\n",
      "26/174, train_loss: 22.9004, time taken: 2.091110944747925s\n",
      "27/174, train_loss: 16.4386, time taken: 2.197350263595581s\n",
      "28/174, train_loss: 17.4424, time taken: 2.0006558895111084s\n",
      "29/174, train_loss: 17.7716, time taken: 1.900435209274292s\n",
      "30/174, train_loss: 15.6719, time taken: 1.7787089347839355s\n",
      "31/174, train_loss: 17.0577, time taken: 3.4937572479248047s\n",
      "32/174, train_loss: 17.1119, time taken: 1.9155426025390625s\n",
      "33/174, train_loss: 10.7427, time taken: 1.8680710792541504s\n",
      "34/174, train_loss: 16.6094, time taken: 2.1110382080078125s\n",
      "35/174, train_loss: 18.1208, time taken: 1.8873686790466309s\n",
      "36/174, train_loss: 16.4766, time taken: 2.011309862136841s\n",
      "37/174, train_loss: 21.0823, time taken: 1.9799387454986572s\n",
      "38/174, train_loss: 18.6799, time taken: 2.0024449825286865s\n",
      "39/174, train_loss: 20.5338, time taken: 1.8089051246643066s\n",
      "40/174, train_loss: 20.2203, time taken: 1.8009974956512451s\n",
      "41/174, train_loss: 15.4313, time taken: 1.7718780040740967s\n",
      "42/174, train_loss: 21.1680, time taken: 1.8123013973236084s\n",
      "43/174, train_loss: 14.4583, time taken: 1.8805205821990967s\n",
      "44/174, train_loss: 17.9363, time taken: 1.9885313510894775s\n",
      "45/174, train_loss: 15.2104, time taken: 1.838850736618042s\n",
      "46/174, train_loss: 14.6459, time taken: 1.8797106742858887s\n",
      "47/174, train_loss: 18.7331, time taken: 1.7867481708526611s\n",
      "48/174, train_loss: 15.0920, time taken: 1.6992108821868896s\n",
      "49/174, train_loss: 15.7664, time taken: 1.7125110626220703s\n",
      "50/174, train_loss: 25.8107, time taken: 1.7881622314453125s\n",
      "51/174, train_loss: 18.3634, time taken: 1.916940450668335s\n",
      "52/174, train_loss: 16.6301, time taken: 1.9250388145446777s\n",
      "53/174, train_loss: 15.7394, time taken: 1.9749031066894531s\n",
      "54/174, train_loss: 17.9898, time taken: 1.7097887992858887s\n",
      "55/174, train_loss: 18.0900, time taken: 2.0075340270996094s\n",
      "56/174, train_loss: 15.3064, time taken: 1.8759000301361084s\n",
      "57/174, train_loss: 16.3358, time taken: 2.0034804344177246s\n",
      "58/174, train_loss: 16.5820, time taken: 1.791172742843628s\n",
      "59/174, train_loss: 19.7961, time taken: 1.89805006980896s\n",
      "60/174, train_loss: 15.5899, time taken: 1.880906343460083s\n",
      "61/174, train_loss: 19.9763, time taken: 1.7371058464050293s\n",
      "62/174, train_loss: 21.4345, time taken: 1.6974995136260986s\n",
      "63/174, train_loss: 16.6203, time taken: 2.0245206356048584s\n",
      "64/174, train_loss: 15.1073, time taken: 2.1166813373565674s\n",
      "65/174, train_loss: 16.4018, time taken: 2.450155735015869s\n",
      "66/174, train_loss: 16.3536, time taken: 2.203807830810547s\n",
      "67/174, train_loss: 17.6420, time taken: 2.0125579833984375s\n",
      "68/174, train_loss: 18.1176, time taken: 2.1091697216033936s\n",
      "69/174, train_loss: 13.1049, time taken: 2.177961826324463s\n",
      "70/174, train_loss: 14.9182, time taken: 1.9087812900543213s\n",
      "71/174, train_loss: 17.0297, time taken: 2.293877124786377s\n",
      "72/174, train_loss: 21.0200, time taken: 1.785632610321045s\n",
      "73/174, train_loss: 17.7916, time taken: 2.084181070327759s\n",
      "74/174, train_loss: 16.4370, time taken: 2.1199915409088135s\n",
      "75/174, train_loss: 15.7614, time taken: 2.074254035949707s\n",
      "76/174, train_loss: 14.4237, time taken: 2.30556583404541s\n",
      "77/174, train_loss: 19.8943, time taken: 1.9063408374786377s\n",
      "78/174, train_loss: 12.9770, time taken: 1.7096655368804932s\n",
      "79/174, train_loss: 20.5019, time taken: 1.7148280143737793s\n",
      "80/174, train_loss: 15.3346, time taken: 1.8969190120697021s\n",
      "81/174, train_loss: 18.1478, time taken: 1.8139894008636475s\n",
      "82/174, train_loss: 16.7162, time taken: 1.6894447803497314s\n",
      "83/174, train_loss: 20.6893, time taken: 1.8860077857971191s\n",
      "84/174, train_loss: 19.4808, time taken: 1.7811062335968018s\n",
      "85/174, train_loss: 16.3434, time taken: 1.7307994365692139s\n",
      "86/174, train_loss: 16.7025, time taken: 1.8327205181121826s\n",
      "87/174, train_loss: 17.9734, time taken: 1.7767853736877441s\n",
      "88/174, train_loss: 18.8019, time taken: 1.7827608585357666s\n",
      "89/174, train_loss: 17.4946, time taken: 1.7055447101593018s\n",
      "90/174, train_loss: 13.7839, time taken: 1.8141093254089355s\n",
      "91/174, train_loss: 13.4024, time taken: 1.9969513416290283s\n",
      "92/174, train_loss: 17.7797, time taken: 1.7879719734191895s\n",
      "93/174, train_loss: 14.5447, time taken: 1.8118033409118652s\n",
      "94/174, train_loss: 17.0062, time taken: 1.8974123001098633s\n",
      "95/174, train_loss: 14.3653, time taken: 1.8923254013061523s\n",
      "96/174, train_loss: 18.8029, time taken: 1.7719781398773193s\n",
      "97/174, train_loss: 15.4146, time taken: 1.881605863571167s\n",
      "98/174, train_loss: 22.1133, time taken: 1.9193274974822998s\n",
      "99/174, train_loss: 18.2029, time taken: 1.7774596214294434s\n",
      "100/174, train_loss: 23.1049, time taken: 1.9012632369995117s\n",
      "101/174, train_loss: 19.6943, time taken: 1.8024492263793945s\n",
      "102/174, train_loss: 17.1036, time taken: 1.7155654430389404s\n",
      "103/174, train_loss: 15.3324, time taken: 1.7750968933105469s\n",
      "104/174, train_loss: 20.4276, time taken: 1.9842562675476074s\n",
      "105/174, train_loss: 18.3166, time taken: 1.8297529220581055s\n",
      "106/174, train_loss: 13.5985, time taken: 1.862046241760254s\n",
      "107/174, train_loss: 17.8001, time taken: 1.982271432876587s\n",
      "108/174, train_loss: 20.7715, time taken: 2.0221657752990723s\n",
      "109/174, train_loss: 16.7226, time taken: 1.90230393409729s\n",
      "110/174, train_loss: 20.7516, time taken: 1.8626534938812256s\n",
      "111/174, train_loss: 16.5241, time taken: 1.9222939014434814s\n",
      "112/174, train_loss: 23.7382, time taken: 1.9121713638305664s\n",
      "113/174, train_loss: 18.4559, time taken: 1.6943683624267578s\n",
      "114/174, train_loss: 14.9377, time taken: 1.7846508026123047s\n",
      "115/174, train_loss: 17.1143, time taken: 1.871877908706665s\n",
      "116/174, train_loss: 18.1862, time taken: 1.8301448822021484s\n",
      "117/174, train_loss: 13.8534, time taken: 1.692915439605713s\n",
      "118/174, train_loss: 12.8101, time taken: 1.87385892868042s\n",
      "119/174, train_loss: 16.5483, time taken: 2.096691370010376s\n",
      "120/174, train_loss: 17.2644, time taken: 1.7850067615509033s\n",
      "121/174, train_loss: 18.2653, time taken: 1.9027752876281738s\n",
      "122/174, train_loss: 16.4310, time taken: 2.512974262237549s\n",
      "123/174, train_loss: 16.8528, time taken: 1.8836650848388672s\n",
      "124/174, train_loss: 19.8697, time taken: 1.893824577331543s\n",
      "125/174, train_loss: 17.4373, time taken: 2.0145349502563477s\n",
      "126/174, train_loss: 20.5966, time taken: 2.282705307006836s\n",
      "127/174, train_loss: 22.2260, time taken: 1.8227083683013916s\n",
      "128/174, train_loss: 15.4494, time taken: 1.8079218864440918s\n",
      "129/174, train_loss: 24.2740, time taken: 1.9061279296875s\n",
      "130/174, train_loss: 18.9369, time taken: 1.7879586219787598s\n",
      "131/174, train_loss: 15.6843, time taken: 2.0035293102264404s\n",
      "132/174, train_loss: 20.2915, time taken: 2.2949750423431396s\n",
      "133/174, train_loss: 11.5387, time taken: 2.110295057296753s\n",
      "134/174, train_loss: 18.9255, time taken: 1.8021671772003174s\n",
      "135/174, train_loss: 17.2770, time taken: 1.7993898391723633s\n",
      "136/174, train_loss: 16.7105, time taken: 1.8756401538848877s\n",
      "137/174, train_loss: 19.5094, time taken: 1.8977952003479004s\n",
      "138/174, train_loss: 17.6928, time taken: 1.8058054447174072s\n",
      "139/174, train_loss: 21.0537, time taken: 1.7903571128845215s\n",
      "140/174, train_loss: 16.6578, time taken: 1.9979462623596191s\n",
      "141/174, train_loss: 16.9986, time taken: 1.875739574432373s\n",
      "142/174, train_loss: 19.0782, time taken: 1.9015626907348633s\n",
      "143/174, train_loss: 15.7049, time taken: 1.8193325996398926s\n",
      "144/174, train_loss: 16.8263, time taken: 1.969116449356079s\n",
      "145/174, train_loss: 20.4212, time taken: 1.8094062805175781s\n",
      "146/174, train_loss: 20.5985, time taken: 1.8802204132080078s\n",
      "147/174, train_loss: 14.4509, time taken: 1.7831342220306396s\n",
      "148/174, train_loss: 18.0993, time taken: 1.7985293865203857s\n",
      "149/174, train_loss: 14.2425, time taken: 2.1143388748168945s\n",
      "150/174, train_loss: 17.4690, time taken: 1.7803447246551514s\n",
      "151/174, train_loss: 16.6558, time taken: 1.8888108730316162s\n",
      "152/174, train_loss: 27.0456, time taken: 1.9883308410644531s\n",
      "153/174, train_loss: 17.8411, time taken: 1.9318654537200928s\n",
      "154/174, train_loss: 17.4442, time taken: 1.9634721279144287s\n",
      "155/174, train_loss: 17.3186, time taken: 1.7301998138427734s\n",
      "156/174, train_loss: 16.0299, time taken: 2.0027952194213867s\n",
      "157/174, train_loss: 22.2342, time taken: 1.7998862266540527s\n",
      "158/174, train_loss: 16.2388, time taken: 1.9760382175445557s\n",
      "159/174, train_loss: 15.9209, time taken: 2.188058853149414s\n",
      "160/174, train_loss: 20.5997, time taken: 1.81209135055542s\n",
      "161/174, train_loss: 12.0990, time taken: 1.8883929252624512s\n",
      "162/174, train_loss: 18.6150, time taken: 2.0091404914855957s\n",
      "163/174, train_loss: 16.1575, time taken: 1.897587776184082s\n",
      "164/174, train_loss: 19.1569, time taken: 2.0774309635162354s\n",
      "165/174, train_loss: 18.3475, time taken: 1.7220771312713623s\n",
      "166/174, train_loss: 23.4122, time taken: 2.037393093109131s\n",
      "167/174, train_loss: 19.0148, time taken: 2.3624396324157715s\n",
      "168/174, train_loss: 16.5128, time taken: 1.8010187149047852s\n",
      "169/174, train_loss: 18.9294, time taken: 1.8317298889160156s\n",
      "170/174, train_loss: 18.1144, time taken: 1.9981389045715332s\n",
      "171/174, train_loss: 19.5647, time taken: 1.8297338485717773s\n",
      "172/174, train_loss: 20.3649, time taken: 1.8732068538665771s\n",
      "173/174, train_loss: 17.3780, time taken: 2.1150243282318115s\n",
      "174/174, train_loss: 19.2907, time taken: 1.797224998474121s\n",
      "175/174, train_loss: 11.7325, time taken: 1.311164379119873s\n",
      "epoch 5 average loss: 17.5460\n",
      "Entering Validation for epoch: 5\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 5 Validation avg loss: 11.6866, time taken: 1.0416314601898193s\n",
      "----------\n",
      "epoch 6/2000\n",
      "1/174, train_loss: 22.7248, time taken: 8.160062551498413s\n",
      "2/174, train_loss: 22.0408, time taken: 2.1723132133483887s\n",
      "3/174, train_loss: 18.5889, time taken: 1.8994245529174805s\n",
      "4/174, train_loss: 16.5687, time taken: 1.9111897945404053s\n",
      "5/174, train_loss: 19.9269, time taken: 1.7929766178131104s\n",
      "6/174, train_loss: 14.1237, time taken: 1.9787118434906006s\n",
      "7/174, train_loss: 14.8322, time taken: 1.813075304031372s\n",
      "8/174, train_loss: 14.5953, time taken: 1.8063554763793945s\n",
      "9/174, train_loss: 16.6384, time taken: 1.9572632312774658s\n",
      "10/174, train_loss: 16.7230, time taken: 1.8234105110168457s\n",
      "11/174, train_loss: 14.3978, time taken: 1.8917949199676514s\n",
      "12/174, train_loss: 19.6638, time taken: 1.7983214855194092s\n",
      "13/174, train_loss: 22.2827, time taken: 1.7878735065460205s\n",
      "14/174, train_loss: 16.0689, time taken: 1.8013184070587158s\n",
      "15/174, train_loss: 17.4323, time taken: 1.714763879776001s\n",
      "16/174, train_loss: 19.2777, time taken: 1.7817511558532715s\n",
      "17/174, train_loss: 14.9403, time taken: 1.9038951396942139s\n",
      "18/174, train_loss: 21.1155, time taken: 1.6960985660552979s\n",
      "19/174, train_loss: 16.7125, time taken: 1.7693016529083252s\n",
      "20/174, train_loss: 18.8623, time taken: 1.7127525806427002s\n",
      "21/174, train_loss: 23.5462, time taken: 3.170058250427246s\n",
      "22/174, train_loss: 14.7632, time taken: 1.7293624877929688s\n",
      "23/174, train_loss: 13.7597, time taken: 1.8005530834197998s\n",
      "24/174, train_loss: 14.9090, time taken: 2.0973050594329834s\n",
      "25/174, train_loss: 17.8447, time taken: 1.7647709846496582s\n",
      "26/174, train_loss: 15.8543, time taken: 1.701869249343872s\n",
      "27/174, train_loss: 14.0154, time taken: 2.4075684547424316s\n",
      "28/174, train_loss: 17.3115, time taken: 1.7896771430969238s\n",
      "29/174, train_loss: 19.0949, time taken: 2.0265414714813232s\n",
      "30/174, train_loss: 19.4425, time taken: 1.870441198348999s\n",
      "31/174, train_loss: 20.2025, time taken: 1.917271375656128s\n",
      "32/174, train_loss: 19.2215, time taken: 1.895233392715454s\n",
      "33/174, train_loss: 18.2287, time taken: 1.904862642288208s\n",
      "34/174, train_loss: 14.1341, time taken: 2.000269889831543s\n",
      "35/174, train_loss: 14.6193, time taken: 1.8915019035339355s\n",
      "36/174, train_loss: 19.2743, time taken: 2.0150938034057617s\n",
      "37/174, train_loss: 19.9490, time taken: 1.8096041679382324s\n",
      "38/174, train_loss: 16.0708, time taken: 1.7344577312469482s\n",
      "39/174, train_loss: 16.3058, time taken: 1.7550721168518066s\n",
      "40/174, train_loss: 14.4326, time taken: 1.7251689434051514s\n",
      "41/174, train_loss: 20.9359, time taken: 1.8673162460327148s\n",
      "42/174, train_loss: 14.0187, time taken: 1.7133851051330566s\n",
      "43/174, train_loss: 16.4194, time taken: 1.9968373775482178s\n",
      "44/174, train_loss: 10.0839, time taken: 2.0072391033172607s\n",
      "45/174, train_loss: 19.9118, time taken: 1.708418369293213s\n",
      "46/174, train_loss: 17.4703, time taken: 1.8282744884490967s\n",
      "47/174, train_loss: 15.4319, time taken: 1.9958140850067139s\n",
      "48/174, train_loss: 17.3880, time taken: 1.8759331703186035s\n",
      "49/174, train_loss: 18.2352, time taken: 1.7998881340026855s\n",
      "50/174, train_loss: 15.0036, time taken: 1.9141926765441895s\n",
      "51/174, train_loss: 18.6475, time taken: 1.7661285400390625s\n",
      "52/174, train_loss: 14.0367, time taken: 1.791778802871704s\n",
      "53/174, train_loss: 16.1712, time taken: 1.8370475769042969s\n",
      "54/174, train_loss: 13.7440, time taken: 1.8583011627197266s\n",
      "55/174, train_loss: 16.9987, time taken: 1.7249603271484375s\n",
      "56/174, train_loss: 19.0336, time taken: 2.078502655029297s\n",
      "57/174, train_loss: 15.8021, time taken: 1.9123096466064453s\n",
      "58/174, train_loss: 15.4535, time taken: 1.7414627075195312s\n",
      "59/174, train_loss: 18.3450, time taken: 1.7810709476470947s\n",
      "60/174, train_loss: 19.3991, time taken: 1.965677261352539s\n",
      "61/174, train_loss: 15.0577, time taken: 1.9346134662628174s\n",
      "62/174, train_loss: 16.3158, time taken: 1.7885487079620361s\n",
      "63/174, train_loss: 18.3934, time taken: 1.8596525192260742s\n",
      "64/174, train_loss: 23.4496, time taken: 1.8106603622436523s\n",
      "65/174, train_loss: 20.3713, time taken: 2.09686279296875s\n",
      "66/174, train_loss: 17.0990, time taken: 1.8166818618774414s\n",
      "67/174, train_loss: 20.4748, time taken: 2.2765679359436035s\n",
      "68/174, train_loss: 15.7902, time taken: 1.9675123691558838s\n",
      "69/174, train_loss: 18.5521, time taken: 1.9298467636108398s\n",
      "70/174, train_loss: 16.5615, time taken: 2.2024850845336914s\n",
      "71/174, train_loss: 17.6248, time taken: 2.099774122238159s\n",
      "72/174, train_loss: 17.6035, time taken: 1.951106071472168s\n",
      "73/174, train_loss: 14.4074, time taken: 1.7265050411224365s\n",
      "74/174, train_loss: 14.3100, time taken: 2.0846686363220215s\n",
      "75/174, train_loss: 14.9189, time taken: 1.9853205680847168s\n",
      "76/174, train_loss: 18.2767, time taken: 1.823852300643921s\n",
      "77/174, train_loss: 12.5567, time taken: 1.786161184310913s\n",
      "78/174, train_loss: 22.0758, time taken: 1.9789564609527588s\n",
      "79/174, train_loss: 17.7310, time taken: 1.8262698650360107s\n",
      "80/174, train_loss: 13.7748, time taken: 2.1960678100585938s\n",
      "81/174, train_loss: 16.6823, time taken: 1.7776644229888916s\n",
      "82/174, train_loss: 20.0907, time taken: 3.9888577461242676s\n",
      "83/174, train_loss: 14.6287, time taken: 1.915071725845337s\n",
      "84/174, train_loss: 16.2300, time taken: 2.104396104812622s\n",
      "85/174, train_loss: 15.5840, time taken: 1.9639010429382324s\n",
      "86/174, train_loss: 17.3775, time taken: 2.591491937637329s\n",
      "87/174, train_loss: 16.5020, time taken: 2.008408546447754s\n",
      "88/174, train_loss: 22.7541, time taken: 2.1846907138824463s\n",
      "89/174, train_loss: 12.8475, time taken: 1.8147199153900146s\n",
      "90/174, train_loss: 17.7905, time taken: 2.0999929904937744s\n",
      "91/174, train_loss: 15.8995, time taken: 2.1036548614501953s\n",
      "92/174, train_loss: 14.2271, time taken: 1.9729533195495605s\n",
      "93/174, train_loss: 16.0766, time taken: 1.7367103099822998s\n",
      "94/174, train_loss: 17.4730, time taken: 2.0500199794769287s\n",
      "95/174, train_loss: 18.2616, time taken: 1.8368761539459229s\n",
      "96/174, train_loss: 19.8242, time taken: 1.961108922958374s\n",
      "97/174, train_loss: 16.7504, time taken: 1.7924020290374756s\n",
      "98/174, train_loss: 18.9527, time taken: 1.8305528163909912s\n",
      "99/174, train_loss: 19.6230, time taken: 2.080244302749634s\n",
      "100/174, train_loss: 14.4849, time taken: 1.8872148990631104s\n",
      "101/174, train_loss: 15.6532, time taken: 1.8815102577209473s\n",
      "102/174, train_loss: 24.9668, time taken: 2.2133982181549072s\n",
      "103/174, train_loss: 20.8643, time taken: 2.1890273094177246s\n",
      "104/174, train_loss: 16.2654, time taken: 1.8982219696044922s\n",
      "105/174, train_loss: 17.7060, time taken: 1.918760061264038s\n",
      "106/174, train_loss: 15.2415, time taken: 1.9650731086730957s\n",
      "107/174, train_loss: 14.4588, time taken: 1.9351780414581299s\n",
      "108/174, train_loss: 16.7884, time taken: 2.1238481998443604s\n",
      "109/174, train_loss: 18.3839, time taken: 1.8046247959136963s\n",
      "110/174, train_loss: 16.6035, time taken: 4.408012866973877s\n",
      "111/174, train_loss: 16.1564, time taken: 1.8424971103668213s\n",
      "112/174, train_loss: 14.5081, time taken: 2.1839818954467773s\n",
      "113/174, train_loss: 12.9505, time taken: 1.9029731750488281s\n",
      "114/174, train_loss: 19.1221, time taken: 1.8814363479614258s\n",
      "115/174, train_loss: 13.2800, time taken: 1.7984704971313477s\n",
      "116/174, train_loss: 19.7468, time taken: 1.8072960376739502s\n",
      "117/174, train_loss: 20.5832, time taken: 1.7923085689544678s\n",
      "118/174, train_loss: 17.8256, time taken: 1.8100998401641846s\n",
      "119/174, train_loss: 18.7528, time taken: 2.1016273498535156s\n",
      "120/174, train_loss: 19.3855, time taken: 1.7322325706481934s\n",
      "121/174, train_loss: 18.3126, time taken: 1.8087942600250244s\n",
      "122/174, train_loss: 17.1573, time taken: 2.301825523376465s\n",
      "123/174, train_loss: 17.4752, time taken: 2.0186967849731445s\n",
      "124/174, train_loss: 17.0723, time taken: 1.8806955814361572s\n",
      "125/174, train_loss: 15.2855, time taken: 1.790637731552124s\n",
      "126/174, train_loss: 18.1653, time taken: 1.8000216484069824s\n",
      "127/174, train_loss: 16.3785, time taken: 1.7017319202423096s\n",
      "128/174, train_loss: 16.7597, time taken: 2.386063814163208s\n",
      "129/174, train_loss: 15.4320, time taken: 1.9220960140228271s\n",
      "130/174, train_loss: 19.9499, time taken: 1.7760024070739746s\n",
      "131/174, train_loss: 19.8655, time taken: 2.0771937370300293s\n",
      "132/174, train_loss: 16.9702, time taken: 2.027005910873413s\n",
      "133/174, train_loss: 16.1493, time taken: 1.8226535320281982s\n",
      "134/174, train_loss: 19.8759, time taken: 2.000673770904541s\n",
      "135/174, train_loss: 16.0749, time taken: 1.7975382804870605s\n",
      "136/174, train_loss: 18.3616, time taken: 1.778644323348999s\n",
      "137/174, train_loss: 16.2747, time taken: 1.996288776397705s\n",
      "138/174, train_loss: 15.0273, time taken: 1.9944283962249756s\n",
      "139/174, train_loss: 15.8789, time taken: 2.2248423099517822s\n",
      "140/174, train_loss: 15.9275, time taken: 1.9657886028289795s\n",
      "141/174, train_loss: 18.8500, time taken: 1.7216415405273438s\n",
      "142/174, train_loss: 19.4042, time taken: 1.905785322189331s\n",
      "143/174, train_loss: 21.6273, time taken: 1.9774982929229736s\n",
      "144/174, train_loss: 17.7835, time taken: 3.802751302719116s\n",
      "145/174, train_loss: 13.7765, time taken: 2.0057191848754883s\n",
      "146/174, train_loss: 19.5941, time taken: 1.8649981021881104s\n",
      "147/174, train_loss: 15.5972, time taken: 1.7203352451324463s\n",
      "148/174, train_loss: 20.0683, time taken: 2.164759635925293s\n",
      "149/174, train_loss: 17.1961, time taken: 1.8126177787780762s\n",
      "150/174, train_loss: 12.4811, time taken: 1.9764676094055176s\n",
      "151/174, train_loss: 16.1988, time taken: 2.0264930725097656s\n",
      "152/174, train_loss: 16.8359, time taken: 2.0707404613494873s\n",
      "153/174, train_loss: 17.5277, time taken: 1.9058504104614258s\n",
      "154/174, train_loss: 20.7373, time taken: 1.7999377250671387s\n",
      "155/174, train_loss: 18.4896, time taken: 1.884258508682251s\n",
      "156/174, train_loss: 20.5251, time taken: 1.806511402130127s\n",
      "157/174, train_loss: 18.8655, time taken: 1.806654930114746s\n",
      "158/174, train_loss: 11.8569, time taken: 2.2957894802093506s\n",
      "159/174, train_loss: 15.4016, time taken: 2.0849523544311523s\n",
      "160/174, train_loss: 20.7305, time taken: 2.2049190998077393s\n",
      "161/174, train_loss: 16.2390, time taken: 1.694793701171875s\n",
      "162/174, train_loss: 16.5078, time taken: 1.6931705474853516s\n",
      "163/174, train_loss: 16.6778, time taken: 1.7938933372497559s\n",
      "164/174, train_loss: 14.7329, time taken: 1.9982004165649414s\n",
      "165/174, train_loss: 16.8398, time taken: 1.8092591762542725s\n",
      "166/174, train_loss: 16.6179, time taken: 1.7702183723449707s\n",
      "167/174, train_loss: 18.6653, time taken: 1.8106129169464111s\n",
      "168/174, train_loss: 22.3230, time taken: 1.7166059017181396s\n",
      "169/174, train_loss: 13.0403, time taken: 1.8575916290283203s\n",
      "170/174, train_loss: 15.6669, time taken: 1.798154354095459s\n",
      "171/174, train_loss: 18.9224, time taken: 1.6863293647766113s\n",
      "172/174, train_loss: 18.1674, time taken: 1.8004605770111084s\n",
      "173/174, train_loss: 17.8950, time taken: 1.8186538219451904s\n",
      "174/174, train_loss: 13.7686, time taken: 1.7867107391357422s\n",
      "175/174, train_loss: 19.5888, time taken: 1.2829177379608154s\n",
      "epoch 6 average loss: 17.2644\n",
      "----------\n",
      "epoch 7/2000\n",
      "1/174, train_loss: 16.9209, time taken: 8.024217128753662s\n",
      "2/174, train_loss: 17.7949, time taken: 4.130140542984009s\n",
      "3/174, train_loss: 14.2217, time taken: 1.9825615882873535s\n",
      "4/174, train_loss: 16.1906, time taken: 2.3181369304656982s\n",
      "5/174, train_loss: 16.8582, time taken: 1.8148167133331299s\n",
      "6/174, train_loss: 16.6988, time taken: 1.9918434619903564s\n",
      "7/174, train_loss: 17.1567, time taken: 1.8994998931884766s\n",
      "8/174, train_loss: 18.5561, time taken: 1.9926097393035889s\n",
      "9/174, train_loss: 16.5311, time taken: 2.000533103942871s\n",
      "10/174, train_loss: 20.0368, time taken: 1.8248095512390137s\n",
      "11/174, train_loss: 23.6077, time taken: 2.058908700942993s\n",
      "12/174, train_loss: 28.2241, time taken: 1.811326026916504s\n",
      "13/174, train_loss: 19.3436, time taken: 1.8910386562347412s\n",
      "14/174, train_loss: 15.8666, time taken: 1.7994983196258545s\n",
      "15/174, train_loss: 12.8106, time taken: 2.282682180404663s\n",
      "16/174, train_loss: 18.3597, time taken: 2.3118138313293457s\n",
      "17/174, train_loss: 13.6692, time taken: 1.976884126663208s\n",
      "18/174, train_loss: 16.8079, time taken: 2.499800205230713s\n",
      "19/174, train_loss: 15.6585, time taken: 1.9217143058776855s\n",
      "20/174, train_loss: 17.0931, time taken: 1.7862765789031982s\n",
      "21/174, train_loss: 20.4634, time taken: 1.7873516082763672s\n",
      "22/174, train_loss: 17.7720, time taken: 1.9960393905639648s\n",
      "23/174, train_loss: 17.5149, time taken: 1.9239976406097412s\n",
      "24/174, train_loss: 16.6452, time taken: 1.8265645503997803s\n",
      "25/174, train_loss: 18.2964, time taken: 1.905193567276001s\n",
      "26/174, train_loss: 18.6108, time taken: 1.7863092422485352s\n",
      "27/174, train_loss: 16.3154, time taken: 1.7968823909759521s\n",
      "28/174, train_loss: 18.4459, time taken: 1.9978797435760498s\n",
      "29/174, train_loss: 20.1070, time taken: 1.7140674591064453s\n",
      "30/174, train_loss: 19.9682, time taken: 1.7796545028686523s\n",
      "31/174, train_loss: 19.8273, time taken: 1.799583911895752s\n",
      "32/174, train_loss: 17.4707, time taken: 1.7898280620574951s\n",
      "33/174, train_loss: 19.7517, time taken: 1.9025416374206543s\n",
      "34/174, train_loss: 15.7267, time taken: 1.8941326141357422s\n",
      "35/174, train_loss: 18.8910, time taken: 1.8996951580047607s\n",
      "36/174, train_loss: 14.3569, time taken: 2.1940274238586426s\n",
      "37/174, train_loss: 20.8054, time taken: 1.808488130569458s\n",
      "38/174, train_loss: 19.0054, time taken: 1.6935906410217285s\n",
      "39/174, train_loss: 19.5590, time taken: 1.7203214168548584s\n",
      "40/174, train_loss: 17.2921, time taken: 1.9119129180908203s\n",
      "41/174, train_loss: 12.5933, time taken: 1.8815889358520508s\n",
      "42/174, train_loss: 16.2040, time taken: 1.886378526687622s\n",
      "43/174, train_loss: 16.8138, time taken: 2.1068389415740967s\n",
      "44/174, train_loss: 19.6517, time taken: 2.087782621383667s\n",
      "45/174, train_loss: 18.3779, time taken: 1.925309419631958s\n",
      "46/174, train_loss: 15.9822, time taken: 2.091602087020874s\n",
      "47/174, train_loss: 21.1087, time taken: 3.6691784858703613s\n",
      "48/174, train_loss: 18.1438, time taken: 1.9061732292175293s\n",
      "49/174, train_loss: 18.8024, time taken: 1.8978347778320312s\n",
      "50/174, train_loss: 17.5948, time taken: 1.872753381729126s\n",
      "51/174, train_loss: 19.2885, time taken: 1.8073704242706299s\n",
      "52/174, train_loss: 16.3984, time taken: 1.9776957035064697s\n",
      "53/174, train_loss: 20.1183, time taken: 1.903376579284668s\n",
      "54/174, train_loss: 14.3201, time taken: 1.8182446956634521s\n",
      "55/174, train_loss: 18.9536, time taken: 1.9077332019805908s\n",
      "56/174, train_loss: 18.4097, time taken: 2.0935847759246826s\n",
      "57/174, train_loss: 18.9047, time taken: 1.7784991264343262s\n",
      "58/174, train_loss: 17.5272, time taken: 1.70829176902771s\n",
      "59/174, train_loss: 19.7579, time taken: 1.8894102573394775s\n",
      "60/174, train_loss: 16.6575, time taken: 2.082559823989868s\n",
      "61/174, train_loss: 15.2064, time taken: 1.7229340076446533s\n",
      "62/174, train_loss: 21.4847, time taken: 2.3665542602539062s\n",
      "63/174, train_loss: 23.0219, time taken: 1.7921252250671387s\n",
      "64/174, train_loss: 17.8836, time taken: 1.9223101139068604s\n",
      "65/174, train_loss: 16.2647, time taken: 1.7862684726715088s\n",
      "66/174, train_loss: 20.1585, time taken: 1.8760435581207275s\n",
      "67/174, train_loss: 15.2804, time taken: 1.922107458114624s\n",
      "68/174, train_loss: 21.4895, time taken: 1.971545696258545s\n",
      "69/174, train_loss: 16.5729, time taken: 1.9955534934997559s\n",
      "70/174, train_loss: 17.4484, time taken: 1.7998104095458984s\n",
      "71/174, train_loss: 17.7129, time taken: 1.7947664260864258s\n",
      "72/174, train_loss: 17.8502, time taken: 1.897789716720581s\n",
      "73/174, train_loss: 17.8193, time taken: 2.005289077758789s\n",
      "74/174, train_loss: 16.6050, time taken: 1.7946648597717285s\n",
      "75/174, train_loss: 18.5158, time taken: 2.0194506645202637s\n",
      "76/174, train_loss: 19.2156, time taken: 1.7678582668304443s\n",
      "77/174, train_loss: 22.0661, time taken: 3.7149770259857178s\n",
      "78/174, train_loss: 15.9254, time taken: 1.8938817977905273s\n",
      "79/174, train_loss: 17.5541, time taken: 1.9787383079528809s\n",
      "80/174, train_loss: 17.8802, time taken: 2.0169715881347656s\n",
      "81/174, train_loss: 21.5380, time taken: 2.087998867034912s\n",
      "82/174, train_loss: 20.0720, time taken: 1.7707793712615967s\n",
      "83/174, train_loss: 14.2409, time taken: 2.61977219581604s\n",
      "84/174, train_loss: 18.7035, time taken: 1.9894351959228516s\n",
      "85/174, train_loss: 16.6283, time taken: 1.8081235885620117s\n",
      "86/174, train_loss: 19.1585, time taken: 1.8730661869049072s\n",
      "87/174, train_loss: 19.7418, time taken: 2.1275763511657715s\n",
      "88/174, train_loss: 20.2177, time taken: 1.7811872959136963s\n",
      "89/174, train_loss: 12.0203, time taken: 1.7775683403015137s\n",
      "90/174, train_loss: 19.6758, time taken: 1.7140758037567139s\n",
      "91/174, train_loss: 16.1472, time taken: 2.197685718536377s\n",
      "92/174, train_loss: 17.2927, time taken: 1.8988885879516602s\n",
      "93/174, train_loss: 13.4893, time taken: 1.8968548774719238s\n",
      "94/174, train_loss: 19.2498, time taken: 1.9978458881378174s\n",
      "95/174, train_loss: 15.9062, time taken: 2.0773701667785645s\n",
      "96/174, train_loss: 15.5242, time taken: 1.8255507946014404s\n",
      "97/174, train_loss: 20.1706, time taken: 1.9024195671081543s\n",
      "98/174, train_loss: 16.5866, time taken: 1.8928728103637695s\n",
      "99/174, train_loss: 12.2211, time taken: 2.0814132690429688s\n",
      "100/174, train_loss: 16.3478, time taken: 1.9151420593261719s\n",
      "101/174, train_loss: 14.3178, time taken: 1.8058276176452637s\n",
      "102/174, train_loss: 24.6165, time taken: 2.000556707382202s\n",
      "103/174, train_loss: 17.8904, time taken: 1.8896992206573486s\n",
      "104/174, train_loss: 16.2251, time taken: 1.9191935062408447s\n",
      "105/174, train_loss: 12.0020, time taken: 1.7975575923919678s\n",
      "106/174, train_loss: 15.9366, time taken: 1.8040406703948975s\n",
      "107/174, train_loss: 18.2751, time taken: 1.7846336364746094s\n",
      "108/174, train_loss: 14.7682, time taken: 1.803131341934204s\n",
      "109/174, train_loss: 18.7921, time taken: 1.696836233139038s\n",
      "110/174, train_loss: 17.5892, time taken: 1.8648133277893066s\n",
      "111/174, train_loss: 18.1037, time taken: 1.8076167106628418s\n",
      "112/174, train_loss: 16.2584, time taken: 1.793269395828247s\n",
      "113/174, train_loss: 18.1489, time taken: 1.989424228668213s\n",
      "114/174, train_loss: 13.9927, time taken: 1.7926440238952637s\n",
      "115/174, train_loss: 22.8710, time taken: 1.7380123138427734s\n",
      "116/174, train_loss: 20.3772, time taken: 1.8030917644500732s\n",
      "117/174, train_loss: 16.3414, time taken: 1.7311749458312988s\n",
      "118/174, train_loss: 15.6011, time taken: 1.7216408252716064s\n",
      "119/174, train_loss: 18.2068, time taken: 1.7072253227233887s\n",
      "120/174, train_loss: 15.1946, time taken: 3.3802080154418945s\n",
      "121/174, train_loss: 13.8474, time taken: 1.793382167816162s\n",
      "122/174, train_loss: 17.7976, time taken: 2.0145692825317383s\n",
      "123/174, train_loss: 22.2885, time taken: 1.8613917827606201s\n",
      "124/174, train_loss: 22.2829, time taken: 1.9238688945770264s\n",
      "125/174, train_loss: 21.1957, time taken: 1.9008984565734863s\n",
      "126/174, train_loss: 21.6093, time taken: 1.789198398590088s\n",
      "127/174, train_loss: 16.2078, time taken: 1.9871416091918945s\n",
      "128/174, train_loss: 16.9148, time taken: 1.787031888961792s\n",
      "129/174, train_loss: 16.9416, time taken: 1.7971136569976807s\n",
      "130/174, train_loss: 16.4327, time taken: 1.925781011581421s\n",
      "131/174, train_loss: 12.8844, time taken: 2.098256826400757s\n",
      "132/174, train_loss: 15.5289, time taken: 1.9794080257415771s\n",
      "133/174, train_loss: 16.7973, time taken: 1.7939255237579346s\n",
      "134/174, train_loss: 18.8622, time taken: 1.780386209487915s\n",
      "135/174, train_loss: 14.6473, time taken: 1.819777011871338s\n",
      "136/174, train_loss: 20.2233, time taken: 1.9888248443603516s\n",
      "137/174, train_loss: 20.3339, time taken: 2.2029953002929688s\n",
      "138/174, train_loss: 15.0917, time taken: 1.9910955429077148s\n",
      "139/174, train_loss: 20.9497, time taken: 1.976449966430664s\n",
      "140/174, train_loss: 15.7780, time taken: 1.9120244979858398s\n",
      "141/174, train_loss: 13.7594, time taken: 1.8819234371185303s\n",
      "142/174, train_loss: 18.4612, time taken: 1.8284502029418945s\n",
      "143/174, train_loss: 17.4033, time taken: 1.777517557144165s\n",
      "144/174, train_loss: 19.8706, time taken: 1.793879508972168s\n",
      "145/174, train_loss: 16.8649, time taken: 1.870619535446167s\n",
      "146/174, train_loss: 16.6353, time taken: 2.1266722679138184s\n",
      "147/174, train_loss: 17.2487, time taken: 1.8852758407592773s\n",
      "148/174, train_loss: 19.2832, time taken: 1.8937599658966064s\n",
      "149/174, train_loss: 22.5099, time taken: 2.006385087966919s\n",
      "150/174, train_loss: 15.4891, time taken: 1.9861657619476318s\n",
      "151/174, train_loss: 19.6921, time taken: 1.7150254249572754s\n",
      "152/174, train_loss: 14.3997, time taken: 1.865142583847046s\n",
      "153/174, train_loss: 21.3792, time taken: 1.8325071334838867s\n",
      "154/174, train_loss: 19.0698, time taken: 1.9672229290008545s\n",
      "155/174, train_loss: 18.0126, time taken: 1.8920924663543701s\n",
      "156/174, train_loss: 21.0999, time taken: 1.714811086654663s\n",
      "157/174, train_loss: 17.4493, time taken: 1.8763065338134766s\n",
      "158/174, train_loss: 14.0895, time taken: 2.407294750213623s\n",
      "159/174, train_loss: 19.4784, time taken: 1.6981713771820068s\n",
      "160/174, train_loss: 16.3174, time taken: 2.3112001419067383s\n",
      "161/174, train_loss: 17.2492, time taken: 2.0764079093933105s\n",
      "162/174, train_loss: 17.8595, time taken: 1.7819533348083496s\n",
      "163/174, train_loss: 13.3051, time taken: 1.8134853839874268s\n",
      "164/174, train_loss: 22.4453, time taken: 2.296781539916992s\n",
      "165/174, train_loss: 19.3118, time taken: 2.3110995292663574s\n",
      "166/174, train_loss: 16.8080, time taken: 1.8839709758758545s\n",
      "167/174, train_loss: 16.3554, time taken: 2.013984203338623s\n",
      "168/174, train_loss: 16.5357, time taken: 1.8381807804107666s\n",
      "169/174, train_loss: 15.2670, time taken: 1.775031328201294s\n",
      "170/174, train_loss: 13.3364, time taken: 1.988917350769043s\n",
      "171/174, train_loss: 16.8988, time taken: 1.9827992916107178s\n",
      "172/174, train_loss: 21.5614, time taken: 2.0349509716033936s\n",
      "173/174, train_loss: 15.5985, time taken: 1.8813793659210205s\n",
      "174/174, train_loss: 19.4128, time taken: 1.9945576190948486s\n",
      "175/174, train_loss: 17.3248, time taken: 1.3936362266540527s\n",
      "epoch 7 average loss: 17.7465\n",
      "Entering Validation for epoch: 7\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 7 Validation avg loss: 13.6288, time taken: 1.0682909488677979s\n",
      "----------\n",
      "epoch 8/2000\n",
      "1/174, train_loss: 18.1091, time taken: 7.709779262542725s\n",
      "2/174, train_loss: 16.3186, time taken: 2.0096116065979004s\n",
      "3/174, train_loss: 13.7573, time taken: 1.8251492977142334s\n",
      "4/174, train_loss: 19.2881, time taken: 1.8928394317626953s\n",
      "5/174, train_loss: 15.1094, time taken: 2.071120262145996s\n",
      "6/174, train_loss: 26.7989, time taken: 1.7169477939605713s\n",
      "7/174, train_loss: 18.7473, time taken: 1.6734883785247803s\n",
      "8/174, train_loss: 21.5542, time taken: 1.7976789474487305s\n",
      "9/174, train_loss: 19.0940, time taken: 1.78751802444458s\n",
      "10/174, train_loss: 18.9416, time taken: 1.899986743927002s\n",
      "11/174, train_loss: 17.4870, time taken: 1.9067437648773193s\n",
      "12/174, train_loss: 18.2703, time taken: 1.782411813735962s\n",
      "13/174, train_loss: 17.4431, time taken: 1.7167649269104004s\n",
      "14/174, train_loss: 17.8774, time taken: 1.9047749042510986s\n",
      "15/174, train_loss: 15.2018, time taken: 1.9650869369506836s\n",
      "16/174, train_loss: 23.0855, time taken: 1.7970497608184814s\n",
      "17/174, train_loss: 18.2770, time taken: 1.699113130569458s\n",
      "18/174, train_loss: 16.4135, time taken: 2.014714479446411s\n",
      "19/174, train_loss: 15.2665, time taken: 2.0724074840545654s\n",
      "20/174, train_loss: 14.8549, time taken: 1.818159580230713s\n",
      "21/174, train_loss: 12.7407, time taken: 1.782503604888916s\n",
      "22/174, train_loss: 15.5470, time taken: 1.7946877479553223s\n",
      "23/174, train_loss: 21.6883, time taken: 1.8036472797393799s\n",
      "24/174, train_loss: 18.6312, time taken: 1.717437982559204s\n",
      "25/174, train_loss: 17.9295, time taken: 1.7723097801208496s\n",
      "26/174, train_loss: 16.8109, time taken: 1.8911809921264648s\n",
      "27/174, train_loss: 22.8722, time taken: 1.7332978248596191s\n",
      "28/174, train_loss: 21.5107, time taken: 1.7757556438446045s\n",
      "29/174, train_loss: 18.6228, time taken: 2.005021572113037s\n",
      "30/174, train_loss: 15.3740, time taken: 1.7973027229309082s\n",
      "31/174, train_loss: 24.4311, time taken: 1.9986982345581055s\n",
      "32/174, train_loss: 20.9786, time taken: 2.0797317028045654s\n",
      "33/174, train_loss: 20.1424, time taken: 1.8003544807434082s\n",
      "34/174, train_loss: 17.4249, time taken: 1.7146906852722168s\n",
      "35/174, train_loss: 14.3414, time taken: 4.186908483505249s\n",
      "36/174, train_loss: 16.0406, time taken: 1.902087926864624s\n",
      "37/174, train_loss: 15.3361, time taken: 2.381671667098999s\n",
      "38/174, train_loss: 17.6876, time taken: 1.7828450202941895s\n",
      "39/174, train_loss: 19.1226, time taken: 2.288815975189209s\n",
      "40/174, train_loss: 16.8831, time taken: 1.8154032230377197s\n",
      "41/174, train_loss: 14.7942, time taken: 2.0044479370117188s\n",
      "42/174, train_loss: 22.3880, time taken: 2.290597438812256s\n",
      "43/174, train_loss: 23.9512, time taken: 1.8837523460388184s\n",
      "44/174, train_loss: 14.6667, time taken: 1.9068355560302734s\n",
      "45/174, train_loss: 24.0295, time taken: 1.7905089855194092s\n",
      "46/174, train_loss: 19.3938, time taken: 1.8149187564849854s\n",
      "47/174, train_loss: 20.0997, time taken: 2.1037380695343018s\n",
      "48/174, train_loss: 17.8326, time taken: 2.154271364212036s\n",
      "49/174, train_loss: 17.9066, time taken: 1.9066805839538574s\n",
      "50/174, train_loss: 12.5898, time taken: 2.0091235637664795s\n",
      "51/174, train_loss: 18.8724, time taken: 2.5070300102233887s\n",
      "52/174, train_loss: 18.2502, time taken: 1.8719329833984375s\n",
      "53/174, train_loss: 16.6694, time taken: 1.9271252155303955s\n",
      "54/174, train_loss: 14.0425, time taken: 2.0879757404327393s\n",
      "55/174, train_loss: 14.7273, time taken: 2.0827560424804688s\n",
      "56/174, train_loss: 12.2244, time taken: 2.00492000579834s\n",
      "57/174, train_loss: 17.7154, time taken: 1.8072354793548584s\n",
      "58/174, train_loss: 19.3474, time taken: 1.8917651176452637s\n",
      "59/174, train_loss: 16.8737, time taken: 1.8986635208129883s\n",
      "60/174, train_loss: 12.9423, time taken: 2.0940072536468506s\n",
      "61/174, train_loss: 14.8977, time taken: 1.7992792129516602s\n",
      "62/174, train_loss: 15.7002, time taken: 2.104473114013672s\n",
      "63/174, train_loss: 16.7499, time taken: 1.7252564430236816s\n",
      "64/174, train_loss: 15.6540, time taken: 1.9702332019805908s\n",
      "65/174, train_loss: 21.8220, time taken: 1.8964273929595947s\n",
      "66/174, train_loss: 20.0233, time taken: 1.906184434890747s\n",
      "67/174, train_loss: 18.3720, time taken: 2.023293972015381s\n",
      "68/174, train_loss: 12.5737, time taken: 1.812375545501709s\n",
      "69/174, train_loss: 15.3505, time taken: 1.9004034996032715s\n",
      "70/174, train_loss: 24.2479, time taken: 1.8100006580352783s\n",
      "71/174, train_loss: 14.9484, time taken: 1.9835560321807861s\n",
      "72/174, train_loss: 16.7624, time taken: 1.8959178924560547s\n",
      "73/174, train_loss: 15.9212, time taken: 2.11222243309021s\n",
      "74/174, train_loss: 15.4050, time taken: 2.1002182960510254s\n",
      "75/174, train_loss: 15.0572, time taken: 1.9636895656585693s\n",
      "76/174, train_loss: 17.1305, time taken: 1.9244890213012695s\n",
      "77/174, train_loss: 16.1924, time taken: 1.9999234676361084s\n",
      "78/174, train_loss: 16.9714, time taken: 1.871915340423584s\n",
      "79/174, train_loss: 20.8755, time taken: 1.893127202987671s\n",
      "80/174, train_loss: 21.3732, time taken: 1.9256415367126465s\n",
      "81/174, train_loss: 15.7507, time taken: 1.881950855255127s\n",
      "82/174, train_loss: 15.2141, time taken: 2.0017571449279785s\n",
      "83/174, train_loss: 15.9319, time taken: 1.9938194751739502s\n",
      "84/174, train_loss: 18.7463, time taken: 2.2923498153686523s\n",
      "85/174, train_loss: 17.2957, time taken: 2.2162303924560547s\n",
      "86/174, train_loss: 16.5143, time taken: 2.060335874557495s\n",
      "87/174, train_loss: 17.9854, time taken: 1.8988139629364014s\n",
      "88/174, train_loss: 15.0210, time taken: 2.0253536701202393s\n",
      "89/174, train_loss: 17.2782, time taken: 1.9956660270690918s\n",
      "90/174, train_loss: 23.4354, time taken: 1.9833991527557373s\n",
      "91/174, train_loss: 17.1818, time taken: 2.0073983669281006s\n",
      "92/174, train_loss: 20.1488, time taken: 1.985327959060669s\n",
      "93/174, train_loss: 14.6915, time taken: 1.8903918266296387s\n",
      "94/174, train_loss: 18.6880, time taken: 2.229421377182007s\n",
      "95/174, train_loss: 16.1107, time taken: 2.0327844619750977s\n",
      "96/174, train_loss: 15.1743, time taken: 1.9731433391571045s\n",
      "97/174, train_loss: 15.3474, time taken: 1.9156782627105713s\n",
      "98/174, train_loss: 17.7948, time taken: 3.890472173690796s\n",
      "99/174, train_loss: 18.1137, time taken: 2.4818291664123535s\n",
      "100/174, train_loss: 17.9150, time taken: 2.087066411972046s\n",
      "101/174, train_loss: 13.5796, time taken: 2.0387306213378906s\n",
      "102/174, train_loss: 17.2045, time taken: 1.8121635913848877s\n",
      "103/174, train_loss: 14.4672, time taken: 1.829111099243164s\n",
      "104/174, train_loss: 20.9911, time taken: 2.0850446224212646s\n",
      "105/174, train_loss: 16.1789, time taken: 1.78383469581604s\n",
      "106/174, train_loss: 16.7046, time taken: 1.795769214630127s\n",
      "107/174, train_loss: 21.9919, time taken: 1.7951078414916992s\n",
      "108/174, train_loss: 18.6886, time taken: 1.8133995532989502s\n",
      "109/174, train_loss: 17.2281, time taken: 1.9872827529907227s\n",
      "110/174, train_loss: 13.4884, time taken: 2.0791738033294678s\n",
      "111/174, train_loss: 15.1301, time taken: 2.1309266090393066s\n",
      "112/174, train_loss: 15.6807, time taken: 2.0307066440582275s\n",
      "113/174, train_loss: 19.3465, time taken: 2.3681480884552s\n",
      "114/174, train_loss: 18.1337, time taken: 1.8029873371124268s\n",
      "115/174, train_loss: 15.6690, time taken: 1.8309683799743652s\n",
      "116/174, train_loss: 19.0232, time taken: 1.8187577724456787s\n",
      "117/174, train_loss: 21.4392, time taken: 1.920276403427124s\n",
      "118/174, train_loss: 16.8960, time taken: 1.7787537574768066s\n",
      "119/174, train_loss: 15.2016, time taken: 1.6914143562316895s\n",
      "120/174, train_loss: 18.4785, time taken: 1.8754222393035889s\n",
      "121/174, train_loss: 16.5883, time taken: 1.7235667705535889s\n",
      "122/174, train_loss: 24.5891, time taken: 1.7729973793029785s\n",
      "123/174, train_loss: 18.3806, time taken: 1.7298223972320557s\n",
      "124/174, train_loss: 17.7113, time taken: 1.7954320907592773s\n",
      "125/174, train_loss: 17.9702, time taken: 2.025644540786743s\n",
      "126/174, train_loss: 18.1691, time taken: 1.8890774250030518s\n",
      "127/174, train_loss: 20.5996, time taken: 1.80259370803833s\n",
      "128/174, train_loss: 15.3422, time taken: 1.873840093612671s\n",
      "129/174, train_loss: 24.6275, time taken: 2.0185623168945312s\n",
      "130/174, train_loss: 20.1351, time taken: 1.7925946712493896s\n",
      "131/174, train_loss: 14.9107, time taken: 1.7823734283447266s\n",
      "132/174, train_loss: 23.5438, time taken: 2.001628875732422s\n",
      "133/174, train_loss: 16.4771, time taken: 1.9854521751403809s\n",
      "134/174, train_loss: 14.8584, time taken: 3.818408489227295s\n",
      "135/174, train_loss: 16.1228, time taken: 1.7908267974853516s\n",
      "136/174, train_loss: 16.4293, time taken: 2.6917784214019775s\n",
      "137/174, train_loss: 16.4872, time taken: 1.8863732814788818s\n",
      "138/174, train_loss: 14.4852, time taken: 1.993607759475708s\n",
      "139/174, train_loss: 12.9104, time taken: 1.9250860214233398s\n",
      "140/174, train_loss: 13.1557, time taken: 1.7921116352081299s\n",
      "141/174, train_loss: 16.8364, time taken: 2.091643810272217s\n",
      "142/174, train_loss: 23.1577, time taken: 1.9749031066894531s\n",
      "143/174, train_loss: 14.2884, time taken: 2.12410569190979s\n",
      "144/174, train_loss: 15.2952, time taken: 1.9047741889953613s\n",
      "145/174, train_loss: 14.8644, time taken: 1.87786865234375s\n",
      "146/174, train_loss: 20.4047, time taken: 2.019570827484131s\n",
      "147/174, train_loss: 16.5824, time taken: 1.8946027755737305s\n",
      "148/174, train_loss: 18.4935, time taken: 1.766054630279541s\n",
      "149/174, train_loss: 18.0567, time taken: 2.11181640625s\n",
      "150/174, train_loss: 19.1725, time taken: 2.274402379989624s\n",
      "151/174, train_loss: 16.0627, time taken: 1.8194615840911865s\n",
      "152/174, train_loss: 14.3061, time taken: 1.9881956577301025s\n",
      "153/174, train_loss: 19.5966, time taken: 1.7910563945770264s\n",
      "154/174, train_loss: 21.0958, time taken: 1.7072176933288574s\n",
      "155/174, train_loss: 15.4917, time taken: 1.8113915920257568s\n",
      "156/174, train_loss: 17.8875, time taken: 1.7558255195617676s\n",
      "157/174, train_loss: 17.3656, time taken: 1.9340133666992188s\n",
      "158/174, train_loss: 16.4370, time taken: 1.9732508659362793s\n",
      "159/174, train_loss: 19.2475, time taken: 1.8075828552246094s\n",
      "160/174, train_loss: 12.7928, time taken: 1.9857628345489502s\n",
      "161/174, train_loss: 17.9659, time taken: 2.2008957862854004s\n",
      "162/174, train_loss: 18.1387, time taken: 2.089834690093994s\n",
      "163/174, train_loss: 18.1141, time taken: 1.8052754402160645s\n",
      "164/174, train_loss: 17.0775, time taken: 1.770618200302124s\n",
      "165/174, train_loss: 17.3956, time taken: 1.8977293968200684s\n",
      "166/174, train_loss: 17.4804, time taken: 1.9178078174591064s\n",
      "167/174, train_loss: 15.8803, time taken: 1.890427827835083s\n",
      "168/174, train_loss: 17.5967, time taken: 1.8157618045806885s\n",
      "169/174, train_loss: 18.8651, time taken: 1.7717242240905762s\n",
      "170/174, train_loss: 17.0442, time taken: 1.8978908061981201s\n",
      "171/174, train_loss: 16.5470, time taken: 1.7315068244934082s\n",
      "172/174, train_loss: 14.6566, time taken: 1.834707498550415s\n",
      "173/174, train_loss: 21.5131, time taken: 1.9732391834259033s\n",
      "174/174, train_loss: 14.1563, time taken: 1.9168078899383545s\n",
      "175/174, train_loss: 15.7800, time taken: 1.3675568103790283s\n",
      "epoch 8 average loss: 17.5449\n",
      "----------\n",
      "epoch 9/2000\n",
      "1/174, train_loss: 15.1685, time taken: 7.9295899868011475s\n",
      "2/174, train_loss: 18.4499, time taken: 2.39558482170105s\n",
      "3/174, train_loss: 16.3092, time taken: 1.8259217739105225s\n",
      "4/174, train_loss: 16.8244, time taken: 2.1850905418395996s\n",
      "5/174, train_loss: 16.1573, time taken: 1.8838145732879639s\n",
      "6/174, train_loss: 16.8146, time taken: 1.876467227935791s\n",
      "7/174, train_loss: 17.0152, time taken: 2.027611494064331s\n",
      "8/174, train_loss: 17.8242, time taken: 1.9668021202087402s\n",
      "9/174, train_loss: 15.8404, time taken: 2.20631742477417s\n",
      "10/174, train_loss: 16.9249, time taken: 1.8224682807922363s\n",
      "11/174, train_loss: 14.4473, time taken: 2.080409526824951s\n",
      "12/174, train_loss: 17.2149, time taken: 1.8222591876983643s\n",
      "13/174, train_loss: 17.1830, time taken: 2.3786442279815674s\n",
      "14/174, train_loss: 12.2103, time taken: 2.5093564987182617s\n",
      "15/174, train_loss: 17.1795, time taken: 2.002134084701538s\n",
      "16/174, train_loss: 22.1484, time taken: 2.113239288330078s\n",
      "17/174, train_loss: 17.5828, time taken: 1.9042298793792725s\n",
      "18/174, train_loss: 21.0547, time taken: 1.8718900680541992s\n",
      "19/174, train_loss: 17.1857, time taken: 1.917938470840454s\n",
      "20/174, train_loss: 22.1237, time taken: 1.7945926189422607s\n",
      "21/174, train_loss: 15.8781, time taken: 1.9806699752807617s\n",
      "22/174, train_loss: 17.0975, time taken: 1.8129732608795166s\n",
      "23/174, train_loss: 19.1674, time taken: 1.787888526916504s\n",
      "24/174, train_loss: 17.4878, time taken: 1.8052549362182617s\n",
      "25/174, train_loss: 17.6458, time taken: 2.0064473152160645s\n",
      "26/174, train_loss: 17.3432, time taken: 2.0149500370025635s\n",
      "27/174, train_loss: 18.5060, time taken: 2.039093017578125s\n",
      "28/174, train_loss: 18.4603, time taken: 1.863771677017212s\n",
      "29/174, train_loss: 16.9311, time taken: 2.313952684402466s\n",
      "30/174, train_loss: 14.9365, time taken: 1.7789857387542725s\n",
      "31/174, train_loss: 19.5348, time taken: 1.904588222503662s\n",
      "32/174, train_loss: 15.3940, time taken: 1.8151121139526367s\n",
      "33/174, train_loss: 16.3209, time taken: 1.8803718090057373s\n",
      "34/174, train_loss: 22.2241, time taken: 1.695713758468628s\n",
      "35/174, train_loss: 17.5966, time taken: 1.7939770221710205s\n",
      "36/174, train_loss: 17.0133, time taken: 1.7959742546081543s\n",
      "37/174, train_loss: 16.7237, time taken: 1.9891729354858398s\n",
      "38/174, train_loss: 18.4502, time taken: 1.79221510887146s\n",
      "39/174, train_loss: 13.4759, time taken: 1.8139257431030273s\n",
      "40/174, train_loss: 24.5053, time taken: 2.26896071434021s\n",
      "41/174, train_loss: 17.0486, time taken: 1.9098196029663086s\n",
      "42/174, train_loss: 19.6967, time taken: 1.9053668975830078s\n",
      "43/174, train_loss: 15.1439, time taken: 2.482815980911255s\n",
      "44/174, train_loss: 20.0005, time taken: 1.9930520057678223s\n",
      "45/174, train_loss: 24.6479, time taken: 2.0979268550872803s\n",
      "46/174, train_loss: 17.6714, time taken: 1.8178033828735352s\n",
      "47/174, train_loss: 16.2957, time taken: 2.3006932735443115s\n",
      "48/174, train_loss: 18.4408, time taken: 2.0735647678375244s\n",
      "49/174, train_loss: 18.2033, time taken: 2.003573179244995s\n",
      "50/174, train_loss: 17.4410, time taken: 2.028500556945801s\n",
      "51/174, train_loss: 19.0655, time taken: 2.3152177333831787s\n",
      "52/174, train_loss: 15.3409, time taken: 2.1157422065734863s\n",
      "53/174, train_loss: 19.2269, time taken: 1.8872730731964111s\n",
      "54/174, train_loss: 14.7048, time taken: 1.8980121612548828s\n",
      "55/174, train_loss: 20.6361, time taken: 2.0124337673187256s\n",
      "56/174, train_loss: 17.5307, time taken: 1.89219331741333s\n",
      "57/174, train_loss: 17.6567, time taken: 2.0799217224121094s\n",
      "58/174, train_loss: 18.7987, time taken: 1.8038520812988281s\n",
      "59/174, train_loss: 14.9178, time taken: 1.7843124866485596s\n",
      "60/174, train_loss: 15.7805, time taken: 1.8084688186645508s\n",
      "61/174, train_loss: 24.0144, time taken: 2.0723679065704346s\n",
      "62/174, train_loss: 18.7992, time taken: 1.8006346225738525s\n",
      "63/174, train_loss: 16.2116, time taken: 1.7169833183288574s\n",
      "64/174, train_loss: 17.9689, time taken: 1.778520107269287s\n",
      "65/174, train_loss: 15.4994, time taken: 1.70973801612854s\n",
      "66/174, train_loss: 15.2065, time taken: 1.886491060256958s\n",
      "67/174, train_loss: 23.7685, time taken: 1.9294490814208984s\n",
      "68/174, train_loss: 17.5706, time taken: 1.8721327781677246s\n",
      "69/174, train_loss: 16.9277, time taken: 1.9036190509796143s\n",
      "70/174, train_loss: 15.6880, time taken: 1.8699653148651123s\n",
      "71/174, train_loss: 17.2729, time taken: 1.8052895069122314s\n",
      "72/174, train_loss: 14.2955, time taken: 1.9984874725341797s\n",
      "73/174, train_loss: 24.0320, time taken: 2.2877931594848633s\n",
      "74/174, train_loss: 21.1421, time taken: 2.207380771636963s\n",
      "75/174, train_loss: 15.7733, time taken: 2.1226940155029297s\n",
      "76/174, train_loss: 15.4711, time taken: 2.0970025062561035s\n",
      "77/174, train_loss: 14.7713, time taken: 1.9929511547088623s\n",
      "78/174, train_loss: 17.6383, time taken: 1.910034418106079s\n",
      "79/174, train_loss: 21.4675, time taken: 2.2515151500701904s\n",
      "80/174, train_loss: 16.8079, time taken: 2.098830223083496s\n",
      "81/174, train_loss: 18.8505, time taken: 2.1081154346466064s\n",
      "82/174, train_loss: 20.3128, time taken: 2.2079086303710938s\n",
      "83/174, train_loss: 15.1071, time taken: 2.7089598178863525s\n",
      "84/174, train_loss: 19.9253, time taken: 2.1928229331970215s\n",
      "85/174, train_loss: 20.0237, time taken: 1.9028103351593018s\n",
      "86/174, train_loss: 22.9323, time taken: 2.056452989578247s\n",
      "87/174, train_loss: 19.6366, time taken: 1.7358853816986084s\n",
      "88/174, train_loss: 23.6475, time taken: 1.7672514915466309s\n",
      "89/174, train_loss: 15.2170, time taken: 1.7148823738098145s\n",
      "90/174, train_loss: 14.1770, time taken: 1.7094101905822754s\n",
      "91/174, train_loss: 15.0781, time taken: 1.8616101741790771s\n",
      "92/174, train_loss: 16.2698, time taken: 2.0068647861480713s\n",
      "93/174, train_loss: 11.6732, time taken: 1.783813238143921s\n",
      "94/174, train_loss: 17.6344, time taken: 2.029202938079834s\n",
      "95/174, train_loss: 16.3285, time taken: 2.272169828414917s\n",
      "96/174, train_loss: 18.2595, time taken: 1.7289235591888428s\n",
      "97/174, train_loss: 20.7975, time taken: 1.8948745727539062s\n",
      "98/174, train_loss: 18.9157, time taken: 1.8606221675872803s\n",
      "99/174, train_loss: 13.1937, time taken: 1.8252105712890625s\n",
      "100/174, train_loss: 18.4701, time taken: 1.8016667366027832s\n",
      "101/174, train_loss: 16.0722, time taken: 1.7909784317016602s\n",
      "102/174, train_loss: 14.9833, time taken: 1.8946313858032227s\n",
      "103/174, train_loss: 17.7990, time taken: 2.200822353363037s\n",
      "104/174, train_loss: 20.2148, time taken: 1.9673357009887695s\n",
      "105/174, train_loss: 18.5529, time taken: 1.7245962619781494s\n",
      "106/174, train_loss: 19.9110, time taken: 1.9764106273651123s\n",
      "107/174, train_loss: 12.4487, time taken: 2.2111012935638428s\n",
      "108/174, train_loss: 17.9849, time taken: 3.497036933898926s\n",
      "109/174, train_loss: 13.9010, time taken: 1.791541576385498s\n",
      "110/174, train_loss: 17.5254, time taken: 2.1990249156951904s\n",
      "111/174, train_loss: 18.5377, time taken: 1.9018726348876953s\n",
      "112/174, train_loss: 14.8617, time taken: 1.9824938774108887s\n",
      "113/174, train_loss: 15.3267, time taken: 1.9895102977752686s\n",
      "114/174, train_loss: 21.7105, time taken: 2.193434715270996s\n",
      "115/174, train_loss: 20.1522, time taken: 1.8966026306152344s\n",
      "116/174, train_loss: 15.5959, time taken: 1.8183960914611816s\n",
      "117/174, train_loss: 18.6894, time taken: 1.7970423698425293s\n",
      "118/174, train_loss: 21.1587, time taken: 1.8040461540222168s\n",
      "119/174, train_loss: 19.4797, time taken: 1.963634729385376s\n",
      "120/174, train_loss: 14.3387, time taken: 1.7966604232788086s\n",
      "121/174, train_loss: 13.4301, time taken: 2.026059150695801s\n",
      "122/174, train_loss: 20.5185, time taken: 1.9861781597137451s\n",
      "123/174, train_loss: 13.1122, time taken: 1.6986899375915527s\n",
      "124/174, train_loss: 17.0163, time taken: 2.259606122970581s\n",
      "125/174, train_loss: 16.0479, time taken: 1.9200634956359863s\n",
      "126/174, train_loss: 19.2122, time taken: 1.8122642040252686s\n",
      "127/174, train_loss: 21.3109, time taken: 1.8629395961761475s\n",
      "128/174, train_loss: 15.7656, time taken: 1.7233564853668213s\n",
      "129/174, train_loss: 23.8887, time taken: 1.776721715927124s\n",
      "130/174, train_loss: 14.0694, time taken: 1.7986361980438232s\n",
      "131/174, train_loss: 20.0051, time taken: 1.899590015411377s\n",
      "132/174, train_loss: 15.3784, time taken: 1.7251837253570557s\n",
      "133/174, train_loss: 15.3720, time taken: 1.833573341369629s\n",
      "134/174, train_loss: 23.8835, time taken: 1.8770761489868164s\n",
      "135/174, train_loss: 19.2510, time taken: 1.8096425533294678s\n",
      "136/174, train_loss: 22.7964, time taken: 2.091733694076538s\n",
      "137/174, train_loss: 17.0148, time taken: 1.7813284397125244s\n",
      "138/174, train_loss: 14.7269, time taken: 1.7143502235412598s\n",
      "139/174, train_loss: 19.6400, time taken: 1.9854152202606201s\n",
      "140/174, train_loss: 20.1810, time taken: 1.7210936546325684s\n",
      "141/174, train_loss: 18.5777, time taken: 1.862210988998413s\n",
      "142/174, train_loss: 17.9215, time taken: 1.9371938705444336s\n",
      "143/174, train_loss: 16.0905, time taken: 2.0811426639556885s\n",
      "144/174, train_loss: 17.7821, time taken: 1.9847922325134277s\n",
      "145/174, train_loss: 19.9684, time taken: 2.2024788856506348s\n",
      "146/174, train_loss: 17.9859, time taken: 1.7844715118408203s\n",
      "147/174, train_loss: 17.6971, time taken: 1.905381679534912s\n",
      "148/174, train_loss: 15.4256, time taken: 1.999746322631836s\n",
      "149/174, train_loss: 16.5995, time taken: 1.8057777881622314s\n",
      "150/174, train_loss: 17.0726, time taken: 1.7709102630615234s\n",
      "151/174, train_loss: 20.1915, time taken: 1.9233365058898926s\n",
      "152/174, train_loss: 12.7247, time taken: 2.396817922592163s\n",
      "153/174, train_loss: 18.9644, time taken: 1.7942941188812256s\n",
      "154/174, train_loss: 16.9842, time taken: 1.7773361206054688s\n",
      "155/174, train_loss: 16.4014, time taken: 1.826427936553955s\n",
      "156/174, train_loss: 18.2429, time taken: 1.771399736404419s\n",
      "157/174, train_loss: 26.2987, time taken: 1.8829591274261475s\n",
      "158/174, train_loss: 18.5095, time taken: 1.921492576599121s\n",
      "159/174, train_loss: 17.8497, time taken: 2.004509210586548s\n",
      "160/174, train_loss: 18.2287, time taken: 2.0910091400146484s\n",
      "161/174, train_loss: 24.4366, time taken: 1.8715834617614746s\n",
      "162/174, train_loss: 23.9873, time taken: 1.8245251178741455s\n",
      "163/174, train_loss: 14.8579, time taken: 2.096376419067383s\n",
      "164/174, train_loss: 19.0507, time taken: 1.897585153579712s\n",
      "165/174, train_loss: 19.2275, time taken: 1.9058570861816406s\n",
      "166/174, train_loss: 16.8182, time taken: 1.9587085247039795s\n",
      "167/174, train_loss: 14.3214, time taken: 2.018980026245117s\n",
      "168/174, train_loss: 15.5317, time taken: 2.189056158065796s\n",
      "169/174, train_loss: 13.3040, time taken: 1.7160754203796387s\n",
      "170/174, train_loss: 16.2921, time taken: 1.9665634632110596s\n",
      "171/174, train_loss: 19.5758, time taken: 1.899721622467041s\n",
      "172/174, train_loss: 18.2770, time taken: 1.7105016708374023s\n",
      "173/174, train_loss: 18.9011, time taken: 1.886089563369751s\n",
      "174/174, train_loss: 17.7055, time taken: 1.9140455722808838s\n",
      "175/174, train_loss: 10.9559, time taken: 1.3973212242126465s\n",
      "epoch 9 average loss: 17.7714\n",
      "Entering Validation for epoch: 9\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 9 Validation avg loss: 10.0958, time taken: 1.1461193561553955s\n",
      "Saving new model based on validation loss 10.0958\n",
      "----------\n",
      "epoch 10/2000\n",
      "1/174, train_loss: 18.9620, time taken: 7.699089050292969s\n",
      "2/174, train_loss: 18.9262, time taken: 2.0860018730163574s\n",
      "3/174, train_loss: 20.0770, time taken: 1.8959250450134277s\n",
      "4/174, train_loss: 15.7715, time taken: 1.9121623039245605s\n",
      "5/174, train_loss: 16.5470, time taken: 1.8046870231628418s\n",
      "6/174, train_loss: 18.4509, time taken: 2.0751612186431885s\n",
      "7/174, train_loss: 17.9624, time taken: 1.795790195465088s\n",
      "8/174, train_loss: 16.0994, time taken: 1.9143002033233643s\n",
      "9/174, train_loss: 13.4635, time taken: 4.024511098861694s\n",
      "10/174, train_loss: 17.8493, time taken: 1.9721949100494385s\n",
      "11/174, train_loss: 15.9081, time taken: 2.2130324840545654s\n",
      "12/174, train_loss: 22.8219, time taken: 1.805832862854004s\n",
      "13/174, train_loss: 17.6502, time taken: 2.252283811569214s\n",
      "14/174, train_loss: 18.1485, time taken: 1.9108965396881104s\n",
      "15/174, train_loss: 19.1256, time taken: 1.9247498512268066s\n",
      "16/174, train_loss: 14.2793, time taken: 2.1679067611694336s\n",
      "17/174, train_loss: 13.0425, time taken: 1.9132812023162842s\n",
      "18/174, train_loss: 12.9141, time taken: 2.4805490970611572s\n",
      "19/174, train_loss: 15.3815, time taken: 1.895223617553711s\n",
      "20/174, train_loss: 15.7240, time taken: 1.9834036827087402s\n",
      "21/174, train_loss: 15.6369, time taken: 2.005563497543335s\n",
      "22/174, train_loss: 16.5490, time taken: 1.9122185707092285s\n",
      "23/174, train_loss: 17.4898, time taken: 2.005214214324951s\n",
      "24/174, train_loss: 16.4862, time taken: 1.991415023803711s\n",
      "25/174, train_loss: 19.8375, time taken: 2.16892147064209s\n",
      "26/174, train_loss: 18.1414, time taken: 1.806471824645996s\n",
      "27/174, train_loss: 16.8503, time taken: 2.1026711463928223s\n",
      "28/174, train_loss: 17.4747, time taken: 1.8127961158752441s\n",
      "29/174, train_loss: 19.3809, time taken: 1.885601282119751s\n",
      "30/174, train_loss: 20.2291, time taken: 1.8098230361938477s\n",
      "31/174, train_loss: 20.2088, time taken: 1.7812778949737549s\n",
      "32/174, train_loss: 20.3424, time taken: 1.806882381439209s\n",
      "33/174, train_loss: 15.4763, time taken: 1.8782556056976318s\n",
      "34/174, train_loss: 17.8468, time taken: 1.7069342136383057s\n",
      "35/174, train_loss: 29.5827, time taken: 1.79862642288208s\n",
      "36/174, train_loss: 20.7196, time taken: 1.7985033988952637s\n",
      "37/174, train_loss: 18.2259, time taken: 1.7969145774841309s\n",
      "38/174, train_loss: 19.6226, time taken: 1.887944221496582s\n",
      "39/174, train_loss: 15.6476, time taken: 1.9104537963867188s\n",
      "40/174, train_loss: 20.7473, time taken: 1.9876506328582764s\n",
      "41/174, train_loss: 21.8506, time taken: 2.0146565437316895s\n",
      "42/174, train_loss: 18.6318, time taken: 1.8129701614379883s\n",
      "43/174, train_loss: 27.5643, time taken: 1.8189632892608643s\n",
      "44/174, train_loss: 22.9895, time taken: 1.7929489612579346s\n",
      "45/174, train_loss: 15.5568, time taken: 1.7792797088623047s\n",
      "46/174, train_loss: 13.7635, time taken: 1.8995742797851562s\n",
      "47/174, train_loss: 21.9961, time taken: 1.8111109733581543s\n",
      "48/174, train_loss: 21.6130, time taken: 1.6804373264312744s\n",
      "49/174, train_loss: 16.7599, time taken: 1.8179576396942139s\n",
      "50/174, train_loss: 22.8848, time taken: 1.9791295528411865s\n",
      "51/174, train_loss: 16.9554, time taken: 1.7994513511657715s\n",
      "52/174, train_loss: 18.4338, time taken: 1.6982734203338623s\n",
      "53/174, train_loss: 15.8229, time taken: 1.7907462120056152s\n",
      "54/174, train_loss: 16.3042, time taken: 2.0941193103790283s\n",
      "55/174, train_loss: 18.9097, time taken: 1.7873380184173584s\n",
      "56/174, train_loss: 14.9882, time taken: 1.7170000076293945s\n",
      "57/174, train_loss: 15.1054, time taken: 1.9710257053375244s\n",
      "58/174, train_loss: 19.0625, time taken: 1.915161371231079s\n",
      "59/174, train_loss: 14.3957, time taken: 2.0103232860565186s\n",
      "60/174, train_loss: 16.2938, time taken: 1.8941090106964111s\n",
      "61/174, train_loss: 17.3579, time taken: 2.4799857139587402s\n",
      "62/174, train_loss: 17.8228, time taken: 2.0936899185180664s\n",
      "63/174, train_loss: 20.3059, time taken: 1.8076884746551514s\n",
      "64/174, train_loss: 18.0479, time taken: 2.0036895275115967s\n",
      "65/174, train_loss: 17.2368, time taken: 1.794226884841919s\n",
      "66/174, train_loss: 20.5321, time taken: 1.7877697944641113s\n",
      "67/174, train_loss: 20.8759, time taken: 2.019861936569214s\n",
      "68/174, train_loss: 14.6631, time taken: 1.9158830642700195s\n",
      "69/174, train_loss: 14.7463, time taken: 2.0173065662384033s\n",
      "70/174, train_loss: 19.6654, time taken: 1.8864495754241943s\n",
      "71/174, train_loss: 17.9599, time taken: 1.905238151550293s\n",
      "72/174, train_loss: 14.6593, time taken: 1.8707304000854492s\n",
      "73/174, train_loss: 33.1647, time taken: 1.8933916091918945s\n",
      "74/174, train_loss: 18.5884, time taken: 1.7205336093902588s\n",
      "75/174, train_loss: 14.7549, time taken: 1.8695125579833984s\n",
      "76/174, train_loss: 19.7831, time taken: 1.899496078491211s\n",
      "77/174, train_loss: 19.9360, time taken: 1.8158109188079834s\n",
      "78/174, train_loss: 34.1818, time taken: 1.867943525314331s\n",
      "79/174, train_loss: 22.8345, time taken: 1.9265336990356445s\n",
      "80/174, train_loss: 21.8951, time taken: 1.7794535160064697s\n",
      "81/174, train_loss: 14.2594, time taken: 1.7974154949188232s\n",
      "82/174, train_loss: 20.8280, time taken: 1.9815678596496582s\n",
      "83/174, train_loss: 17.2420, time taken: 2.0039148330688477s\n",
      "84/174, train_loss: 19.1223, time taken: 1.7350428104400635s\n",
      "85/174, train_loss: 17.0982, time taken: 1.7823834419250488s\n",
      "86/174, train_loss: 20.5040, time taken: 3.866361618041992s\n",
      "87/174, train_loss: 16.8528, time taken: 2.29738187789917s\n",
      "88/174, train_loss: 15.2478, time taken: 2.2967684268951416s\n",
      "89/174, train_loss: 13.9672, time taken: 2.0299429893493652s\n",
      "90/174, train_loss: 20.3481, time taken: 1.9752070903778076s\n",
      "91/174, train_loss: 16.9744, time taken: 1.6894664764404297s\n",
      "92/174, train_loss: 20.5234, time taken: 1.8236305713653564s\n",
      "93/174, train_loss: 18.8455, time taken: 1.801670789718628s\n",
      "94/174, train_loss: 13.7322, time taken: 2.199117660522461s\n",
      "95/174, train_loss: 15.8781, time taken: 1.7096893787384033s\n",
      "96/174, train_loss: 20.3464, time taken: 1.8031260967254639s\n",
      "97/174, train_loss: 19.0895, time taken: 2.218270778656006s\n",
      "98/174, train_loss: 20.7416, time taken: 2.196068048477173s\n",
      "99/174, train_loss: 20.3214, time taken: 1.7812843322753906s\n",
      "100/174, train_loss: 18.5206, time taken: 1.9914803504943848s\n",
      "101/174, train_loss: 17.4652, time taken: 1.6202850341796875s\n",
      "102/174, train_loss: 14.3883, time taken: 1.715989112854004s\n",
      "103/174, train_loss: 15.6068, time taken: 1.910611867904663s\n",
      "104/174, train_loss: 14.5599, time taken: 1.7917752265930176s\n",
      "105/174, train_loss: 16.7466, time taken: 1.8830649852752686s\n",
      "106/174, train_loss: 18.0045, time taken: 1.8026039600372314s\n",
      "107/174, train_loss: 23.8024, time taken: 1.8072974681854248s\n",
      "108/174, train_loss: 16.1424, time taken: 1.7850902080535889s\n",
      "109/174, train_loss: 15.2643, time taken: 1.9916749000549316s\n",
      "110/174, train_loss: 18.0108, time taken: 1.9092638492584229s\n",
      "111/174, train_loss: 14.7962, time taken: 1.7898335456848145s\n",
      "112/174, train_loss: 14.5964, time taken: 1.8172760009765625s\n",
      "113/174, train_loss: 13.7873, time taken: 1.9815943241119385s\n",
      "114/174, train_loss: 24.0149, time taken: 1.8825387954711914s\n",
      "115/174, train_loss: 14.8263, time taken: 1.804161548614502s\n",
      "116/174, train_loss: 12.3214, time taken: 1.893984079360962s\n",
      "117/174, train_loss: 21.3183, time taken: 3.3234074115753174s\n",
      "118/174, train_loss: 14.9588, time taken: 2.5594751834869385s\n",
      "119/174, train_loss: 15.6980, time taken: 2.0042295455932617s\n",
      "120/174, train_loss: 19.9670, time taken: 1.8968546390533447s\n",
      "121/174, train_loss: 15.9040, time taken: 2.1179111003875732s\n",
      "122/174, train_loss: 16.7999, time taken: 1.9644801616668701s\n",
      "123/174, train_loss: 14.5758, time taken: 2.022752046585083s\n",
      "124/174, train_loss: 20.8623, time taken: 1.8878016471862793s\n",
      "125/174, train_loss: 13.2524, time taken: 1.808349847793579s\n",
      "126/174, train_loss: 17.3113, time taken: 2.063265085220337s\n",
      "127/174, train_loss: 15.4183, time taken: 2.0150532722473145s\n",
      "128/174, train_loss: 20.3805, time taken: 2.1043646335601807s\n",
      "129/174, train_loss: 20.4043, time taken: 2.086463212966919s\n",
      "130/174, train_loss: 14.3382, time taken: 2.0099833011627197s\n",
      "131/174, train_loss: 12.6024, time taken: 1.9866969585418701s\n",
      "132/174, train_loss: 17.6497, time taken: 1.8784873485565186s\n",
      "133/174, train_loss: 21.3572, time taken: 1.9285624027252197s\n",
      "134/174, train_loss: 15.7180, time taken: 1.9956846237182617s\n",
      "135/174, train_loss: 19.6045, time taken: 1.9063498973846436s\n",
      "136/174, train_loss: 16.8832, time taken: 1.9007019996643066s\n",
      "137/174, train_loss: 19.2952, time taken: 1.735154151916504s\n",
      "138/174, train_loss: 13.5665, time taken: 1.8858532905578613s\n",
      "139/174, train_loss: 18.8633, time taken: 2.011809825897217s\n",
      "140/174, train_loss: 18.3220, time taken: 1.9779717922210693s\n",
      "141/174, train_loss: 16.4131, time taken: 1.8201794624328613s\n",
      "142/174, train_loss: 18.1856, time taken: 2.1853814125061035s\n",
      "143/174, train_loss: 15.7906, time taken: 1.972196102142334s\n",
      "144/174, train_loss: 17.9677, time taken: 1.7302775382995605s\n",
      "145/174, train_loss: 15.0477, time taken: 1.8031842708587646s\n",
      "146/174, train_loss: 16.2652, time taken: 1.894235372543335s\n",
      "147/174, train_loss: 18.7129, time taken: 1.8173575401306152s\n",
      "148/174, train_loss: 15.2019, time taken: 1.7983849048614502s\n",
      "149/174, train_loss: 16.6073, time taken: 1.7837493419647217s\n",
      "150/174, train_loss: 19.3317, time taken: 2.0121614933013916s\n",
      "151/174, train_loss: 16.0631, time taken: 2.2918448448181152s\n",
      "152/174, train_loss: 20.7069, time taken: 1.892726182937622s\n",
      "153/174, train_loss: 18.7807, time taken: 1.8058195114135742s\n",
      "154/174, train_loss: 17.3784, time taken: 3.903184652328491s\n",
      "155/174, train_loss: 19.2140, time taken: 1.8946611881256104s\n",
      "156/174, train_loss: 16.5180, time taken: 2.19598650932312s\n",
      "157/174, train_loss: 15.3848, time taken: 2.1036572456359863s\n",
      "158/174, train_loss: 16.0339, time taken: 1.9780652523040771s\n",
      "159/174, train_loss: 13.9148, time taken: 2.1769626140594482s\n",
      "160/174, train_loss: 16.2189, time taken: 2.2027063369750977s\n",
      "161/174, train_loss: 21.4370, time taken: 1.998896598815918s\n",
      "162/174, train_loss: 16.7325, time taken: 2.0868682861328125s\n",
      "163/174, train_loss: 16.3562, time taken: 2.129366159439087s\n",
      "164/174, train_loss: 13.8196, time taken: 1.7937901020050049s\n",
      "165/174, train_loss: 17.7046, time taken: 1.9010426998138428s\n",
      "166/174, train_loss: 17.3805, time taken: 1.9820172786712646s\n",
      "167/174, train_loss: 16.7570, time taken: 2.4888134002685547s\n",
      "168/174, train_loss: 13.8164, time taken: 2.0209221839904785s\n",
      "169/174, train_loss: 19.4126, time taken: 1.9991319179534912s\n",
      "170/174, train_loss: 17.7790, time taken: 1.9083795547485352s\n",
      "171/174, train_loss: 15.2966, time taken: 1.8089795112609863s\n",
      "172/174, train_loss: 16.9262, time taken: 1.7328147888183594s\n",
      "173/174, train_loss: 16.0539, time taken: 1.7600088119506836s\n",
      "174/174, train_loss: 13.7050, time taken: 1.7111377716064453s\n",
      "175/174, train_loss: 16.8997, time taken: 1.3756630420684814s\n",
      "epoch 10 average loss: 17.7951\n",
      "----------\n",
      "epoch 11/2000\n",
      "1/174, train_loss: 14.8590, time taken: 8.011909484863281s\n",
      "2/174, train_loss: 14.0859, time taken: 2.1977641582489014s\n",
      "3/174, train_loss: 15.4265, time taken: 1.896233320236206s\n",
      "4/174, train_loss: 16.9561, time taken: 1.9702491760253906s\n",
      "5/174, train_loss: 17.6405, time taken: 1.802335500717163s\n",
      "6/174, train_loss: 20.8294, time taken: 1.9087367057800293s\n",
      "7/174, train_loss: 18.4561, time taken: 1.8887896537780762s\n",
      "8/174, train_loss: 18.4848, time taken: 1.9028263092041016s\n",
      "9/174, train_loss: 19.4671, time taken: 2.079058885574341s\n",
      "10/174, train_loss: 15.8028, time taken: 2.088317632675171s\n",
      "11/174, train_loss: 14.9652, time taken: 1.9024403095245361s\n",
      "12/174, train_loss: 23.2453, time taken: 3.403494119644165s\n",
      "13/174, train_loss: 14.2085, time taken: 1.7822813987731934s\n",
      "14/174, train_loss: 20.8364, time taken: 1.8123931884765625s\n",
      "15/174, train_loss: 17.7375, time taken: 2.0867292881011963s\n",
      "16/174, train_loss: 17.5800, time taken: 2.1245479583740234s\n",
      "17/174, train_loss: 12.3637, time taken: 1.9773979187011719s\n",
      "18/174, train_loss: 19.9398, time taken: 2.288081407546997s\n",
      "19/174, train_loss: 14.9997, time taken: 2.02410888671875s\n",
      "20/174, train_loss: 18.7429, time taken: 1.8651924133300781s\n",
      "21/174, train_loss: 19.5065, time taken: 2.2077479362487793s\n",
      "22/174, train_loss: 21.6852, time taken: 2.1947858333587646s\n",
      "23/174, train_loss: 20.6280, time taken: 1.8916699886322021s\n",
      "24/174, train_loss: 23.1613, time taken: 2.113335371017456s\n",
      "25/174, train_loss: 17.5317, time taken: 1.8957433700561523s\n",
      "26/174, train_loss: 17.2319, time taken: 1.9060189723968506s\n",
      "27/174, train_loss: 11.9798, time taken: 1.8845160007476807s\n",
      "28/174, train_loss: 22.5961, time taken: 1.894376516342163s\n",
      "29/174, train_loss: 17.2217, time taken: 2.208609104156494s\n",
      "30/174, train_loss: 18.0853, time taken: 1.8016881942749023s\n",
      "31/174, train_loss: 19.8680, time taken: 1.98496413230896s\n",
      "32/174, train_loss: 16.1288, time taken: 1.8092224597930908s\n",
      "33/174, train_loss: 11.5169, time taken: 1.792238473892212s\n",
      "34/174, train_loss: 15.3696, time taken: 1.9934256076812744s\n",
      "35/174, train_loss: 18.9928, time taken: 1.8024718761444092s\n",
      "36/174, train_loss: 22.3850, time taken: 1.7846453189849854s\n",
      "37/174, train_loss: 14.8241, time taken: 1.8144659996032715s\n",
      "38/174, train_loss: 16.7723, time taken: 2.068009614944458s\n",
      "39/174, train_loss: 15.1608, time taken: 1.7136485576629639s\n",
      "40/174, train_loss: 16.9058, time taken: 1.7992539405822754s\n",
      "41/174, train_loss: 15.4775, time taken: 1.9703702926635742s\n",
      "42/174, train_loss: 16.4591, time taken: 1.9051132202148438s\n",
      "43/174, train_loss: 19.1871, time taken: 1.8891186714172363s\n",
      "44/174, train_loss: 17.7016, time taken: 1.9296119213104248s\n",
      "45/174, train_loss: 14.6493, time taken: 1.9624524116516113s\n",
      "46/174, train_loss: 14.2047, time taken: 1.8253817558288574s\n",
      "47/174, train_loss: 19.6957, time taken: 1.7965478897094727s\n",
      "48/174, train_loss: 19.3977, time taken: 1.8929495811462402s\n",
      "49/174, train_loss: 17.2171, time taken: 1.7806227207183838s\n",
      "50/174, train_loss: 18.0887, time taken: 1.7242546081542969s\n",
      "51/174, train_loss: 15.4082, time taken: 2.2589261531829834s\n",
      "52/174, train_loss: 17.3845, time taken: 1.824202060699463s\n",
      "53/174, train_loss: 21.4379, time taken: 1.787459135055542s\n",
      "54/174, train_loss: 15.8753, time taken: 1.7976937294006348s\n",
      "55/174, train_loss: 15.3057, time taken: 1.9788117408752441s\n",
      "56/174, train_loss: 17.4002, time taken: 1.9263498783111572s\n",
      "57/174, train_loss: 19.9975, time taken: 1.871649980545044s\n",
      "58/174, train_loss: 19.6982, time taken: 1.8164572715759277s\n",
      "59/174, train_loss: 20.2395, time taken: 2.199902057647705s\n",
      "60/174, train_loss: 15.9218, time taken: 1.7709579467773438s\n",
      "61/174, train_loss: 15.6624, time taken: 1.7114341259002686s\n",
      "62/174, train_loss: 18.8553, time taken: 1.885383129119873s\n",
      "63/174, train_loss: 19.8016, time taken: 1.8984873294830322s\n",
      "64/174, train_loss: 18.0708, time taken: 1.727581262588501s\n",
      "65/174, train_loss: 16.6896, time taken: 2.054023027420044s\n",
      "66/174, train_loss: 20.5849, time taken: 2.1225273609161377s\n",
      "67/174, train_loss: 16.4568, time taken: 1.8958442211151123s\n",
      "68/174, train_loss: 17.1194, time taken: 1.8745527267456055s\n",
      "69/174, train_loss: 17.9564, time taken: 1.9052073955535889s\n",
      "70/174, train_loss: 15.1247, time taken: 1.90169358253479s\n",
      "71/174, train_loss: 14.6328, time taken: 1.8912899494171143s\n",
      "72/174, train_loss: 18.4097, time taken: 1.9087951183319092s\n",
      "73/174, train_loss: 18.7857, time taken: 1.903019666671753s\n",
      "74/174, train_loss: 19.3126, time taken: 1.996061086654663s\n",
      "75/174, train_loss: 22.6798, time taken: 1.786372184753418s\n",
      "76/174, train_loss: 13.3096, time taken: 1.8971691131591797s\n",
      "77/174, train_loss: 18.9326, time taken: 2.0132787227630615s\n",
      "78/174, train_loss: 16.6309, time taken: 1.6850826740264893s\n",
      "79/174, train_loss: 17.8157, time taken: 1.8051836490631104s\n",
      "80/174, train_loss: 19.9132, time taken: 1.7774066925048828s\n",
      "81/174, train_loss: 21.8744, time taken: 1.7759850025177002s\n",
      "82/174, train_loss: 19.9617, time taken: 1.8075602054595947s\n",
      "83/174, train_loss: 14.8796, time taken: 1.8141019344329834s\n",
      "84/174, train_loss: 16.5613, time taken: 1.8087029457092285s\n",
      "85/174, train_loss: 20.9931, time taken: 1.913130760192871s\n",
      "86/174, train_loss: 20.0782, time taken: 1.725266456604004s\n",
      "87/174, train_loss: 15.5690, time taken: 1.895007848739624s\n",
      "88/174, train_loss: 19.2049, time taken: 1.9823307991027832s\n",
      "89/174, train_loss: 16.5387, time taken: 2.096442222595215s\n",
      "90/174, train_loss: 15.5354, time taken: 1.975656509399414s\n",
      "91/174, train_loss: 17.6565, time taken: 1.7989404201507568s\n",
      "92/174, train_loss: 15.6692, time taken: 2.1271092891693115s\n",
      "93/174, train_loss: 18.4432, time taken: 1.9777483940124512s\n",
      "94/174, train_loss: 17.2867, time taken: 2.006225347518921s\n",
      "95/174, train_loss: 13.5441, time taken: 2.1926653385162354s\n",
      "96/174, train_loss: 23.0540, time taken: 2.405513286590576s\n",
      "97/174, train_loss: 16.0251, time taken: 2.0353102684020996s\n",
      "98/174, train_loss: 16.5284, time taken: 2.40191388130188s\n",
      "99/174, train_loss: 21.2737, time taken: 1.898047685623169s\n",
      "100/174, train_loss: 15.5076, time taken: 2.3173513412475586s\n",
      "101/174, train_loss: 22.3420, time taken: 1.8941278457641602s\n",
      "102/174, train_loss: 15.1350, time taken: 2.1175456047058105s\n",
      "103/174, train_loss: 19.3221, time taken: 2.1918182373046875s\n",
      "104/174, train_loss: 18.4264, time taken: 2.0961077213287354s\n",
      "105/174, train_loss: 18.0065, time taken: 1.890120267868042s\n",
      "106/174, train_loss: 20.0682, time taken: 2.0977632999420166s\n",
      "107/174, train_loss: 13.4852, time taken: 2.083082675933838s\n",
      "108/174, train_loss: 15.2043, time taken: 1.9148318767547607s\n",
      "109/174, train_loss: 20.7533, time taken: 1.9940686225891113s\n",
      "110/174, train_loss: 16.8395, time taken: 1.9021596908569336s\n",
      "111/174, train_loss: 18.5937, time taken: 2.0772571563720703s\n",
      "112/174, train_loss: 16.2401, time taken: 2.097463846206665s\n",
      "113/174, train_loss: 19.6908, time taken: 2.0046489238739014s\n",
      "114/174, train_loss: 21.9012, time taken: 1.890411376953125s\n",
      "115/174, train_loss: 13.4883, time taken: 2.076096534729004s\n",
      "116/174, train_loss: 19.1717, time taken: 2.2148208618164062s\n",
      "117/174, train_loss: 13.2654, time taken: 2.0021655559539795s\n",
      "118/174, train_loss: 17.9631, time taken: 1.8023951053619385s\n",
      "119/174, train_loss: 20.9693, time taken: 1.7978055477142334s\n",
      "120/174, train_loss: 11.7184, time taken: 2.078751564025879s\n",
      "121/174, train_loss: 13.0272, time taken: 1.9008054733276367s\n",
      "122/174, train_loss: 15.1634, time taken: 1.9841461181640625s\n",
      "123/174, train_loss: 16.3361, time taken: 1.8285126686096191s\n",
      "124/174, train_loss: 15.1974, time taken: 2.268434762954712s\n",
      "125/174, train_loss: 19.9432, time taken: 2.6949877738952637s\n",
      "126/174, train_loss: 15.8528, time taken: 2.2938950061798096s\n",
      "127/174, train_loss: 18.5350, time taken: 2.4289987087249756s\n",
      "128/174, train_loss: 14.8001, time taken: 2.0926995277404785s\n",
      "129/174, train_loss: 22.8588, time taken: 2.076159954071045s\n",
      "130/174, train_loss: 16.4489, time taken: 2.122854471206665s\n",
      "131/174, train_loss: 17.3247, time taken: 2.000375986099243s\n",
      "132/174, train_loss: 16.2990, time taken: 2.093187093734741s\n",
      "133/174, train_loss: 17.1030, time taken: 1.8171067237854004s\n",
      "134/174, train_loss: 19.1392, time taken: 2.10298228263855s\n",
      "135/174, train_loss: 18.1227, time taken: 1.8671543598175049s\n",
      "136/174, train_loss: 18.2672, time taken: 2.322152853012085s\n",
      "137/174, train_loss: 17.2119, time taken: 2.289479970932007s\n",
      "138/174, train_loss: 18.9363, time taken: 2.38698410987854s\n",
      "139/174, train_loss: 16.1654, time taken: 2.5346720218658447s\n",
      "140/174, train_loss: 15.1765, time taken: 2.3226802349090576s\n",
      "141/174, train_loss: 18.6599, time taken: 2.5912554264068604s\n",
      "142/174, train_loss: 17.2515, time taken: 2.412646532058716s\n",
      "143/174, train_loss: 20.2359, time taken: 2.3926987648010254s\n",
      "144/174, train_loss: 19.5210, time taken: 2.6023876667022705s\n",
      "145/174, train_loss: 17.6502, time taken: 2.4892818927764893s\n",
      "146/174, train_loss: 21.8125, time taken: 2.397794485092163s\n",
      "147/174, train_loss: 20.1694, time taken: 2.4857635498046875s\n",
      "148/174, train_loss: 16.9730, time taken: 2.3922386169433594s\n",
      "149/174, train_loss: 16.0318, time taken: 2.4970157146453857s\n",
      "150/174, train_loss: 16.9864, time taken: 2.421130895614624s\n",
      "151/174, train_loss: 20.9866, time taken: 2.398693799972534s\n",
      "152/174, train_loss: 19.7977, time taken: 2.6252052783966064s\n",
      "153/174, train_loss: 16.9948, time taken: 2.900273084640503s\n",
      "154/174, train_loss: 16.8689, time taken: 3.470301628112793s\n",
      "155/174, train_loss: 19.1709, time taken: 2.8366713523864746s\n",
      "156/174, train_loss: 18.5675, time taken: 2.4868855476379395s\n",
      "157/174, train_loss: 18.1540, time taken: 2.7695302963256836s\n",
      "158/174, train_loss: 17.2554, time taken: 2.93300199508667s\n",
      "159/174, train_loss: 15.6300, time taken: 3.059425115585327s\n",
      "160/174, train_loss: 17.5994, time taken: 2.845891237258911s\n",
      "161/174, train_loss: 10.8950, time taken: 2.6163880825042725s\n",
      "162/174, train_loss: 15.9779, time taken: 2.6033778190612793s\n",
      "163/174, train_loss: 13.8800, time taken: 2.977858781814575s\n",
      "164/174, train_loss: 18.4186, time taken: 2.517705202102661s\n",
      "165/174, train_loss: 15.1857, time taken: 2.7149569988250732s\n",
      "166/174, train_loss: 19.0940, time taken: 2.7827234268188477s\n",
      "167/174, train_loss: 17.3871, time taken: 2.5697176456451416s\n",
      "168/174, train_loss: 16.0740, time taken: 2.6278164386749268s\n",
      "169/174, train_loss: 16.3639, time taken: 2.3834826946258545s\n",
      "170/174, train_loss: 12.3201, time taken: 2.4876890182495117s\n",
      "171/174, train_loss: 18.2516, time taken: 2.517413854598999s\n",
      "172/174, train_loss: 14.6675, time taken: 2.469217538833618s\n",
      "173/174, train_loss: 17.1302, time taken: 2.3218040466308594s\n",
      "174/174, train_loss: 17.2843, time taken: 2.477672815322876s\n",
      "175/174, train_loss: 19.5356, time taken: 1.7236366271972656s\n",
      "epoch 11 average loss: 17.5493\n",
      "Entering Validation for epoch: 11\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 11 Validation avg loss: 13.4061, time taken: 1.7165966033935547s\n",
      "----------\n",
      "epoch 12/2000\n",
      "1/174, train_loss: 16.5282, time taken: 10.891762733459473s\n",
      "2/174, train_loss: 22.1643, time taken: 3.3614566326141357s\n",
      "3/174, train_loss: 21.2756, time taken: 4.206855535507202s\n",
      "4/174, train_loss: 17.4977, time taken: 2.985757350921631s\n",
      "5/174, train_loss: 16.5431, time taken: 3.3298280239105225s\n",
      "6/174, train_loss: 17.5468, time taken: 2.865962505340576s\n",
      "7/174, train_loss: 15.7923, time taken: 3.1943438053131104s\n",
      "8/174, train_loss: 14.9126, time taken: 3.245205879211426s\n",
      "9/174, train_loss: 16.5699, time taken: 2.793494939804077s\n",
      "10/174, train_loss: 16.2779, time taken: 2.2287259101867676s\n",
      "11/174, train_loss: 20.6540, time taken: 3.4898154735565186s\n",
      "12/174, train_loss: 18.8104, time taken: 3.2733564376831055s\n",
      "13/174, train_loss: 17.0998, time taken: 3.012472152709961s\n",
      "14/174, train_loss: 15.3620, time taken: 2.978423833847046s\n",
      "15/174, train_loss: 14.2116, time taken: 2.514803647994995s\n",
      "16/174, train_loss: 17.0776, time taken: 2.415773868560791s\n",
      "17/174, train_loss: 18.0283, time taken: 2.5002973079681396s\n",
      "18/174, train_loss: 17.9970, time taken: 2.845703125s\n",
      "19/174, train_loss: 15.2555, time taken: 2.5512256622314453s\n",
      "20/174, train_loss: 15.9708, time taken: 2.3126473426818848s\n",
      "21/174, train_loss: 11.8997, time taken: 2.5780863761901855s\n",
      "22/174, train_loss: 18.8974, time taken: 2.5045289993286133s\n",
      "23/174, train_loss: 14.9461, time taken: 2.491645574569702s\n",
      "24/174, train_loss: 19.5014, time taken: 2.5113131999969482s\n",
      "25/174, train_loss: 14.4241, time taken: 2.395237684249878s\n",
      "26/174, train_loss: 22.2342, time taken: 2.4965999126434326s\n",
      "27/174, train_loss: 13.2657, time taken: 2.7040679454803467s\n",
      "28/174, train_loss: 17.0648, time taken: 2.6798250675201416s\n",
      "29/174, train_loss: 19.5543, time taken: 2.5736634731292725s\n",
      "30/174, train_loss: 19.7370, time taken: 2.731792688369751s\n",
      "31/174, train_loss: 20.0086, time taken: 4.8944315910339355s\n",
      "32/174, train_loss: 17.4869, time taken: 2.6873505115509033s\n",
      "33/174, train_loss: 19.2526, time taken: 2.6054975986480713s\n",
      "34/174, train_loss: 12.7272, time taken: 2.6924686431884766s\n",
      "35/174, train_loss: 22.1303, time taken: 2.5698671340942383s\n",
      "36/174, train_loss: 17.7111, time taken: 2.8363685607910156s\n",
      "37/174, train_loss: 16.2998, time taken: 2.996650218963623s\n",
      "38/174, train_loss: 16.8304, time taken: 2.5496742725372314s\n",
      "39/174, train_loss: 18.3524, time taken: 2.3196232318878174s\n",
      "40/174, train_loss: 16.3966, time taken: 2.6909444332122803s\n",
      "41/174, train_loss: 21.2012, time taken: 2.6166274547576904s\n",
      "42/174, train_loss: 16.6888, time taken: 2.4176204204559326s\n",
      "43/174, train_loss: 18.0574, time taken: 2.4966001510620117s\n",
      "44/174, train_loss: 19.4492, time taken: 2.9773292541503906s\n",
      "45/174, train_loss: 19.6426, time taken: 2.741643190383911s\n",
      "46/174, train_loss: 21.3797, time taken: 2.5812087059020996s\n",
      "47/174, train_loss: 16.3904, time taken: 2.9709627628326416s\n",
      "48/174, train_loss: 21.2090, time taken: 3.0295066833496094s\n",
      "49/174, train_loss: 17.9435, time taken: 3.1729888916015625s\n",
      "50/174, train_loss: 19.0356, time taken: 3.0286853313446045s\n",
      "51/174, train_loss: 18.2952, time taken: 2.6783056259155273s\n",
      "52/174, train_loss: 19.9159, time taken: 3.2174317836761475s\n",
      "53/174, train_loss: 17.8184, time taken: 2.622880458831787s\n",
      "54/174, train_loss: 17.6738, time taken: 2.9729530811309814s\n",
      "55/174, train_loss: 16.1346, time taken: 2.827432632446289s\n",
      "56/174, train_loss: 17.3612, time taken: 2.4749703407287598s\n",
      "57/174, train_loss: 16.9847, time taken: 2.4103212356567383s\n",
      "58/174, train_loss: 16.5285, time taken: 2.6096222400665283s\n",
      "59/174, train_loss: 20.5582, time taken: 2.4707415103912354s\n",
      "60/174, train_loss: 15.1388, time taken: 2.5115232467651367s\n",
      "61/174, train_loss: 19.0893, time taken: 2.4930334091186523s\n",
      "62/174, train_loss: 15.6086, time taken: 2.809893846511841s\n",
      "63/174, train_loss: 13.8138, time taken: 2.4817140102386475s\n",
      "64/174, train_loss: 15.0558, time taken: 2.497185230255127s\n",
      "65/174, train_loss: 15.5852, time taken: 2.8751161098480225s\n",
      "66/174, train_loss: 20.5070, time taken: 2.593768835067749s\n",
      "67/174, train_loss: 13.4529, time taken: 2.418100595474243s\n",
      "68/174, train_loss: 18.6768, time taken: 2.9767980575561523s\n",
      "69/174, train_loss: 16.2262, time taken: 2.5190815925598145s\n",
      "70/174, train_loss: 15.3196, time taken: 2.6978182792663574s\n",
      "71/174, train_loss: 15.6641, time taken: 2.7795705795288086s\n",
      "72/174, train_loss: 16.2749, time taken: 3.2979073524475098s\n",
      "73/174, train_loss: 18.3968, time taken: 2.9021365642547607s\n",
      "74/174, train_loss: 11.9774, time taken: 2.4980173110961914s\n",
      "75/174, train_loss: 18.0035, time taken: 2.5027480125427246s\n",
      "76/174, train_loss: 18.7580, time taken: 2.9915623664855957s\n",
      "77/174, train_loss: 21.6395, time taken: 2.711393356323242s\n",
      "78/174, train_loss: 17.9407, time taken: 2.851616621017456s\n",
      "79/174, train_loss: 12.7626, time taken: 2.7337026596069336s\n",
      "80/174, train_loss: 18.6405, time taken: 2.4984049797058105s\n",
      "81/174, train_loss: 19.7482, time taken: 2.522695302963257s\n",
      "82/174, train_loss: 17.7346, time taken: 2.702782392501831s\n",
      "83/174, train_loss: 12.7612, time taken: 2.5033347606658936s\n",
      "84/174, train_loss: 17.9311, time taken: 2.569918632507324s\n",
      "85/174, train_loss: 21.9256, time taken: 2.3915724754333496s\n",
      "86/174, train_loss: 13.4977, time taken: 2.5326621532440186s\n",
      "87/174, train_loss: 18.1459, time taken: 2.4871060848236084s\n",
      "88/174, train_loss: 14.6382, time taken: 2.500023603439331s\n",
      "89/174, train_loss: 14.6515, time taken: 2.6652088165283203s\n",
      "90/174, train_loss: 10.6693, time taken: 2.799208641052246s\n",
      "91/174, train_loss: 15.4856, time taken: 2.5152251720428467s\n",
      "92/174, train_loss: 15.8862, time taken: 2.897155523300171s\n",
      "93/174, train_loss: 16.3679, time taken: 2.8811161518096924s\n",
      "94/174, train_loss: 17.1022, time taken: 2.915945053100586s\n",
      "95/174, train_loss: 18.0002, time taken: 2.7133264541625977s\n",
      "96/174, train_loss: 11.6090, time taken: 2.9349091053009033s\n",
      "97/174, train_loss: 16.6294, time taken: 3.199850559234619s\n",
      "98/174, train_loss: 16.6778, time taken: 2.693852186203003s\n",
      "99/174, train_loss: 16.3542, time taken: 2.705674409866333s\n",
      "100/174, train_loss: 19.4520, time taken: 2.605515718460083s\n",
      "101/174, train_loss: 15.5798, time taken: 3.003141164779663s\n",
      "102/174, train_loss: 17.6367, time taken: 2.898240327835083s\n",
      "103/174, train_loss: 15.2684, time taken: 2.5102665424346924s\n",
      "104/174, train_loss: 17.9125, time taken: 2.4945945739746094s\n",
      "105/174, train_loss: 19.8413, time taken: 2.773498296737671s\n",
      "106/174, train_loss: 16.7466, time taken: 2.486478567123413s\n",
      "107/174, train_loss: 20.5370, time taken: 2.3218159675598145s\n",
      "108/174, train_loss: 14.9332, time taken: 2.6142470836639404s\n",
      "109/174, train_loss: 19.8742, time taken: 2.579684257507324s\n",
      "110/174, train_loss: 18.2796, time taken: 2.6666409969329834s\n",
      "111/174, train_loss: 17.8890, time taken: 2.516679048538208s\n",
      "112/174, train_loss: 20.4687, time taken: 2.5224997997283936s\n",
      "113/174, train_loss: 24.7770, time taken: 2.3356611728668213s\n",
      "114/174, train_loss: 15.4941, time taken: 2.6746416091918945s\n",
      "115/174, train_loss: 15.4345, time taken: 2.3923826217651367s\n",
      "116/174, train_loss: 19.0730, time taken: 2.388514757156372s\n",
      "117/174, train_loss: 20.0711, time taken: 2.5255167484283447s\n",
      "118/174, train_loss: 17.2287, time taken: 2.787025213241577s\n",
      "119/174, train_loss: 12.8632, time taken: 2.610867500305176s\n",
      "120/174, train_loss: 20.8993, time taken: 2.5615384578704834s\n",
      "121/174, train_loss: 15.2497, time taken: 2.406364917755127s\n",
      "122/174, train_loss: 15.9135, time taken: 3.2888035774230957s\n",
      "123/174, train_loss: 14.5131, time taken: 2.6959116458892822s\n",
      "124/174, train_loss: 13.8826, time taken: 2.919545888900757s\n",
      "125/174, train_loss: 15.3344, time taken: 2.904123544692993s\n",
      "126/174, train_loss: 16.3161, time taken: 2.860802412033081s\n",
      "127/174, train_loss: 15.9674, time taken: 2.5862462520599365s\n",
      "128/174, train_loss: 13.2960, time taken: 2.324157238006592s\n",
      "129/174, train_loss: 15.4533, time taken: 2.5072641372680664s\n",
      "130/174, train_loss: 18.5426, time taken: 2.76767897605896s\n",
      "131/174, train_loss: 15.8313, time taken: 2.7257630825042725s\n",
      "132/174, train_loss: 15.9057, time taken: 3.1220736503601074s\n",
      "133/174, train_loss: 14.7288, time taken: 2.7236833572387695s\n",
      "134/174, train_loss: 19.5547, time taken: 2.7483909130096436s\n",
      "135/174, train_loss: 15.0022, time taken: 2.3176565170288086s\n",
      "136/174, train_loss: 16.7760, time taken: 2.596484899520874s\n",
      "137/174, train_loss: 14.7476, time taken: 2.396676540374756s\n",
      "138/174, train_loss: 17.2934, time taken: 2.5043888092041016s\n",
      "139/174, train_loss: 17.5350, time taken: 2.48563551902771s\n",
      "140/174, train_loss: 18.8313, time taken: 2.3105175495147705s\n",
      "141/174, train_loss: 17.4556, time taken: 2.5622098445892334s\n",
      "142/174, train_loss: 15.9750, time taken: 2.4990131855010986s\n",
      "143/174, train_loss: 20.9624, time taken: 2.390808343887329s\n",
      "144/174, train_loss: 22.2502, time taken: 2.5955920219421387s\n",
      "145/174, train_loss: 16.8171, time taken: 2.523407459259033s\n",
      "146/174, train_loss: 16.5700, time taken: 2.777503252029419s\n",
      "147/174, train_loss: 20.1847, time taken: 2.5036814212799072s\n",
      "148/174, train_loss: 19.7996, time taken: 2.495063066482544s\n",
      "149/174, train_loss: 18.1850, time taken: 2.573892593383789s\n",
      "150/174, train_loss: 21.1493, time taken: 4.045473575592041s\n",
      "151/174, train_loss: 15.3887, time taken: 2.43023419380188s\n",
      "152/174, train_loss: 21.0155, time taken: 2.794025421142578s\n",
      "153/174, train_loss: 17.0280, time taken: 2.790163040161133s\n",
      "154/174, train_loss: 16.8159, time taken: 2.9945693016052246s\n",
      "155/174, train_loss: 21.9506, time taken: 2.7899277210235596s\n",
      "156/174, train_loss: 14.4162, time taken: 2.6089746952056885s\n",
      "157/174, train_loss: 15.9847, time taken: 2.485299587249756s\n",
      "158/174, train_loss: 16.2186, time taken: 2.815495491027832s\n",
      "159/174, train_loss: 16.8008, time taken: 2.6676647663116455s\n",
      "160/174, train_loss: 18.8773, time taken: 2.9197652339935303s\n",
      "161/174, train_loss: 14.4866, time taken: 3.1897168159484863s\n",
      "162/174, train_loss: 14.6241, time taken: 2.5865936279296875s\n",
      "163/174, train_loss: 19.5317, time taken: 2.5001285076141357s\n",
      "164/174, train_loss: 16.1619, time taken: 2.609624147415161s\n",
      "165/174, train_loss: 17.5143, time taken: 2.6820321083068848s\n",
      "166/174, train_loss: 15.5998, time taken: 2.577451467514038s\n",
      "167/174, train_loss: 15.4019, time taken: 2.512798309326172s\n",
      "168/174, train_loss: 14.8036, time taken: 2.6023197174072266s\n",
      "169/174, train_loss: 17.1082, time taken: 2.3908889293670654s\n",
      "170/174, train_loss: 16.5224, time taken: 2.4114508628845215s\n",
      "171/174, train_loss: 16.1939, time taken: 2.4174509048461914s\n",
      "172/174, train_loss: 19.3512, time taken: 2.3041934967041016s\n",
      "173/174, train_loss: 19.4234, time taken: 2.502807378768921s\n",
      "174/174, train_loss: 20.6523, time taken: 2.497284412384033s\n",
      "175/174, train_loss: 17.1447, time taken: 2.104118585586548s\n",
      "epoch 12 average loss: 17.2583\n",
      "----------\n",
      "epoch 13/2000\n",
      "1/174, train_loss: 17.5723, time taken: 11.41358733177185s\n",
      "2/174, train_loss: 17.7366, time taken: 3.218719720840454s\n",
      "3/174, train_loss: 14.6433, time taken: 2.792593002319336s\n",
      "4/174, train_loss: 16.4356, time taken: 3.1962201595306396s\n",
      "5/174, train_loss: 17.9884, time taken: 3.0150039196014404s\n",
      "6/174, train_loss: 18.4964, time taken: 2.6179754734039307s\n",
      "7/174, train_loss: 19.7527, time taken: 2.5017969608306885s\n",
      "8/174, train_loss: 20.8773, time taken: 2.3721415996551514s\n",
      "9/174, train_loss: 16.6461, time taken: 2.932478189468384s\n",
      "10/174, train_loss: 15.7463, time taken: 2.9970784187316895s\n",
      "11/174, train_loss: 10.8817, time taken: 2.9073057174682617s\n",
      "12/174, train_loss: 12.1361, time taken: 2.5319666862487793s\n",
      "13/174, train_loss: 16.9387, time taken: 2.7621734142303467s\n",
      "14/174, train_loss: 16.0270, time taken: 2.515190839767456s\n",
      "15/174, train_loss: 23.1258, time taken: 2.910094976425171s\n",
      "16/174, train_loss: 16.9332, time taken: 2.434511184692383s\n",
      "17/174, train_loss: 17.5935, time taken: 2.4017508029937744s\n",
      "18/174, train_loss: 16.4360, time taken: 2.5589706897735596s\n",
      "19/174, train_loss: 17.1030, time taken: 2.4219632148742676s\n",
      "20/174, train_loss: 17.5910, time taken: 2.4888291358947754s\n",
      "21/174, train_loss: 18.0143, time taken: 2.718594551086426s\n",
      "22/174, train_loss: 18.4535, time taken: 2.483505964279175s\n",
      "23/174, train_loss: 16.3485, time taken: 2.609715461730957s\n",
      "24/174, train_loss: 11.8257, time taken: 2.6951868534088135s\n",
      "25/174, train_loss: 22.6759, time taken: 2.5287623405456543s\n",
      "26/174, train_loss: 20.1503, time taken: 2.58660888671875s\n",
      "27/174, train_loss: 16.6276, time taken: 2.3961992263793945s\n",
      "28/174, train_loss: 16.3675, time taken: 2.611314058303833s\n",
      "29/174, train_loss: 16.1545, time taken: 2.58365797996521s\n",
      "30/174, train_loss: 14.2800, time taken: 2.285259485244751s\n",
      "31/174, train_loss: 17.9911, time taken: 2.5111515522003174s\n",
      "32/174, train_loss: 17.8466, time taken: 2.6708195209503174s\n",
      "33/174, train_loss: 15.8795, time taken: 2.435741662979126s\n",
      "34/174, train_loss: 14.7518, time taken: 3.4773037433624268s\n",
      "35/174, train_loss: 19.1868, time taken: 2.6923232078552246s\n",
      "36/174, train_loss: 9.2193, time taken: 3.2129266262054443s\n",
      "37/174, train_loss: 15.9434, time taken: 2.6887640953063965s\n",
      "38/174, train_loss: 16.0105, time taken: 2.9831578731536865s\n",
      "39/174, train_loss: 17.0766, time taken: 2.9938158988952637s\n",
      "40/174, train_loss: 23.8522, time taken: 2.8954195976257324s\n",
      "41/174, train_loss: 15.4259, time taken: 3.019508123397827s\n",
      "42/174, train_loss: 18.0863, time taken: 2.6399269104003906s\n",
      "43/174, train_loss: 13.6907, time taken: 2.5029327869415283s\n",
      "44/174, train_loss: 16.6292, time taken: 2.5148868560791016s\n",
      "45/174, train_loss: 20.2179, time taken: 2.4199905395507812s\n",
      "46/174, train_loss: 18.5627, time taken: 2.9625070095062256s\n",
      "47/174, train_loss: 18.1037, time taken: 2.5368471145629883s\n",
      "48/174, train_loss: 16.0376, time taken: 2.435386896133423s\n",
      "49/174, train_loss: 26.9080, time taken: 2.5849733352661133s\n",
      "50/174, train_loss: 15.7513, time taken: 2.486048698425293s\n",
      "51/174, train_loss: 17.6379, time taken: 2.40608811378479s\n",
      "52/174, train_loss: 17.3286, time taken: 2.4905648231506348s\n",
      "53/174, train_loss: 15.0399, time taken: 4.700999021530151s\n",
      "54/174, train_loss: 17.3668, time taken: 3.00222110748291s\n",
      "55/174, train_loss: 18.7104, time taken: 3.002770185470581s\n",
      "56/174, train_loss: 16.8118, time taken: 2.666083335876465s\n",
      "57/174, train_loss: 19.1573, time taken: 2.7941298484802246s\n",
      "58/174, train_loss: 12.0910, time taken: 3.027303695678711s\n",
      "59/174, train_loss: 12.5141, time taken: 2.568601369857788s\n",
      "60/174, train_loss: 16.1312, time taken: 2.409146785736084s\n",
      "61/174, train_loss: 17.1273, time taken: 2.720799446105957s\n",
      "62/174, train_loss: 19.2693, time taken: 2.5557808876037598s\n",
      "63/174, train_loss: 15.2810, time taken: 2.6045725345611572s\n",
      "64/174, train_loss: 22.7556, time taken: 2.2151598930358887s\n",
      "65/174, train_loss: 17.4527, time taken: 2.5042333602905273s\n",
      "66/174, train_loss: 16.4271, time taken: 2.496539831161499s\n",
      "67/174, train_loss: 14.5973, time taken: 2.592616558074951s\n",
      "68/174, train_loss: 17.8598, time taken: 2.8096506595611572s\n",
      "69/174, train_loss: 14.1086, time taken: 2.4953861236572266s\n",
      "70/174, train_loss: 17.5605, time taken: 2.886793613433838s\n",
      "71/174, train_loss: 20.7382, time taken: 2.896393060684204s\n",
      "72/174, train_loss: 17.1842, time taken: 2.5927422046661377s\n",
      "73/174, train_loss: 18.6524, time taken: 2.3090193271636963s\n",
      "74/174, train_loss: 16.8060, time taken: 5.081129550933838s\n",
      "75/174, train_loss: 17.7715, time taken: 2.9087045192718506s\n",
      "76/174, train_loss: 25.0725, time taken: 2.779646635055542s\n",
      "77/174, train_loss: 20.0267, time taken: 2.8118643760681152s\n",
      "78/174, train_loss: 16.0058, time taken: 2.857783794403076s\n",
      "79/174, train_loss: 17.8881, time taken: 2.5216333866119385s\n",
      "80/174, train_loss: 18.0454, time taken: 2.4964399337768555s\n",
      "81/174, train_loss: 16.0335, time taken: 2.818239450454712s\n",
      "82/174, train_loss: 12.5183, time taken: 2.430730104446411s\n",
      "83/174, train_loss: 14.9258, time taken: 2.8775277137756348s\n",
      "84/174, train_loss: 17.2663, time taken: 2.600856065750122s\n",
      "85/174, train_loss: 21.1552, time taken: 2.4985032081604004s\n",
      "86/174, train_loss: 13.3324, time taken: 2.810701608657837s\n",
      "87/174, train_loss: 18.6734, time taken: 2.5751593112945557s\n",
      "88/174, train_loss: 17.4276, time taken: 2.4828710556030273s\n",
      "89/174, train_loss: 21.4060, time taken: 2.503080368041992s\n",
      "90/174, train_loss: 17.7153, time taken: 2.4957339763641357s\n",
      "91/174, train_loss: 15.8900, time taken: 2.3944811820983887s\n",
      "92/174, train_loss: 17.7119, time taken: 2.5357584953308105s\n",
      "93/174, train_loss: 16.2752, time taken: 2.398963212966919s\n",
      "94/174, train_loss: 15.2383, time taken: 2.516714096069336s\n",
      "95/174, train_loss: 13.7833, time taken: 2.424285888671875s\n",
      "96/174, train_loss: 18.5222, time taken: 2.4965591430664062s\n",
      "97/174, train_loss: 15.5239, time taken: 2.3339827060699463s\n",
      "98/174, train_loss: 16.0461, time taken: 2.5666065216064453s\n",
      "99/174, train_loss: 16.2640, time taken: 2.51322603225708s\n",
      "100/174, train_loss: 17.5405, time taken: 4.305154323577881s\n",
      "101/174, train_loss: 18.0481, time taken: 3.294139862060547s\n",
      "102/174, train_loss: 16.2248, time taken: 3.069049835205078s\n",
      "103/174, train_loss: 20.2364, time taken: 2.9030864238739014s\n",
      "104/174, train_loss: 21.1991, time taken: 3.0101747512817383s\n",
      "105/174, train_loss: 17.4128, time taken: 2.407301187515259s\n",
      "106/174, train_loss: 17.2186, time taken: 2.4098949432373047s\n",
      "107/174, train_loss: 18.8243, time taken: 2.5650086402893066s\n",
      "108/174, train_loss: 17.9102, time taken: 2.5946860313415527s\n",
      "109/174, train_loss: 19.0491, time taken: 3.0104305744171143s\n",
      "110/174, train_loss: 15.1465, time taken: 2.694148063659668s\n",
      "111/174, train_loss: 16.0536, time taken: 2.4975571632385254s\n",
      "112/174, train_loss: 15.4897, time taken: 2.6712448596954346s\n",
      "113/174, train_loss: 17.3406, time taken: 2.534653425216675s\n",
      "114/174, train_loss: 14.5707, time taken: 2.686655282974243s\n",
      "115/174, train_loss: 13.9786, time taken: 2.5127527713775635s\n",
      "116/174, train_loss: 17.7693, time taken: 2.4428727626800537s\n",
      "117/174, train_loss: 17.0839, time taken: 2.795386791229248s\n",
      "118/174, train_loss: 19.2745, time taken: 2.5852925777435303s\n",
      "119/174, train_loss: 17.0223, time taken: 2.40114688873291s\n",
      "120/174, train_loss: 17.8846, time taken: 2.3933660984039307s\n",
      "121/174, train_loss: 18.6693, time taken: 2.4716875553131104s\n",
      "122/174, train_loss: 15.2969, time taken: 2.695448398590088s\n",
      "123/174, train_loss: 22.5194, time taken: 2.706156015396118s\n",
      "124/174, train_loss: 15.6712, time taken: 2.569514036178589s\n",
      "125/174, train_loss: 17.2004, time taken: 2.8300724029541016s\n",
      "126/174, train_loss: 13.2178, time taken: 2.5870773792266846s\n",
      "127/174, train_loss: 18.5587, time taken: 2.6178388595581055s\n",
      "128/174, train_loss: 17.1827, time taken: 2.8699865341186523s\n",
      "129/174, train_loss: 16.4839, time taken: 2.486210346221924s\n",
      "130/174, train_loss: 15.6994, time taken: 3.0374860763549805s\n",
      "131/174, train_loss: 14.1839, time taken: 2.7986624240875244s\n",
      "132/174, train_loss: 16.6403, time taken: 2.340155839920044s\n",
      "133/174, train_loss: 17.3172, time taken: 2.6041576862335205s\n",
      "134/174, train_loss: 20.5716, time taken: 3.015446662902832s\n",
      "135/174, train_loss: 15.4518, time taken: 2.7237188816070557s\n",
      "136/174, train_loss: 14.4141, time taken: 2.565382719039917s\n",
      "137/174, train_loss: 18.2139, time taken: 2.721083879470825s\n",
      "138/174, train_loss: 15.2990, time taken: 2.690260887145996s\n",
      "139/174, train_loss: 19.5903, time taken: 2.707120180130005s\n",
      "140/174, train_loss: 19.6073, time taken: 2.7909607887268066s\n",
      "141/174, train_loss: 16.5710, time taken: 2.5979130268096924s\n",
      "142/174, train_loss: 13.6732, time taken: 2.505558490753174s\n",
      "143/174, train_loss: 16.9113, time taken: 2.6625008583068848s\n",
      "144/174, train_loss: 20.0291, time taken: 2.404465675354004s\n",
      "145/174, train_loss: 17.0713, time taken: 2.5017638206481934s\n",
      "146/174, train_loss: 24.2844, time taken: 2.3989291191101074s\n",
      "147/174, train_loss: 15.1783, time taken: 2.8909780979156494s\n",
      "148/174, train_loss: 23.2665, time taken: 2.403449296951294s\n",
      "149/174, train_loss: 14.6971, time taken: 2.4836552143096924s\n",
      "150/174, train_loss: 13.4312, time taken: 2.503283739089966s\n",
      "151/174, train_loss: 16.5623, time taken: 2.518533945083618s\n",
      "152/174, train_loss: 26.8439, time taken: 2.5221071243286133s\n",
      "153/174, train_loss: 16.2888, time taken: 2.77054762840271s\n",
      "154/174, train_loss: 20.4192, time taken: 2.524670362472534s\n",
      "155/174, train_loss: 20.6178, time taken: 2.7065775394439697s\n",
      "156/174, train_loss: 19.1264, time taken: 2.6780025959014893s\n",
      "157/174, train_loss: 16.8375, time taken: 2.6055941581726074s\n",
      "158/174, train_loss: 22.0194, time taken: 2.7933480739593506s\n",
      "159/174, train_loss: 15.4047, time taken: 2.5114035606384277s\n",
      "160/174, train_loss: 16.0179, time taken: 2.3174328804016113s\n",
      "161/174, train_loss: 18.6186, time taken: 2.515575885772705s\n",
      "162/174, train_loss: 19.8484, time taken: 2.4614508152008057s\n",
      "163/174, train_loss: 18.6656, time taken: 3.006460428237915s\n",
      "164/174, train_loss: 16.5195, time taken: 2.901613712310791s\n",
      "165/174, train_loss: 17.1466, time taken: 2.5954158306121826s\n",
      "166/174, train_loss: 14.4824, time taken: 2.912203550338745s\n",
      "167/174, train_loss: 12.1459, time taken: 2.413248062133789s\n",
      "168/174, train_loss: 18.7012, time taken: 2.3998115062713623s\n",
      "169/174, train_loss: 18.8342, time taken: 2.7103819847106934s\n",
      "170/174, train_loss: 15.5828, time taken: 2.8245527744293213s\n",
      "171/174, train_loss: 18.5728, time taken: 2.5663421154022217s\n",
      "172/174, train_loss: 21.1474, time taken: 2.525627374649048s\n",
      "173/174, train_loss: 14.5640, time taken: 2.676586389541626s\n",
      "174/174, train_loss: 17.3530, time taken: 2.621527671813965s\n",
      "175/174, train_loss: 18.2791, time taken: 2.095808506011963s\n",
      "epoch 13 average loss: 17.3179\n",
      "Entering Validation for epoch: 13\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 13 Validation avg loss: 12.4288, time taken: 1.6086087226867676s\n",
      "----------\n",
      "epoch 14/2000\n",
      "1/174, train_loss: 15.4829, time taken: 10.867936849594116s\n",
      "2/174, train_loss: 18.2921, time taken: 3.0185046195983887s\n",
      "3/174, train_loss: 19.3683, time taken: 2.794963836669922s\n",
      "4/174, train_loss: 22.3893, time taken: 2.5848476886749268s\n",
      "5/174, train_loss: 18.3786, time taken: 2.415921449661255s\n",
      "6/174, train_loss: 14.3877, time taken: 3.0642952919006348s\n",
      "7/174, train_loss: 17.0273, time taken: 2.7299647331237793s\n",
      "8/174, train_loss: 16.9720, time taken: 2.583672523498535s\n",
      "9/174, train_loss: 16.5772, time taken: 2.3098158836364746s\n",
      "10/174, train_loss: 14.4883, time taken: 2.6658835411071777s\n",
      "11/174, train_loss: 17.8901, time taken: 3.0118465423583984s\n",
      "12/174, train_loss: 11.2926, time taken: 2.7021055221557617s\n",
      "13/174, train_loss: 13.5316, time taken: 2.5678937435150146s\n",
      "14/174, train_loss: 23.0939, time taken: 2.620980978012085s\n",
      "15/174, train_loss: 13.3564, time taken: 2.774423837661743s\n",
      "16/174, train_loss: 23.7542, time taken: 2.51153826713562s\n",
      "17/174, train_loss: 16.0627, time taken: 2.3757967948913574s\n",
      "18/174, train_loss: 14.7117, time taken: 2.7494349479675293s\n",
      "19/174, train_loss: 17.2603, time taken: 2.624892234802246s\n",
      "20/174, train_loss: 14.9825, time taken: 2.4722704887390137s\n",
      "21/174, train_loss: 15.1815, time taken: 2.60308575630188s\n",
      "22/174, train_loss: 17.0432, time taken: 2.5263888835906982s\n",
      "23/174, train_loss: 22.3160, time taken: 2.805332899093628s\n",
      "24/174, train_loss: 14.2937, time taken: 2.4550793170928955s\n",
      "25/174, train_loss: 16.0707, time taken: 2.60830020904541s\n",
      "26/174, train_loss: 16.4144, time taken: 2.6089112758636475s\n",
      "27/174, train_loss: 20.1984, time taken: 2.608884334564209s\n",
      "28/174, train_loss: 22.9589, time taken: 2.394899606704712s\n",
      "29/174, train_loss: 20.9265, time taken: 2.595198631286621s\n",
      "30/174, train_loss: 16.4324, time taken: 2.475074052810669s\n",
      "31/174, train_loss: 17.3064, time taken: 2.518014907836914s\n",
      "32/174, train_loss: 18.1927, time taken: 2.570986747741699s\n",
      "33/174, train_loss: 12.7489, time taken: 2.889742136001587s\n",
      "34/174, train_loss: 15.7891, time taken: 2.5282270908355713s\n",
      "35/174, train_loss: 19.7436, time taken: 2.495114326477051s\n",
      "36/174, train_loss: 18.9965, time taken: 4.789353609085083s\n",
      "37/174, train_loss: 17.2674, time taken: 2.9778850078582764s\n",
      "38/174, train_loss: 16.2795, time taken: 2.797895669937134s\n",
      "39/174, train_loss: 15.8926, time taken: 2.5914480686187744s\n",
      "40/174, train_loss: 19.6928, time taken: 2.8093321323394775s\n",
      "41/174, train_loss: 18.1940, time taken: 2.9982495307922363s\n",
      "42/174, train_loss: 20.3746, time taken: 2.515659809112549s\n",
      "43/174, train_loss: 14.6923, time taken: 2.5780863761901855s\n",
      "44/174, train_loss: 19.5020, time taken: 2.3097140789031982s\n",
      "45/174, train_loss: 19.0561, time taken: 2.476658821105957s\n",
      "46/174, train_loss: 14.8303, time taken: 2.591289520263672s\n",
      "47/174, train_loss: 16.6730, time taken: 2.4972853660583496s\n",
      "48/174, train_loss: 20.3345, time taken: 2.4303324222564697s\n",
      "49/174, train_loss: 13.7829, time taken: 2.655606985092163s\n",
      "50/174, train_loss: 17.1446, time taken: 2.8079891204833984s\n",
      "51/174, train_loss: 19.8129, time taken: 2.799988031387329s\n",
      "52/174, train_loss: 18.8479, time taken: 3.2979090213775635s\n",
      "53/174, train_loss: 15.1794, time taken: 2.814258098602295s\n",
      "54/174, train_loss: 15.1957, time taken: 2.829502582550049s\n",
      "55/174, train_loss: 13.1249, time taken: 2.900848865509033s\n",
      "56/174, train_loss: 17.1566, time taken: 2.5719685554504395s\n",
      "57/174, train_loss: 22.5833, time taken: 2.5219457149505615s\n",
      "58/174, train_loss: 17.1457, time taken: 3.37722110748291s\n",
      "59/174, train_loss: 17.2614, time taken: 2.819293975830078s\n",
      "60/174, train_loss: 19.7257, time taken: 3.078399896621704s\n",
      "61/174, train_loss: 15.8550, time taken: 2.8089518547058105s\n",
      "62/174, train_loss: 15.6790, time taken: 2.489957094192505s\n",
      "63/174, train_loss: 14.8177, time taken: 2.5932369232177734s\n",
      "64/174, train_loss: 16.0440, time taken: 2.5983264446258545s\n",
      "65/174, train_loss: 16.2272, time taken: 2.6882050037384033s\n",
      "66/174, train_loss: 16.4814, time taken: 2.4897067546844482s\n",
      "67/174, train_loss: 15.2731, time taken: 2.4875648021698s\n",
      "68/174, train_loss: 20.7234, time taken: 2.419119358062744s\n",
      "69/174, train_loss: 16.9611, time taken: 2.3200013637542725s\n",
      "70/174, train_loss: 14.5568, time taken: 2.259080171585083s\n",
      "71/174, train_loss: 17.1537, time taken: 2.8089776039123535s\n",
      "72/174, train_loss: 15.7472, time taken: 2.589799642562866s\n",
      "73/174, train_loss: 12.8857, time taken: 2.4177138805389404s\n",
      "74/174, train_loss: 17.7591, time taken: 2.332716464996338s\n",
      "75/174, train_loss: 21.2431, time taken: 2.798206090927124s\n",
      "76/174, train_loss: 17.1374, time taken: 2.6745338439941406s\n",
      "77/174, train_loss: 17.9336, time taken: 2.5032827854156494s\n",
      "78/174, train_loss: 15.4362, time taken: 2.498131275177002s\n",
      "79/174, train_loss: 16.9529, time taken: 2.9954516887664795s\n",
      "80/174, train_loss: 16.1568, time taken: 2.7919161319732666s\n",
      "81/174, train_loss: 16.9921, time taken: 2.593998670578003s\n",
      "82/174, train_loss: 17.9379, time taken: 2.690042018890381s\n",
      "83/174, train_loss: 15.9398, time taken: 2.9945802688598633s\n",
      "84/174, train_loss: 17.0202, time taken: 2.931964635848999s\n",
      "85/174, train_loss: 14.5644, time taken: 2.6921327114105225s\n",
      "86/174, train_loss: 15.4742, time taken: 2.8977062702178955s\n",
      "87/174, train_loss: 15.8858, time taken: 2.7758841514587402s\n",
      "88/174, train_loss: 10.9415, time taken: 2.5003509521484375s\n",
      "89/174, train_loss: 21.2322, time taken: 2.3783187866210938s\n",
      "90/174, train_loss: 16.3935, time taken: 2.526803493499756s\n",
      "91/174, train_loss: 17.3852, time taken: 2.505801200866699s\n",
      "92/174, train_loss: 12.4178, time taken: 2.347801685333252s\n",
      "93/174, train_loss: 18.1717, time taken: 2.4132721424102783s\n",
      "94/174, train_loss: 18.5060, time taken: 2.28865122795105s\n",
      "95/174, train_loss: 15.4159, time taken: 2.633486270904541s\n",
      "96/174, train_loss: 15.9946, time taken: 2.4417243003845215s\n",
      "97/174, train_loss: 20.5998, time taken: 2.4291839599609375s\n",
      "98/174, train_loss: 19.4332, time taken: 2.6055684089660645s\n",
      "99/174, train_loss: 20.2363, time taken: 2.5998148918151855s\n",
      "100/174, train_loss: 14.8945, time taken: 2.490610361099243s\n",
      "101/174, train_loss: 18.0574, time taken: 2.5320303440093994s\n",
      "102/174, train_loss: 11.7608, time taken: 2.710519790649414s\n",
      "103/174, train_loss: 17.1271, time taken: 2.235783338546753s\n",
      "104/174, train_loss: 15.2332, time taken: 2.410308599472046s\n",
      "105/174, train_loss: 19.4494, time taken: 4.412118673324585s\n",
      "106/174, train_loss: 18.5852, time taken: 2.4947257041931152s\n",
      "107/174, train_loss: 16.5845, time taken: 2.5108113288879395s\n",
      "108/174, train_loss: 18.2798, time taken: 2.782764434814453s\n",
      "109/174, train_loss: 16.8664, time taken: 2.90635085105896s\n",
      "110/174, train_loss: 16.2309, time taken: 2.9275968074798584s\n",
      "111/174, train_loss: 17.5612, time taken: 2.58858323097229s\n",
      "112/174, train_loss: 18.3978, time taken: 2.5669610500335693s\n",
      "113/174, train_loss: 16.1198, time taken: 2.901521682739258s\n",
      "114/174, train_loss: 18.8880, time taken: 2.4827969074249268s\n",
      "115/174, train_loss: 11.8049, time taken: 2.9942543506622314s\n",
      "116/174, train_loss: 18.2666, time taken: 2.535309076309204s\n",
      "117/174, train_loss: 13.8270, time taken: 2.8650310039520264s\n",
      "118/174, train_loss: 16.3947, time taken: 2.4216115474700928s\n",
      "119/174, train_loss: 19.4909, time taken: 2.474327325820923s\n",
      "120/174, train_loss: 19.0489, time taken: 2.527836799621582s\n",
      "121/174, train_loss: 16.3068, time taken: 2.4673380851745605s\n",
      "122/174, train_loss: 19.4552, time taken: 2.4070730209350586s\n",
      "123/174, train_loss: 17.0458, time taken: 2.315584182739258s\n",
      "124/174, train_loss: 18.5041, time taken: 2.3932957649230957s\n",
      "125/174, train_loss: 11.3478, time taken: 2.3619754314422607s\n",
      "126/174, train_loss: 17.4100, time taken: 2.533036231994629s\n",
      "127/174, train_loss: 21.9058, time taken: 2.685058116912842s\n",
      "128/174, train_loss: 15.2072, time taken: 2.4982409477233887s\n",
      "129/174, train_loss: 16.7351, time taken: 2.496047019958496s\n",
      "130/174, train_loss: 19.7915, time taken: 2.4962210655212402s\n",
      "131/174, train_loss: 15.1411, time taken: 2.781374454498291s\n",
      "132/174, train_loss: 18.2087, time taken: 4.275103330612183s\n",
      "133/174, train_loss: 17.0119, time taken: 3.0991384983062744s\n",
      "134/174, train_loss: 15.0323, time taken: 2.909205436706543s\n",
      "135/174, train_loss: 13.2946, time taken: 3.084773063659668s\n",
      "136/174, train_loss: 15.2827, time taken: 2.7175779342651367s\n",
      "137/174, train_loss: 17.6744, time taken: 2.5735366344451904s\n",
      "138/174, train_loss: 18.4653, time taken: 2.6039185523986816s\n",
      "139/174, train_loss: 17.7463, time taken: 2.5160818099975586s\n",
      "140/174, train_loss: 16.4224, time taken: 2.775465965270996s\n",
      "141/174, train_loss: 17.7524, time taken: 2.385648250579834s\n",
      "142/174, train_loss: 13.5890, time taken: 2.92983341217041s\n",
      "143/174, train_loss: 17.5070, time taken: 2.793184280395508s\n",
      "144/174, train_loss: 12.1009, time taken: 2.803365707397461s\n",
      "145/174, train_loss: 15.1480, time taken: 2.6839516162872314s\n",
      "146/174, train_loss: 14.9179, time taken: 2.5745608806610107s\n",
      "147/174, train_loss: 15.1657, time taken: 2.7202324867248535s\n",
      "148/174, train_loss: 17.4234, time taken: 2.570826292037964s\n",
      "149/174, train_loss: 22.9591, time taken: 2.4047131538391113s\n",
      "150/174, train_loss: 19.1296, time taken: 2.4044196605682373s\n",
      "151/174, train_loss: 16.2496, time taken: 2.4982550144195557s\n",
      "152/174, train_loss: 12.7159, time taken: 2.4104080200195312s\n",
      "153/174, train_loss: 18.5290, time taken: 2.6827564239501953s\n",
      "154/174, train_loss: 17.7181, time taken: 2.4772303104400635s\n",
      "155/174, train_loss: 18.9615, time taken: 2.530735492706299s\n",
      "156/174, train_loss: 14.4605, time taken: 2.694164752960205s\n",
      "157/174, train_loss: 16.5772, time taken: 2.393521308898926s\n",
      "158/174, train_loss: 15.4472, time taken: 2.4977052211761475s\n",
      "159/174, train_loss: 15.8897, time taken: 2.5155701637268066s\n",
      "160/174, train_loss: 20.9386, time taken: 2.420880079269409s\n",
      "161/174, train_loss: 12.7365, time taken: 2.4968104362487793s\n",
      "162/174, train_loss: 15.7196, time taken: 4.980400323867798s\n",
      "163/174, train_loss: 16.5483, time taken: 2.3152055740356445s\n",
      "164/174, train_loss: 16.5132, time taken: 2.70253324508667s\n",
      "165/174, train_loss: 13.7382, time taken: 2.9854419231414795s\n",
      "166/174, train_loss: 20.3047, time taken: 2.720843553543091s\n",
      "167/174, train_loss: 25.2282, time taken: 2.5699825286865234s\n",
      "168/174, train_loss: 17.1369, time taken: 2.423198699951172s\n",
      "169/174, train_loss: 16.4981, time taken: 2.5548155307769775s\n",
      "170/174, train_loss: 18.2540, time taken: 2.7219409942626953s\n",
      "171/174, train_loss: 15.6071, time taken: 2.490570306777954s\n",
      "172/174, train_loss: 18.3748, time taken: 2.5099802017211914s\n",
      "173/174, train_loss: 15.7520, time taken: 2.823986053466797s\n",
      "174/174, train_loss: 15.3364, time taken: 2.641167640686035s\n",
      "175/174, train_loss: 16.8414, time taken: 2.1097571849823s\n",
      "epoch 14 average loss: 16.9935\n",
      "----------\n",
      "epoch 15/2000\n",
      "1/174, train_loss: 15.9859, time taken: 10.490762948989868s\n",
      "2/174, train_loss: 14.4455, time taken: 3.3960530757904053s\n",
      "3/174, train_loss: 17.3116, time taken: 2.4771909713745117s\n",
      "4/174, train_loss: 18.8925, time taken: 2.417332887649536s\n",
      "5/174, train_loss: 17.7604, time taken: 2.4634552001953125s\n",
      "6/174, train_loss: 17.6797, time taken: 2.6004767417907715s\n",
      "7/174, train_loss: 20.6324, time taken: 2.520906448364258s\n",
      "8/174, train_loss: 17.6791, time taken: 2.6991043090820312s\n",
      "9/174, train_loss: 26.9294, time taken: 2.6031861305236816s\n",
      "10/174, train_loss: 18.0398, time taken: 2.4835476875305176s\n",
      "11/174, train_loss: 18.2344, time taken: 2.9150969982147217s\n",
      "12/174, train_loss: 15.5325, time taken: 2.3783600330352783s\n",
      "13/174, train_loss: 20.2209, time taken: 2.6190173625946045s\n",
      "14/174, train_loss: 18.7440, time taken: 4.824737548828125s\n",
      "15/174, train_loss: 23.4374, time taken: 2.572751760482788s\n",
      "16/174, train_loss: 14.9460, time taken: 3.2267801761627197s\n",
      "17/174, train_loss: 13.3486, time taken: 2.910689353942871s\n",
      "18/174, train_loss: 21.6919, time taken: 2.6216766834259033s\n",
      "19/174, train_loss: 16.8172, time taken: 2.782620429992676s\n",
      "20/174, train_loss: 17.8776, time taken: 3.1148877143859863s\n",
      "21/174, train_loss: 17.4311, time taken: 3.003335475921631s\n",
      "22/174, train_loss: 14.9473, time taken: 2.703336477279663s\n",
      "23/174, train_loss: 18.8116, time taken: 2.6310460567474365s\n",
      "24/174, train_loss: 19.2945, time taken: 2.806321859359741s\n",
      "25/174, train_loss: 12.5195, time taken: 3.3744442462921143s\n",
      "26/174, train_loss: 13.1123, time taken: 3.084855318069458s\n",
      "27/174, train_loss: 15.1007, time taken: 2.8220653533935547s\n",
      "28/174, train_loss: 16.6210, time taken: 2.8989522457122803s\n",
      "29/174, train_loss: 17.6667, time taken: 2.700275421142578s\n",
      "30/174, train_loss: 17.5652, time taken: 2.4971442222595215s\n",
      "31/174, train_loss: 19.6073, time taken: 2.5030100345611572s\n",
      "32/174, train_loss: 17.3216, time taken: 2.289623737335205s\n",
      "33/174, train_loss: 18.5566, time taken: 2.4054625034332275s\n",
      "34/174, train_loss: 13.0264, time taken: 2.500218152999878s\n",
      "35/174, train_loss: 16.4271, time taken: 2.421658992767334s\n",
      "36/174, train_loss: 13.7074, time taken: 2.3060338497161865s\n",
      "37/174, train_loss: 21.3199, time taken: 2.4853122234344482s\n",
      "38/174, train_loss: 13.6481, time taken: 2.490145444869995s\n",
      "39/174, train_loss: 14.7374, time taken: 2.5795671939849854s\n",
      "40/174, train_loss: 15.4863, time taken: 2.4969537258148193s\n",
      "41/174, train_loss: 18.5803, time taken: 2.6252987384796143s\n",
      "42/174, train_loss: 13.9298, time taken: 2.5760364532470703s\n",
      "43/174, train_loss: 17.9227, time taken: 2.579277276992798s\n",
      "44/174, train_loss: 20.3091, time taken: 2.7111263275146484s\n",
      "45/174, train_loss: 14.6459, time taken: 2.7014358043670654s\n",
      "46/174, train_loss: 12.2077, time taken: 2.805238723754883s\n",
      "47/174, train_loss: 13.6459, time taken: 3.176898717880249s\n",
      "48/174, train_loss: 22.1800, time taken: 3.0103044509887695s\n",
      "49/174, train_loss: 14.5559, time taken: 2.702582597732544s\n",
      "50/174, train_loss: 17.9093, time taken: 2.7687947750091553s\n",
      "51/174, train_loss: 15.6372, time taken: 3.227666139602661s\n",
      "52/174, train_loss: 15.7692, time taken: 2.5963544845581055s\n",
      "53/174, train_loss: 15.2141, time taken: 2.3726675510406494s\n",
      "54/174, train_loss: 14.5525, time taken: 2.6164069175720215s\n",
      "55/174, train_loss: 16.2838, time taken: 2.462785482406616s\n",
      "56/174, train_loss: 14.8576, time taken: 2.4011082649230957s\n",
      "57/174, train_loss: 15.9313, time taken: 2.517308235168457s\n",
      "58/174, train_loss: 17.7372, time taken: 2.4009602069854736s\n",
      "59/174, train_loss: 15.5472, time taken: 2.2862162590026855s\n",
      "60/174, train_loss: 16.8866, time taken: 2.712982654571533s\n",
      "61/174, train_loss: 14.6117, time taken: 4.3530237674713135s\n",
      "62/174, train_loss: 25.3976, time taken: 2.906273365020752s\n",
      "63/174, train_loss: 17.0134, time taken: 2.4979705810546875s\n",
      "64/174, train_loss: 15.4156, time taken: 2.807661771774292s\n",
      "65/174, train_loss: 16.2991, time taken: 2.7750515937805176s\n",
      "66/174, train_loss: 19.8582, time taken: 2.8919403553009033s\n",
      "67/174, train_loss: 20.3388, time taken: 2.52260160446167s\n",
      "68/174, train_loss: 16.2688, time taken: 2.910672664642334s\n",
      "69/174, train_loss: 16.9873, time taken: 2.9319305419921875s\n",
      "70/174, train_loss: 14.3243, time taken: 2.881391763687134s\n",
      "71/174, train_loss: 17.2289, time taken: 2.918281316757202s\n",
      "72/174, train_loss: 17.0540, time taken: 2.4971184730529785s\n",
      "73/174, train_loss: 15.4107, time taken: 2.4976561069488525s\n",
      "74/174, train_loss: 16.5447, time taken: 2.5368504524230957s\n",
      "75/174, train_loss: 12.9830, time taken: 2.6713478565216064s\n",
      "76/174, train_loss: 15.4022, time taken: 2.4078402519226074s\n",
      "77/174, train_loss: 19.9169, time taken: 2.490220308303833s\n",
      "78/174, train_loss: 15.9148, time taken: 2.5102217197418213s\n",
      "79/174, train_loss: 19.5399, time taken: 2.475163459777832s\n",
      "80/174, train_loss: 18.1921, time taken: 2.897230863571167s\n",
      "81/174, train_loss: 16.2074, time taken: 2.4041552543640137s\n",
      "82/174, train_loss: 15.7796, time taken: 2.595346689224243s\n",
      "83/174, train_loss: 15.0332, time taken: 2.6756832599639893s\n",
      "84/174, train_loss: 21.7434, time taken: 2.522649049758911s\n",
      "85/174, train_loss: 17.9911, time taken: 2.4679603576660156s\n",
      "86/174, train_loss: 20.0769, time taken: 1.8445260524749756s\n",
      "87/174, train_loss: 18.9351, time taken: 1.838571310043335s\n",
      "88/174, train_loss: 12.8840, time taken: 1.9849977493286133s\n",
      "89/174, train_loss: 18.4014, time taken: 2.3027918338775635s\n",
      "90/174, train_loss: 16.3787, time taken: 1.887394905090332s\n",
      "91/174, train_loss: 16.9094, time taken: 3.991485118865967s\n",
      "92/174, train_loss: 12.7242, time taken: 2.267080307006836s\n",
      "93/174, train_loss: 19.4609, time taken: 1.8289237022399902s\n",
      "94/174, train_loss: 18.3184, time taken: 2.2717716693878174s\n",
      "95/174, train_loss: 18.0425, time taken: 2.2232635021209717s\n",
      "96/174, train_loss: 15.6795, time taken: 2.1695778369903564s\n",
      "97/174, train_loss: 19.8913, time taken: 1.7930104732513428s\n",
      "98/174, train_loss: 15.7847, time taken: 2.0025088787078857s\n",
      "99/174, train_loss: 18.2939, time taken: 2.0889573097229004s\n",
      "100/174, train_loss: 17.5126, time taken: 1.923401117324829s\n",
      "101/174, train_loss: 14.8818, time taken: 2.0845444202423096s\n",
      "102/174, train_loss: 17.3100, time taken: 2.197889804840088s\n",
      "103/174, train_loss: 15.5225, time taken: 2.1936962604522705s\n",
      "104/174, train_loss: 17.2710, time taken: 2.3924851417541504s\n",
      "105/174, train_loss: 18.4953, time taken: 2.2079758644104004s\n",
      "106/174, train_loss: 12.6603, time taken: 2.065774440765381s\n",
      "107/174, train_loss: 16.7908, time taken: 2.424133777618408s\n",
      "108/174, train_loss: 17.1138, time taken: 1.9973766803741455s\n",
      "109/174, train_loss: 20.4317, time taken: 1.7965691089630127s\n",
      "110/174, train_loss: 16.4632, time taken: 2.2949182987213135s\n",
      "111/174, train_loss: 15.2777, time taken: 2.17642879486084s\n",
      "112/174, train_loss: 12.8765, time taken: 1.9322798252105713s\n",
      "113/174, train_loss: 17.1709, time taken: 2.0249924659729004s\n",
      "114/174, train_loss: 18.3954, time taken: 2.2800910472869873s\n",
      "115/174, train_loss: 16.2293, time taken: 2.0232598781585693s\n",
      "116/174, train_loss: 15.4676, time taken: 1.7736842632293701s\n",
      "117/174, train_loss: 20.4067, time taken: 1.9034314155578613s\n",
      "118/174, train_loss: 24.1256, time taken: 1.8716835975646973s\n",
      "119/174, train_loss: 19.6658, time taken: 1.802880048751831s\n",
      "120/174, train_loss: 13.4995, time taken: 1.7877888679504395s\n",
      "121/174, train_loss: 19.4947, time taken: 1.835906982421875s\n",
      "122/174, train_loss: 20.2495, time taken: 2.1608214378356934s\n",
      "123/174, train_loss: 15.8451, time taken: 1.9318349361419678s\n",
      "124/174, train_loss: 17.9104, time taken: 1.8823363780975342s\n",
      "125/174, train_loss: 20.9545, time taken: 2.112086534500122s\n",
      "126/174, train_loss: 19.8463, time taken: 2.1076605319976807s\n",
      "127/174, train_loss: 18.9186, time taken: 2.487910509109497s\n",
      "128/174, train_loss: 16.2879, time taken: 1.9991106986999512s\n",
      "129/174, train_loss: 19.8616, time taken: 2.0055415630340576s\n",
      "130/174, train_loss: 15.5628, time taken: 1.921262264251709s\n",
      "131/174, train_loss: 13.6186, time taken: 2.4581398963928223s\n",
      "132/174, train_loss: 16.7697, time taken: 1.9279186725616455s\n",
      "133/174, train_loss: 13.4504, time taken: 1.7896220684051514s\n",
      "134/174, train_loss: 16.8562, time taken: 2.215749502182007s\n",
      "135/174, train_loss: 19.8169, time taken: 2.5231549739837646s\n",
      "136/174, train_loss: 14.7988, time taken: 1.9985063076019287s\n",
      "137/174, train_loss: 18.7028, time taken: 1.8918795585632324s\n",
      "138/174, train_loss: 18.6239, time taken: 2.0838003158569336s\n",
      "139/174, train_loss: 18.5206, time taken: 2.518467903137207s\n",
      "140/174, train_loss: 17.9853, time taken: 1.9849145412445068s\n",
      "141/174, train_loss: 11.6673, time taken: 1.9000158309936523s\n",
      "142/174, train_loss: 17.5422, time taken: 2.013150453567505s\n",
      "143/174, train_loss: 17.2890, time taken: 2.469224214553833s\n",
      "144/174, train_loss: 19.8264, time taken: 2.106354236602783s\n",
      "145/174, train_loss: 18.7347, time taken: 2.091465711593628s\n",
      "146/174, train_loss: 16.3342, time taken: 1.9776010513305664s\n",
      "147/174, train_loss: 17.9566, time taken: 2.0161185264587402s\n",
      "148/174, train_loss: 15.3900, time taken: 1.7965822219848633s\n",
      "149/174, train_loss: 18.1721, time taken: 1.8218023777008057s\n",
      "150/174, train_loss: 18.2866, time taken: 1.9643163681030273s\n",
      "151/174, train_loss: 21.4325, time taken: 1.8993430137634277s\n",
      "152/174, train_loss: 17.9666, time taken: 2.0844359397888184s\n",
      "153/174, train_loss: 26.7191, time taken: 1.8224728107452393s\n",
      "154/174, train_loss: 16.9313, time taken: 1.9903416633605957s\n",
      "155/174, train_loss: 15.4707, time taken: 1.911440134048462s\n",
      "156/174, train_loss: 9.5681, time taken: 1.9029428958892822s\n",
      "157/174, train_loss: 13.5751, time taken: 1.8118751049041748s\n",
      "158/174, train_loss: 15.1437, time taken: 1.8191542625427246s\n",
      "159/174, train_loss: 18.3570, time taken: 1.8601922988891602s\n",
      "160/174, train_loss: 18.3501, time taken: 1.8894214630126953s\n",
      "161/174, train_loss: 20.4088, time taken: 2.028621196746826s\n",
      "162/174, train_loss: 17.2851, time taken: 2.082340955734253s\n",
      "163/174, train_loss: 17.7006, time taken: 2.293076992034912s\n",
      "164/174, train_loss: 18.9260, time taken: 2.209129810333252s\n",
      "165/174, train_loss: 16.9323, time taken: 2.0066115856170654s\n",
      "166/174, train_loss: 17.9576, time taken: 1.7809832096099854s\n",
      "167/174, train_loss: 17.3050, time taken: 1.9878575801849365s\n",
      "168/174, train_loss: 21.0445, time taken: 2.088372230529785s\n",
      "169/174, train_loss: 14.1971, time taken: 1.721099615097046s\n",
      "170/174, train_loss: 16.9404, time taken: 1.9040513038635254s\n",
      "171/174, train_loss: 25.3507, time taken: 1.7200775146484375s\n",
      "172/174, train_loss: 13.9121, time taken: 1.8940370082855225s\n",
      "173/174, train_loss: 12.6410, time taken: 2.005215883255005s\n",
      "174/174, train_loss: 19.2677, time taken: 3.789876937866211s\n",
      "175/174, train_loss: 20.7008, time taken: 1.2249655723571777s\n",
      "epoch 15 average loss: 17.2433\n",
      "Entering Validation for epoch: 15\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 15 Validation avg loss: 9.3042, time taken: 1.1061575412750244s\n",
      "Saving new model based on validation loss 9.3042\n",
      "----------\n",
      "epoch 16/2000\n",
      "1/174, train_loss: 18.4882, time taken: 8.424396276473999s\n",
      "2/174, train_loss: 20.7235, time taken: 2.6917006969451904s\n",
      "3/174, train_loss: 17.2169, time taken: 2.003863573074341s\n",
      "4/174, train_loss: 15.6831, time taken: 1.9010636806488037s\n",
      "5/174, train_loss: 19.0574, time taken: 1.9035887718200684s\n",
      "6/174, train_loss: 20.6937, time taken: 1.8917443752288818s\n",
      "7/174, train_loss: 14.9254, time taken: 2.2954537868499756s\n",
      "8/174, train_loss: 15.1133, time taken: 1.8667070865631104s\n",
      "9/174, train_loss: 18.7858, time taken: 1.833338975906372s\n",
      "10/174, train_loss: 18.1360, time taken: 2.361992120742798s\n",
      "11/174, train_loss: 16.8538, time taken: 1.814805030822754s\n",
      "12/174, train_loss: 17.4241, time taken: 2.079376459121704s\n",
      "13/174, train_loss: 20.2823, time taken: 2.0973236560821533s\n",
      "14/174, train_loss: 20.1881, time taken: 2.0007143020629883s\n",
      "15/174, train_loss: 17.1033, time taken: 1.8874800205230713s\n",
      "16/174, train_loss: 21.7371, time taken: 1.9139113426208496s\n",
      "17/174, train_loss: 14.0232, time taken: 1.8134081363677979s\n",
      "18/174, train_loss: 21.5591, time taken: 2.007330894470215s\n",
      "19/174, train_loss: 17.6146, time taken: 1.87064790725708s\n",
      "20/174, train_loss: 16.9072, time taken: 1.9900081157684326s\n",
      "21/174, train_loss: 15.8180, time taken: 2.0311012268066406s\n",
      "22/174, train_loss: 18.5030, time taken: 1.8165795803070068s\n",
      "23/174, train_loss: 14.8061, time taken: 1.899827003479004s\n",
      "24/174, train_loss: 20.5249, time taken: 2.010040044784546s\n",
      "25/174, train_loss: 15.5895, time taken: 1.9027464389801025s\n",
      "26/174, train_loss: 16.9521, time taken: 1.696545124053955s\n",
      "27/174, train_loss: 15.5962, time taken: 1.7391722202301025s\n",
      "28/174, train_loss: 17.6534, time taken: 1.8839194774627686s\n",
      "29/174, train_loss: 20.2269, time taken: 1.8072035312652588s\n",
      "30/174, train_loss: 16.6510, time taken: 1.7852141857147217s\n",
      "31/174, train_loss: 16.6586, time taken: 2.0104222297668457s\n",
      "32/174, train_loss: 19.1053, time taken: 1.7897868156433105s\n",
      "33/174, train_loss: 18.3817, time taken: 1.8127799034118652s\n",
      "34/174, train_loss: 15.5885, time taken: 1.8447532653808594s\n",
      "35/174, train_loss: 14.0932, time taken: 3.7661423683166504s\n",
      "36/174, train_loss: 17.4694, time taken: 1.8083558082580566s\n",
      "37/174, train_loss: 14.2582, time taken: 1.8027749061584473s\n",
      "38/174, train_loss: 13.0074, time taken: 2.292882204055786s\n",
      "39/174, train_loss: 16.0426, time taken: 2.069822311401367s\n",
      "40/174, train_loss: 15.5759, time taken: 1.9331185817718506s\n",
      "41/174, train_loss: 14.9722, time taken: 2.469207763671875s\n",
      "42/174, train_loss: 16.2865, time taken: 2.1228890419006348s\n",
      "43/174, train_loss: 20.7702, time taken: 1.7959439754486084s\n",
      "44/174, train_loss: 18.6956, time taken: 1.967904806137085s\n",
      "45/174, train_loss: 14.6705, time taken: 1.9109711647033691s\n",
      "46/174, train_loss: 17.8520, time taken: 1.9999184608459473s\n",
      "47/174, train_loss: 15.1848, time taken: 2.081434965133667s\n",
      "48/174, train_loss: 14.6403, time taken: 1.9237525463104248s\n",
      "49/174, train_loss: 16.3198, time taken: 1.8954672813415527s\n",
      "50/174, train_loss: 16.9583, time taken: 1.9782333374023438s\n",
      "51/174, train_loss: 15.7262, time taken: 1.7986652851104736s\n",
      "52/174, train_loss: 14.6908, time taken: 1.9844446182250977s\n",
      "53/174, train_loss: 20.3852, time taken: 1.92535400390625s\n",
      "54/174, train_loss: 18.8513, time taken: 2.0789198875427246s\n",
      "55/174, train_loss: 17.4107, time taken: 1.8778448104858398s\n",
      "56/174, train_loss: 18.0871, time taken: 1.932981252670288s\n",
      "57/174, train_loss: 15.9981, time taken: 1.9004907608032227s\n",
      "58/174, train_loss: 21.1010, time taken: 1.7864689826965332s\n",
      "59/174, train_loss: 19.7825, time taken: 1.7858457565307617s\n",
      "60/174, train_loss: 14.7286, time taken: 1.7803263664245605s\n",
      "61/174, train_loss: 12.8570, time taken: 1.7925536632537842s\n",
      "62/174, train_loss: 17.6467, time taken: 1.8021736145019531s\n",
      "63/174, train_loss: 18.3287, time taken: 1.8080427646636963s\n",
      "64/174, train_loss: 16.8436, time taken: 1.7015132904052734s\n",
      "65/174, train_loss: 15.0677, time taken: 1.7199280261993408s\n",
      "66/174, train_loss: 23.8485, time taken: 1.7925834655761719s\n",
      "67/174, train_loss: 14.0764, time taken: 1.772505283355713s\n",
      "68/174, train_loss: 17.7093, time taken: 1.8253858089447021s\n",
      "69/174, train_loss: 19.3776, time taken: 1.859860897064209s\n",
      "70/174, train_loss: 16.4579, time taken: 1.8930213451385498s\n",
      "71/174, train_loss: 20.9393, time taken: 2.029600143432617s\n",
      "72/174, train_loss: 19.5204, time taken: 1.7902774810791016s\n",
      "73/174, train_loss: 20.1820, time taken: 1.8989393711090088s\n",
      "74/174, train_loss: 12.1764, time taken: 3.1708929538726807s\n",
      "75/174, train_loss: 19.1768, time taken: 2.213931083679199s\n",
      "76/174, train_loss: 15.9093, time taken: 2.089447498321533s\n",
      "77/174, train_loss: 11.8641, time taken: 1.8997585773468018s\n",
      "78/174, train_loss: 21.6588, time taken: 1.99308443069458s\n",
      "79/174, train_loss: 17.7373, time taken: 1.9117207527160645s\n",
      "80/174, train_loss: 18.9742, time taken: 1.999459981918335s\n",
      "81/174, train_loss: 15.7089, time taken: 1.9753682613372803s\n",
      "82/174, train_loss: 23.8152, time taken: 1.8229644298553467s\n",
      "83/174, train_loss: 15.3621, time taken: 1.9783120155334473s\n",
      "84/174, train_loss: 19.4944, time taken: 2.1948633193969727s\n",
      "85/174, train_loss: 21.5016, time taken: 2.208165168762207s\n",
      "86/174, train_loss: 12.0789, time taken: 1.8654086589813232s\n",
      "87/174, train_loss: 17.9978, time taken: 1.8281164169311523s\n",
      "88/174, train_loss: 18.8822, time taken: 1.804696798324585s\n",
      "89/174, train_loss: 18.6155, time taken: 1.7957253456115723s\n",
      "90/174, train_loss: 18.6437, time taken: 1.7843294143676758s\n",
      "91/174, train_loss: 14.7207, time taken: 1.915497064590454s\n",
      "92/174, train_loss: 15.8396, time taken: 1.8567965030670166s\n",
      "93/174, train_loss: 18.8790, time taken: 1.9332787990570068s\n",
      "94/174, train_loss: 19.6642, time taken: 1.7741398811340332s\n",
      "95/174, train_loss: 16.8257, time taken: 1.9147253036499023s\n",
      "96/174, train_loss: 16.4279, time taken: 1.9662055969238281s\n",
      "97/174, train_loss: 17.9538, time taken: 1.8004510402679443s\n",
      "98/174, train_loss: 17.3165, time taken: 2.023817300796509s\n",
      "99/174, train_loss: 18.4234, time taken: 1.715463399887085s\n",
      "100/174, train_loss: 17.4508, time taken: 1.7094128131866455s\n",
      "101/174, train_loss: 19.1939, time taken: 1.7934410572052002s\n",
      "102/174, train_loss: 18.1021, time taken: 1.9756708145141602s\n",
      "103/174, train_loss: 14.9419, time taken: 1.726957082748413s\n",
      "104/174, train_loss: 14.8252, time taken: 2.008380174636841s\n",
      "105/174, train_loss: 14.8193, time taken: 1.9190127849578857s\n",
      "106/174, train_loss: 12.9581, time taken: 1.8896281719207764s\n",
      "107/174, train_loss: 19.6026, time taken: 1.9012784957885742s\n",
      "108/174, train_loss: 13.0058, time taken: 1.7264957427978516s\n",
      "109/174, train_loss: 19.9444, time taken: 4.070188522338867s\n",
      "110/174, train_loss: 18.3856, time taken: 2.1818675994873047s\n",
      "111/174, train_loss: 14.6058, time taken: 1.9089202880859375s\n",
      "112/174, train_loss: 17.0223, time taken: 2.109116554260254s\n",
      "113/174, train_loss: 14.1035, time taken: 1.9950671195983887s\n",
      "114/174, train_loss: 17.0584, time taken: 1.9034850597381592s\n",
      "115/174, train_loss: 17.6287, time taken: 1.882767677307129s\n",
      "116/174, train_loss: 15.1076, time taken: 1.8986401557922363s\n",
      "117/174, train_loss: 13.5600, time taken: 2.0042033195495605s\n",
      "118/174, train_loss: 19.1032, time taken: 2.270460605621338s\n",
      "119/174, train_loss: 15.4565, time taken: 2.0013136863708496s\n",
      "120/174, train_loss: 19.1491, time taken: 1.8994016647338867s\n",
      "121/174, train_loss: 17.0186, time taken: 2.1934564113616943s\n",
      "122/174, train_loss: 19.2801, time taken: 1.8868918418884277s\n",
      "123/174, train_loss: 11.6992, time taken: 1.8155722618103027s\n",
      "124/174, train_loss: 14.5161, time taken: 1.7807371616363525s\n",
      "125/174, train_loss: 20.8212, time taken: 1.8192112445831299s\n",
      "126/174, train_loss: 20.0961, time taken: 2.0874335765838623s\n",
      "127/174, train_loss: 16.9235, time taken: 1.8153998851776123s\n",
      "128/174, train_loss: 16.1813, time taken: 1.8033885955810547s\n",
      "129/174, train_loss: 18.5127, time taken: 1.8729979991912842s\n",
      "130/174, train_loss: 12.2507, time taken: 1.8004717826843262s\n",
      "131/174, train_loss: 18.0433, time taken: 2.0076756477355957s\n",
      "132/174, train_loss: 15.9011, time taken: 1.9026505947113037s\n",
      "133/174, train_loss: 16.0605, time taken: 2.0666825771331787s\n",
      "134/174, train_loss: 18.5503, time taken: 2.0036866664886475s\n",
      "135/174, train_loss: 18.0221, time taken: 1.9066097736358643s\n",
      "136/174, train_loss: 17.6224, time taken: 1.9890215396881104s\n",
      "137/174, train_loss: 18.2398, time taken: 1.7184367179870605s\n",
      "138/174, train_loss: 17.4281, time taken: 1.8716378211975098s\n",
      "139/174, train_loss: 18.9027, time taken: 1.9141831398010254s\n",
      "140/174, train_loss: 21.4042, time taken: 1.790285348892212s\n",
      "141/174, train_loss: 11.2978, time taken: 1.999847173690796s\n",
      "142/174, train_loss: 17.4818, time taken: 1.807523250579834s\n",
      "143/174, train_loss: 17.9360, time taken: 2.088685989379883s\n",
      "144/174, train_loss: 15.9434, time taken: 1.8737194538116455s\n",
      "145/174, train_loss: 14.4471, time taken: 1.987980842590332s\n",
      "146/174, train_loss: 17.3203, time taken: 1.8182649612426758s\n",
      "147/174, train_loss: 17.9813, time taken: 1.9787838459014893s\n",
      "148/174, train_loss: 16.9389, time taken: 1.9176006317138672s\n",
      "149/174, train_loss: 15.2846, time taken: 2.100580930709839s\n",
      "150/174, train_loss: 19.9855, time taken: 1.975395917892456s\n",
      "151/174, train_loss: 14.2516, time taken: 1.9964587688446045s\n",
      "152/174, train_loss: 19.5788, time taken: 2.1303906440734863s\n",
      "153/174, train_loss: 16.9291, time taken: 2.074545383453369s\n",
      "154/174, train_loss: 19.8052, time taken: 2.213796615600586s\n",
      "155/174, train_loss: 14.0118, time taken: 2.5033226013183594s\n",
      "156/174, train_loss: 17.9139, time taken: 1.9918603897094727s\n",
      "157/174, train_loss: 20.0822, time taken: 1.9887664318084717s\n",
      "158/174, train_loss: 14.3459, time taken: 1.9092164039611816s\n",
      "159/174, train_loss: 16.3397, time taken: 2.0776991844177246s\n",
      "160/174, train_loss: 17.5050, time taken: 2.0750279426574707s\n",
      "161/174, train_loss: 18.2477, time taken: 2.017254590988159s\n",
      "162/174, train_loss: 22.2375, time taken: 2.0814368724823s\n",
      "163/174, train_loss: 22.9566, time taken: 2.0966739654541016s\n",
      "164/174, train_loss: 14.6931, time taken: 1.8129796981811523s\n",
      "165/174, train_loss: 17.9579, time taken: 1.8867409229278564s\n",
      "166/174, train_loss: 19.3731, time taken: 2.1001899242401123s\n",
      "167/174, train_loss: 21.8458, time taken: 2.0772266387939453s\n",
      "168/174, train_loss: 13.4418, time taken: 1.9297020435333252s\n",
      "169/174, train_loss: 17.3803, time taken: 1.8895559310913086s\n",
      "170/174, train_loss: 16.9284, time taken: 1.975715160369873s\n",
      "171/174, train_loss: 14.2975, time taken: 1.90360426902771s\n",
      "172/174, train_loss: 18.0451, time taken: 1.7318058013916016s\n",
      "173/174, train_loss: 15.7658, time taken: 1.8288164138793945s\n",
      "174/174, train_loss: 16.7655, time taken: 1.8045575618743896s\n",
      "175/174, train_loss: 15.7520, time taken: 1.488490104675293s\n",
      "epoch 16 average loss: 17.2640\n",
      "----------\n",
      "epoch 17/2000\n",
      "1/174, train_loss: 16.0001, time taken: 8.388278007507324s\n",
      "2/174, train_loss: 16.2673, time taken: 2.199223041534424s\n",
      "3/174, train_loss: 16.1088, time taken: 1.9782764911651611s\n",
      "4/174, train_loss: 21.6351, time taken: 1.8241817951202393s\n",
      "5/174, train_loss: 18.4016, time taken: 1.8992536067962646s\n",
      "6/174, train_loss: 14.8618, time taken: 2.2016730308532715s\n",
      "7/174, train_loss: 15.4648, time taken: 1.796630620956421s\n",
      "8/174, train_loss: 12.8580, time taken: 1.8652873039245605s\n",
      "9/174, train_loss: 16.1519, time taken: 1.811737298965454s\n",
      "10/174, train_loss: 15.3438, time taken: 1.9950387477874756s\n",
      "11/174, train_loss: 17.6269, time taken: 1.970409631729126s\n",
      "12/174, train_loss: 16.2619, time taken: 1.7349846363067627s\n",
      "13/174, train_loss: 23.5067, time taken: 1.793090581893921s\n",
      "14/174, train_loss: 18.2926, time taken: 1.8855876922607422s\n",
      "15/174, train_loss: 17.0476, time taken: 1.8005073070526123s\n",
      "16/174, train_loss: 18.6645, time taken: 1.783418893814087s\n",
      "17/174, train_loss: 19.6028, time taken: 1.7138755321502686s\n",
      "18/174, train_loss: 17.7728, time taken: 1.8715951442718506s\n",
      "19/174, train_loss: 18.2748, time taken: 3.92582631111145s\n",
      "20/174, train_loss: 17.8123, time taken: 1.8830397129058838s\n",
      "21/174, train_loss: 19.3954, time taken: 2.19997501373291s\n",
      "22/174, train_loss: 18.7238, time taken: 2.0979092121124268s\n",
      "23/174, train_loss: 15.8999, time taken: 1.9072504043579102s\n",
      "24/174, train_loss: 16.6689, time taken: 1.8556981086730957s\n",
      "25/174, train_loss: 18.3431, time taken: 2.1090900897979736s\n",
      "26/174, train_loss: 17.8371, time taken: 1.7936062812805176s\n",
      "27/174, train_loss: 15.2047, time taken: 1.8881499767303467s\n",
      "28/174, train_loss: 17.5547, time taken: 1.999424695968628s\n",
      "29/174, train_loss: 15.7867, time taken: 2.116302013397217s\n",
      "30/174, train_loss: 16.2714, time taken: 2.287022352218628s\n",
      "31/174, train_loss: 13.2932, time taken: 1.99342679977417s\n",
      "32/174, train_loss: 21.2613, time taken: 1.9097728729248047s\n",
      "33/174, train_loss: 19.5824, time taken: 1.8119943141937256s\n",
      "34/174, train_loss: 17.7411, time taken: 2.362313747406006s\n",
      "35/174, train_loss: 16.6582, time taken: 2.18233060836792s\n",
      "36/174, train_loss: 13.0759, time taken: 1.809546709060669s\n",
      "37/174, train_loss: 20.5943, time taken: 2.0103647708892822s\n",
      "38/174, train_loss: 16.6973, time taken: 1.8928263187408447s\n",
      "39/174, train_loss: 13.9660, time taken: 2.0874006748199463s\n",
      "40/174, train_loss: 14.9646, time taken: 2.214440107345581s\n",
      "41/174, train_loss: 17.2637, time taken: 1.8949227333068848s\n",
      "42/174, train_loss: 10.8779, time taken: 2.163383722305298s\n",
      "43/174, train_loss: 18.0037, time taken: 1.8174805641174316s\n",
      "44/174, train_loss: 16.2149, time taken: 1.8815743923187256s\n",
      "45/174, train_loss: 13.8481, time taken: 1.925046443939209s\n",
      "46/174, train_loss: 18.6026, time taken: 1.9872515201568604s\n",
      "47/174, train_loss: 20.9631, time taken: 1.9774971008300781s\n",
      "48/174, train_loss: 14.1823, time taken: 2.0158560276031494s\n",
      "49/174, train_loss: 15.5728, time taken: 1.9095652103424072s\n",
      "50/174, train_loss: 12.9566, time taken: 1.9633047580718994s\n",
      "51/174, train_loss: 17.9864, time taken: 1.822594404220581s\n",
      "52/174, train_loss: 16.2940, time taken: 1.7836742401123047s\n",
      "53/174, train_loss: 21.1515, time taken: 1.8045055866241455s\n",
      "54/174, train_loss: 14.2966, time taken: 1.8930754661560059s\n",
      "55/174, train_loss: 21.2366, time taken: 1.8948664665222168s\n",
      "56/174, train_loss: 21.3208, time taken: 2.1249167919158936s\n",
      "57/174, train_loss: 10.6807, time taken: 2.0564181804656982s\n",
      "58/174, train_loss: 13.2969, time taken: 1.7134449481964111s\n",
      "59/174, train_loss: 18.0934, time taken: 1.7943613529205322s\n",
      "60/174, train_loss: 17.1440, time taken: 1.8752915859222412s\n",
      "61/174, train_loss: 15.8734, time taken: 1.723966360092163s\n",
      "62/174, train_loss: 16.4127, time taken: 1.8782892227172852s\n",
      "63/174, train_loss: 18.2241, time taken: 1.8091330528259277s\n",
      "64/174, train_loss: 16.0910, time taken: 1.9173271656036377s\n",
      "65/174, train_loss: 17.5596, time taken: 2.0703675746917725s\n",
      "66/174, train_loss: 17.2185, time taken: 2.2037086486816406s\n",
      "67/174, train_loss: 18.4544, time taken: 1.9991273880004883s\n",
      "68/174, train_loss: 17.5742, time taken: 2.0015869140625s\n",
      "69/174, train_loss: 21.7610, time taken: 1.9035072326660156s\n",
      "70/174, train_loss: 16.0172, time taken: 1.7861406803131104s\n",
      "71/174, train_loss: 16.9772, time taken: 1.806804895401001s\n",
      "72/174, train_loss: 14.4679, time taken: 1.7750756740570068s\n",
      "73/174, train_loss: 19.0145, time taken: 2.0192244052886963s\n",
      "74/174, train_loss: 20.5254, time taken: 1.86454176902771s\n",
      "75/174, train_loss: 20.3540, time taken: 1.8976185321807861s\n",
      "76/174, train_loss: 17.8414, time taken: 1.9088420867919922s\n",
      "77/174, train_loss: 20.0650, time taken: 2.000706195831299s\n",
      "78/174, train_loss: 20.9464, time taken: 1.8930585384368896s\n",
      "79/174, train_loss: 18.8615, time taken: 2.1100008487701416s\n",
      "80/174, train_loss: 15.5501, time taken: 1.7991840839385986s\n",
      "81/174, train_loss: 11.5313, time taken: 1.7094650268554688s\n",
      "82/174, train_loss: 17.0635, time taken: 1.7705152034759521s\n",
      "83/174, train_loss: 19.9604, time taken: 1.8141801357269287s\n",
      "84/174, train_loss: 17.7028, time taken: 2.064652442932129s\n",
      "85/174, train_loss: 19.0825, time taken: 1.71553635597229s\n",
      "86/174, train_loss: 18.0310, time taken: 2.117918014526367s\n",
      "87/174, train_loss: 17.2144, time taken: 1.9550347328186035s\n",
      "88/174, train_loss: 16.2096, time taken: 1.7986454963684082s\n",
      "89/174, train_loss: 17.4450, time taken: 1.8949429988861084s\n",
      "90/174, train_loss: 14.8345, time taken: 1.8929071426391602s\n",
      "91/174, train_loss: 17.4850, time taken: 1.7281417846679688s\n",
      "92/174, train_loss: 20.2628, time taken: 3.6584389209747314s\n",
      "93/174, train_loss: 14.4531, time taken: 1.9338946342468262s\n",
      "94/174, train_loss: 21.3674, time taken: 2.289334535598755s\n",
      "95/174, train_loss: 20.5254, time taken: 2.1079325675964355s\n",
      "96/174, train_loss: 13.1724, time taken: 1.8929872512817383s\n",
      "97/174, train_loss: 16.6412, time taken: 1.9796879291534424s\n",
      "98/174, train_loss: 20.8730, time taken: 2.0930516719818115s\n",
      "99/174, train_loss: 17.0394, time taken: 2.00109601020813s\n",
      "100/174, train_loss: 14.4416, time taken: 1.915006160736084s\n",
      "101/174, train_loss: 16.5887, time taken: 2.1795403957366943s\n",
      "102/174, train_loss: 18.8838, time taken: 1.998274564743042s\n",
      "103/174, train_loss: 20.1888, time taken: 2.214789390563965s\n",
      "104/174, train_loss: 15.7538, time taken: 2.3761982917785645s\n",
      "105/174, train_loss: 14.7687, time taken: 2.086848497390747s\n",
      "106/174, train_loss: 17.9100, time taken: 1.9110236167907715s\n",
      "107/174, train_loss: 17.4792, time taken: 1.883674144744873s\n",
      "108/174, train_loss: 20.9584, time taken: 1.798649787902832s\n",
      "109/174, train_loss: 17.9256, time taken: 2.1081857681274414s\n",
      "110/174, train_loss: 18.6838, time taken: 2.0793895721435547s\n",
      "111/174, train_loss: 16.2521, time taken: 1.9136886596679688s\n",
      "112/174, train_loss: 16.3176, time taken: 1.8142337799072266s\n",
      "113/174, train_loss: 19.3535, time taken: 1.9823365211486816s\n",
      "114/174, train_loss: 18.0044, time taken: 1.8748531341552734s\n",
      "115/174, train_loss: 18.2360, time taken: 1.7872822284698486s\n",
      "116/174, train_loss: 16.5296, time taken: 2.008100986480713s\n",
      "117/174, train_loss: 15.4810, time taken: 2.0821053981781006s\n",
      "118/174, train_loss: 14.9340, time taken: 1.8217313289642334s\n",
      "119/174, train_loss: 17.1715, time taken: 1.811513900756836s\n",
      "120/174, train_loss: 19.0625, time taken: 1.7879164218902588s\n",
      "121/174, train_loss: 13.4129, time taken: 1.772782325744629s\n",
      "122/174, train_loss: 19.0120, time taken: 1.9223203659057617s\n",
      "123/174, train_loss: 17.5672, time taken: 1.8735542297363281s\n",
      "124/174, train_loss: 14.9802, time taken: 1.913125991821289s\n",
      "125/174, train_loss: 19.5305, time taken: 2.073448896408081s\n",
      "126/174, train_loss: 17.9021, time taken: 3.5143918991088867s\n",
      "127/174, train_loss: 20.6105, time taken: 1.879870891571045s\n",
      "128/174, train_loss: 16.4121, time taken: 1.8261163234710693s\n",
      "129/174, train_loss: 17.8435, time taken: 2.277843952178955s\n",
      "130/174, train_loss: 15.7097, time taken: 2.1142020225524902s\n",
      "131/174, train_loss: 13.6296, time taken: 1.8840477466583252s\n",
      "132/174, train_loss: 21.6523, time taken: 2.1121912002563477s\n",
      "133/174, train_loss: 21.0878, time taken: 1.932380199432373s\n",
      "134/174, train_loss: 15.4784, time taken: 1.90920090675354s\n",
      "135/174, train_loss: 19.4284, time taken: 2.0718157291412354s\n",
      "136/174, train_loss: 13.8105, time taken: 1.9856328964233398s\n",
      "137/174, train_loss: 13.8525, time taken: 1.8146765232086182s\n",
      "138/174, train_loss: 15.8232, time taken: 2.30544114112854s\n",
      "139/174, train_loss: 19.0642, time taken: 2.288564443588257s\n",
      "140/174, train_loss: 15.5704, time taken: 2.1004397869110107s\n",
      "141/174, train_loss: 15.7323, time taken: 1.9966866970062256s\n",
      "142/174, train_loss: 13.8473, time taken: 2.181523323059082s\n",
      "143/174, train_loss: 23.3940, time taken: 1.813281536102295s\n",
      "144/174, train_loss: 19.7241, time taken: 1.9846558570861816s\n",
      "145/174, train_loss: 17.4527, time taken: 1.8111093044281006s\n",
      "146/174, train_loss: 16.2460, time taken: 1.8877885341644287s\n",
      "147/174, train_loss: 16.5590, time taken: 1.9079797267913818s\n",
      "148/174, train_loss: 11.6194, time taken: 1.7894704341888428s\n",
      "149/174, train_loss: 17.4299, time taken: 1.7835934162139893s\n",
      "150/174, train_loss: 14.4123, time taken: 2.014760971069336s\n",
      "151/174, train_loss: 16.2912, time taken: 1.971099853515625s\n",
      "152/174, train_loss: 13.9871, time taken: 1.8997802734375s\n",
      "153/174, train_loss: 15.1779, time taken: 1.722060203552246s\n",
      "154/174, train_loss: 17.3064, time taken: 2.0909464359283447s\n",
      "155/174, train_loss: 20.7285, time taken: 2.0888350009918213s\n",
      "156/174, train_loss: 18.0303, time taken: 1.7908563613891602s\n",
      "157/174, train_loss: 16.5521, time taken: 1.8161585330963135s\n",
      "158/174, train_loss: 18.3386, time taken: 1.8684866428375244s\n",
      "159/174, train_loss: 15.5599, time taken: 2.3151376247406006s\n",
      "160/174, train_loss: 15.5347, time taken: 1.8988685607910156s\n",
      "161/174, train_loss: 20.8620, time taken: 1.8025929927825928s\n",
      "162/174, train_loss: 17.0020, time taken: 2.1989965438842773s\n",
      "163/174, train_loss: 16.2245, time taken: 2.162935972213745s\n",
      "164/174, train_loss: 17.5634, time taken: 1.7068877220153809s\n",
      "165/174, train_loss: 16.0147, time taken: 2.006270170211792s\n",
      "166/174, train_loss: 14.6445, time taken: 1.969789743423462s\n",
      "167/174, train_loss: 16.8115, time taken: 2.0177457332611084s\n",
      "168/174, train_loss: 17.3997, time taken: 1.9182863235473633s\n",
      "169/174, train_loss: 18.2805, time taken: 1.9641437530517578s\n",
      "170/174, train_loss: 16.3346, time taken: 1.794130802154541s\n",
      "171/174, train_loss: 13.8784, time taken: 1.8362152576446533s\n",
      "172/174, train_loss: 21.5321, time taken: 2.2108638286590576s\n",
      "173/174, train_loss: 14.0282, time taken: 2.0162084102630615s\n",
      "174/174, train_loss: 16.6695, time taken: 2.2066147327423096s\n",
      "175/174, train_loss: 16.3947, time taken: 1.3212306499481201s\n",
      "epoch 17 average loss: 17.1601\n",
      "Entering Validation for epoch: 17\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 17 Validation avg loss: 10.0765, time taken: 1.1065707206726074s\n",
      "----------\n",
      "epoch 18/2000\n",
      "1/174, train_loss: 15.9607, time taken: 8.303009271621704s\n",
      "2/174, train_loss: 18.7034, time taken: 2.1032114028930664s\n",
      "3/174, train_loss: 15.7327, time taken: 1.9681339263916016s\n",
      "4/174, train_loss: 18.0990, time taken: 1.909336805343628s\n",
      "5/174, train_loss: 17.4022, time taken: 1.9967987537384033s\n",
      "6/174, train_loss: 20.9077, time taken: 2.0136146545410156s\n",
      "7/174, train_loss: 13.6862, time taken: 2.197840929031372s\n",
      "8/174, train_loss: 18.2968, time taken: 1.9543793201446533s\n",
      "9/174, train_loss: 17.7451, time taken: 1.8184764385223389s\n",
      "10/174, train_loss: 14.3178, time taken: 1.978360891342163s\n",
      "11/174, train_loss: 17.8708, time taken: 1.7157118320465088s\n",
      "12/174, train_loss: 17.9873, time taken: 1.7041585445404053s\n",
      "13/174, train_loss: 19.9682, time taken: 1.9741034507751465s\n",
      "14/174, train_loss: 14.8365, time taken: 2.100179672241211s\n",
      "15/174, train_loss: 19.0506, time taken: 2.0884931087493896s\n",
      "16/174, train_loss: 15.9078, time taken: 1.923166036605835s\n",
      "17/174, train_loss: 25.0167, time taken: 1.700037956237793s\n",
      "18/174, train_loss: 18.2241, time taken: 1.976219892501831s\n",
      "19/174, train_loss: 16.9499, time taken: 1.7858119010925293s\n",
      "20/174, train_loss: 16.0328, time taken: 1.817021369934082s\n",
      "21/174, train_loss: 14.8376, time taken: 2.1632721424102783s\n",
      "22/174, train_loss: 12.3963, time taken: 1.8320724964141846s\n",
      "23/174, train_loss: 19.8723, time taken: 1.799656867980957s\n",
      "24/174, train_loss: 24.6113, time taken: 1.9990241527557373s\n",
      "25/174, train_loss: 16.0008, time taken: 2.007402181625366s\n",
      "26/174, train_loss: 12.4630, time taken: 1.9354732036590576s\n",
      "27/174, train_loss: 20.5962, time taken: 2.2097864151000977s\n",
      "28/174, train_loss: 19.7783, time taken: 2.096106767654419s\n",
      "29/174, train_loss: 15.1992, time taken: 1.8015975952148438s\n",
      "30/174, train_loss: 16.2937, time taken: 1.7095091342926025s\n",
      "31/174, train_loss: 14.6442, time taken: 1.7878413200378418s\n",
      "32/174, train_loss: 17.1624, time taken: 3.688396692276001s\n",
      "33/174, train_loss: 15.6910, time taken: 1.8152356147766113s\n",
      "34/174, train_loss: 22.1612, time taken: 1.8048675060272217s\n",
      "35/174, train_loss: 15.5086, time taken: 2.4725759029388428s\n",
      "36/174, train_loss: 17.8363, time taken: 2.026978015899658s\n",
      "37/174, train_loss: 17.1899, time taken: 2.432831287384033s\n",
      "38/174, train_loss: 18.8236, time taken: 1.9823720455169678s\n",
      "39/174, train_loss: 19.2847, time taken: 1.877640962600708s\n",
      "40/174, train_loss: 18.7930, time taken: 2.02512264251709s\n",
      "41/174, train_loss: 13.9595, time taken: 1.8016984462738037s\n",
      "42/174, train_loss: 19.1461, time taken: 2.292314052581787s\n",
      "43/174, train_loss: 18.1411, time taken: 2.1674418449401855s\n",
      "44/174, train_loss: 20.6206, time taken: 1.908151626586914s\n",
      "45/174, train_loss: 15.0258, time taken: 2.1027960777282715s\n",
      "46/174, train_loss: 14.1431, time taken: 1.9045209884643555s\n",
      "47/174, train_loss: 20.8301, time taken: 1.768216848373413s\n",
      "48/174, train_loss: 13.5032, time taken: 1.8447744846343994s\n",
      "49/174, train_loss: 19.8418, time taken: 2.1221160888671875s\n",
      "50/174, train_loss: 15.9934, time taken: 1.8000104427337646s\n",
      "51/174, train_loss: 15.8251, time taken: 1.963585376739502s\n",
      "52/174, train_loss: 16.4744, time taken: 2.2194952964782715s\n",
      "53/174, train_loss: 16.8647, time taken: 1.9906713962554932s\n",
      "54/174, train_loss: 18.6856, time taken: 2.1076316833496094s\n",
      "55/174, train_loss: 16.3376, time taken: 2.270031452178955s\n",
      "56/174, train_loss: 17.5322, time taken: 1.8293383121490479s\n",
      "57/174, train_loss: 17.6151, time taken: 1.9809215068817139s\n",
      "58/174, train_loss: 17.6299, time taken: 1.886099100112915s\n",
      "59/174, train_loss: 15.5444, time taken: 2.0103094577789307s\n",
      "60/174, train_loss: 17.4130, time taken: 1.9752874374389648s\n",
      "61/174, train_loss: 14.8280, time taken: 2.009488821029663s\n",
      "62/174, train_loss: 16.5694, time taken: 1.9901578426361084s\n",
      "63/174, train_loss: 14.9482, time taken: 2.280667781829834s\n",
      "64/174, train_loss: 18.3715, time taken: 1.8239312171936035s\n",
      "65/174, train_loss: 15.2170, time taken: 1.8681640625s\n",
      "66/174, train_loss: 19.7516, time taken: 1.7277743816375732s\n",
      "67/174, train_loss: 19.8808, time taken: 1.8970811367034912s\n",
      "68/174, train_loss: 21.7140, time taken: 2.001711130142212s\n",
      "69/174, train_loss: 17.2113, time taken: 1.891524314880371s\n",
      "70/174, train_loss: 15.8560, time taken: 2.00402569770813s\n",
      "71/174, train_loss: 15.9706, time taken: 1.7765486240386963s\n",
      "72/174, train_loss: 17.4060, time taken: 1.784578561782837s\n",
      "73/174, train_loss: 18.5626, time taken: 1.7926266193389893s\n",
      "74/174, train_loss: 28.5471, time taken: 1.7999110221862793s\n",
      "75/174, train_loss: 20.1632, time taken: 1.8164172172546387s\n",
      "76/174, train_loss: 15.0614, time taken: 1.8869743347167969s\n",
      "77/174, train_loss: 17.0982, time taken: 1.782968282699585s\n",
      "78/174, train_loss: 19.0504, time taken: 1.9913301467895508s\n",
      "79/174, train_loss: 21.3636, time taken: 4.115689992904663s\n",
      "80/174, train_loss: 13.8956, time taken: 2.2941577434539795s\n",
      "81/174, train_loss: 14.8826, time taken: 2.0947253704071045s\n",
      "82/174, train_loss: 14.6922, time taken: 1.8828251361846924s\n",
      "83/174, train_loss: 16.0339, time taken: 1.9085137844085693s\n",
      "84/174, train_loss: 22.4051, time taken: 1.9937632083892822s\n",
      "85/174, train_loss: 18.6139, time taken: 1.8146681785583496s\n",
      "86/174, train_loss: 19.6888, time taken: 1.9994127750396729s\n",
      "87/174, train_loss: 12.7442, time taken: 2.0125110149383545s\n",
      "88/174, train_loss: 24.9976, time taken: 1.8835091590881348s\n",
      "89/174, train_loss: 17.4085, time taken: 1.9329323768615723s\n",
      "90/174, train_loss: 16.8689, time taken: 1.9983747005462646s\n",
      "91/174, train_loss: 13.3443, time taken: 1.8749048709869385s\n",
      "92/174, train_loss: 15.2763, time taken: 1.9190967082977295s\n",
      "93/174, train_loss: 16.5109, time taken: 1.765803575515747s\n",
      "94/174, train_loss: 14.7939, time taken: 2.185370922088623s\n",
      "95/174, train_loss: 21.2312, time taken: 1.81223464012146s\n",
      "96/174, train_loss: 19.4506, time taken: 1.7078869342803955s\n",
      "97/174, train_loss: 16.2063, time taken: 1.7818970680236816s\n",
      "98/174, train_loss: 14.5484, time taken: 1.7119462490081787s\n",
      "99/174, train_loss: 24.5894, time taken: 2.2761104106903076s\n",
      "100/174, train_loss: 20.7173, time taken: 1.8052303791046143s\n",
      "101/174, train_loss: 14.9662, time taken: 1.7702710628509521s\n",
      "102/174, train_loss: 21.7140, time taken: 1.6232774257659912s\n",
      "103/174, train_loss: 13.7314, time taken: 1.7902228832244873s\n",
      "104/174, train_loss: 16.1637, time taken: 1.780731201171875s\n",
      "105/174, train_loss: 12.8711, time taken: 1.797050952911377s\n",
      "106/174, train_loss: 14.7103, time taken: 1.812614917755127s\n",
      "107/174, train_loss: 20.2042, time taken: 1.7094495296478271s\n",
      "108/174, train_loss: 18.9541, time taken: 1.7988381385803223s\n",
      "109/174, train_loss: 19.0330, time taken: 1.8039000034332275s\n",
      "110/174, train_loss: 14.4480, time taken: 1.9348468780517578s\n",
      "111/174, train_loss: 13.6581, time taken: 2.0635342597961426s\n",
      "112/174, train_loss: 20.1083, time taken: 1.9325296878814697s\n",
      "113/174, train_loss: 20.5954, time taken: 1.7949166297912598s\n",
      "114/174, train_loss: 18.4472, time taken: 1.8047211170196533s\n",
      "115/174, train_loss: 18.6501, time taken: 1.8075385093688965s\n",
      "116/174, train_loss: 14.2934, time taken: 2.179685354232788s\n",
      "117/174, train_loss: 18.9206, time taken: 1.709916114807129s\n",
      "118/174, train_loss: 19.4619, time taken: 1.8149738311767578s\n",
      "119/174, train_loss: 17.8440, time taken: 1.8789186477661133s\n",
      "120/174, train_loss: 17.1327, time taken: 1.8120861053466797s\n",
      "121/174, train_loss: 14.7844, time taken: 2.165961742401123s\n",
      "122/174, train_loss: 21.0739, time taken: 1.824721097946167s\n",
      "123/174, train_loss: 17.2700, time taken: 1.8097262382507324s\n",
      "124/174, train_loss: 18.0044, time taken: 1.9053547382354736s\n",
      "125/174, train_loss: 24.1425, time taken: 2.193854331970215s\n",
      "126/174, train_loss: 15.1096, time taken: 2.0038628578186035s\n",
      "127/174, train_loss: 18.8543, time taken: 2.1132307052612305s\n",
      "128/174, train_loss: 15.0066, time taken: 2.0138020515441895s\n",
      "129/174, train_loss: 17.7043, time taken: 1.7290711402893066s\n",
      "130/174, train_loss: 18.9352, time taken: 2.285491466522217s\n",
      "131/174, train_loss: 16.3570, time taken: 1.8178718090057373s\n",
      "132/174, train_loss: 13.7044, time taken: 1.798274278640747s\n",
      "133/174, train_loss: 19.9216, time taken: 1.836984395980835s\n",
      "134/174, train_loss: 17.4224, time taken: 1.9001810550689697s\n",
      "135/174, train_loss: 15.3939, time taken: 1.7161076068878174s\n",
      "136/174, train_loss: 16.8592, time taken: 1.8821423053741455s\n",
      "137/174, train_loss: 20.2050, time taken: 2.0923478603363037s\n",
      "138/174, train_loss: 18.0915, time taken: 1.9945240020751953s\n",
      "139/174, train_loss: 23.5748, time taken: 1.8132333755493164s\n",
      "140/174, train_loss: 15.3850, time taken: 1.8877067565917969s\n",
      "141/174, train_loss: 20.1517, time taken: 1.8043229579925537s\n",
      "142/174, train_loss: 18.7189, time taken: 1.8831257820129395s\n",
      "143/174, train_loss: 20.6758, time taken: 1.804504632949829s\n",
      "144/174, train_loss: 15.6583, time taken: 1.807032585144043s\n",
      "145/174, train_loss: 17.7999, time taken: 1.9030046463012695s\n",
      "146/174, train_loss: 12.8212, time taken: 1.8685917854309082s\n",
      "147/174, train_loss: 15.8585, time taken: 1.917536735534668s\n",
      "148/174, train_loss: 16.3166, time taken: 1.8710463047027588s\n",
      "149/174, train_loss: 22.4100, time taken: 1.905421257019043s\n",
      "150/174, train_loss: 19.6730, time taken: 1.9088494777679443s\n",
      "151/174, train_loss: 15.4299, time taken: 1.9812521934509277s\n",
      "152/174, train_loss: 13.6728, time taken: 3.7177224159240723s\n",
      "153/174, train_loss: 17.5499, time taken: 2.166013717651367s\n",
      "154/174, train_loss: 14.4542, time taken: 1.9315035343170166s\n",
      "155/174, train_loss: 20.1690, time taken: 1.8634388446807861s\n",
      "156/174, train_loss: 11.7253, time taken: 1.8453867435455322s\n",
      "157/174, train_loss: 14.8179, time taken: 1.7190723419189453s\n",
      "158/174, train_loss: 15.6741, time taken: 2.2182512283325195s\n",
      "159/174, train_loss: 18.1802, time taken: 1.7356221675872803s\n",
      "160/174, train_loss: 18.6073, time taken: 1.8876104354858398s\n",
      "161/174, train_loss: 21.6760, time taken: 2.0811290740966797s\n",
      "162/174, train_loss: 19.4296, time taken: 2.1201260089874268s\n",
      "163/174, train_loss: 17.6652, time taken: 2.008894205093384s\n",
      "164/174, train_loss: 17.1816, time taken: 2.005291223526001s\n",
      "165/174, train_loss: 17.4242, time taken: 1.8170464038848877s\n",
      "166/174, train_loss: 16.2666, time taken: 1.8983180522918701s\n",
      "167/174, train_loss: 15.4176, time taken: 2.1693074703216553s\n",
      "168/174, train_loss: 16.6206, time taken: 1.822908878326416s\n",
      "169/174, train_loss: 15.6621, time taken: 2.0028276443481445s\n",
      "170/174, train_loss: 23.5582, time taken: 1.890282154083252s\n",
      "171/174, train_loss: 29.9588, time taken: 1.7855689525604248s\n",
      "172/174, train_loss: 19.4868, time taken: 1.881174087524414s\n",
      "173/174, train_loss: 16.7568, time taken: 2.018671751022339s\n",
      "174/174, train_loss: 16.9630, time taken: 1.819582223892212s\n",
      "175/174, train_loss: 19.5729, time taken: 1.4861979484558105s\n",
      "epoch 18 average loss: 17.6409\n",
      "----------\n",
      "epoch 19/2000\n",
      "1/174, train_loss: 22.4332, time taken: 8.801281929016113s\n",
      "2/174, train_loss: 27.7631, time taken: 2.2114455699920654s\n",
      "3/174, train_loss: 15.9396, time taken: 1.9660563468933105s\n",
      "4/174, train_loss: 15.3247, time taken: 1.906231164932251s\n",
      "5/174, train_loss: 18.8312, time taken: 1.7979648113250732s\n",
      "6/174, train_loss: 24.7306, time taken: 1.8895061016082764s\n",
      "7/174, train_loss: 19.0619, time taken: 1.7063968181610107s\n",
      "8/174, train_loss: 16.6330, time taken: 2.3082919120788574s\n",
      "9/174, train_loss: 16.9195, time taken: 1.9157207012176514s\n",
      "10/174, train_loss: 13.3626, time taken: 1.9677982330322266s\n",
      "11/174, train_loss: 14.2730, time taken: 1.89878249168396s\n",
      "12/174, train_loss: 14.9911, time taken: 2.1914050579071045s\n",
      "13/174, train_loss: 16.9935, time taken: 1.8818652629852295s\n",
      "14/174, train_loss: 16.6757, time taken: 2.096576690673828s\n",
      "15/174, train_loss: 10.8679, time taken: 2.008051872253418s\n",
      "16/174, train_loss: 11.6431, time taken: 1.8212213516235352s\n",
      "17/174, train_loss: 21.2166, time taken: 3.574819803237915s\n",
      "18/174, train_loss: 14.5124, time taken: 2.1268701553344727s\n",
      "19/174, train_loss: 22.1196, time taken: 1.8592498302459717s\n",
      "20/174, train_loss: 15.8564, time taken: 2.2163281440734863s\n",
      "21/174, train_loss: 16.7138, time taken: 1.9794301986694336s\n",
      "22/174, train_loss: 13.9366, time taken: 2.2151854038238525s\n",
      "23/174, train_loss: 19.0841, time taken: 1.9014513492584229s\n",
      "24/174, train_loss: 16.9792, time taken: 1.809142827987671s\n",
      "25/174, train_loss: 18.7385, time taken: 1.8197638988494873s\n",
      "26/174, train_loss: 15.4598, time taken: 1.8160760402679443s\n",
      "27/174, train_loss: 18.7763, time taken: 1.9012465476989746s\n",
      "28/174, train_loss: 17.7132, time taken: 1.801527500152588s\n",
      "29/174, train_loss: 15.9172, time taken: 2.0006322860717773s\n",
      "30/174, train_loss: 16.8366, time taken: 1.998039960861206s\n",
      "31/174, train_loss: 23.3919, time taken: 2.0101377964019775s\n",
      "32/174, train_loss: 15.0729, time taken: 1.9862613677978516s\n",
      "33/174, train_loss: 17.8186, time taken: 1.9096367359161377s\n",
      "34/174, train_loss: 20.7796, time taken: 2.4715981483459473s\n",
      "35/174, train_loss: 16.4582, time taken: 2.1206436157226562s\n",
      "36/174, train_loss: 21.4416, time taken: 1.9853322505950928s\n",
      "37/174, train_loss: 15.7506, time taken: 2.2123970985412598s\n",
      "38/174, train_loss: 16.0874, time taken: 2.095057725906372s\n",
      "39/174, train_loss: 16.1680, time taken: 1.7937614917755127s\n",
      "40/174, train_loss: 16.0018, time taken: 1.9760963916778564s\n",
      "41/174, train_loss: 18.8668, time taken: 1.8217594623565674s\n",
      "42/174, train_loss: 14.0822, time taken: 1.8001224994659424s\n",
      "43/174, train_loss: 16.6734, time taken: 1.9626562595367432s\n",
      "44/174, train_loss: 17.6253, time taken: 1.9106898307800293s\n",
      "45/174, train_loss: 16.2541, time taken: 1.8035075664520264s\n",
      "46/174, train_loss: 16.5984, time taken: 1.8846862316131592s\n",
      "47/174, train_loss: 15.6356, time taken: 1.9164419174194336s\n",
      "48/174, train_loss: 14.2313, time taken: 1.8893394470214844s\n",
      "49/174, train_loss: 21.5358, time taken: 1.874211072921753s\n",
      "50/174, train_loss: 18.8661, time taken: 1.7254219055175781s\n",
      "51/174, train_loss: 22.1140, time taken: 1.792348861694336s\n",
      "52/174, train_loss: 16.0557, time taken: 1.794633388519287s\n",
      "53/174, train_loss: 22.2622, time taken: 1.7888834476470947s\n",
      "54/174, train_loss: 14.5728, time taken: 2.0926761627197266s\n",
      "55/174, train_loss: 15.9648, time taken: 1.7994325160980225s\n",
      "56/174, train_loss: 22.9493, time taken: 2.0091631412506104s\n",
      "57/174, train_loss: 16.7006, time taken: 1.7901747226715088s\n",
      "58/174, train_loss: 13.6010, time taken: 1.8810548782348633s\n",
      "59/174, train_loss: 16.3477, time taken: 1.7043328285217285s\n",
      "60/174, train_loss: 15.6368, time taken: 1.7196786403656006s\n",
      "61/174, train_loss: 21.6844, time taken: 1.6990926265716553s\n",
      "62/174, train_loss: 16.0780, time taken: 1.8068468570709229s\n",
      "63/174, train_loss: 20.2167, time taken: 1.7053122520446777s\n",
      "64/174, train_loss: 16.7281, time taken: 1.9101617336273193s\n",
      "65/174, train_loss: 14.8675, time taken: 1.885690689086914s\n",
      "66/174, train_loss: 14.8141, time taken: 1.995974063873291s\n",
      "67/174, train_loss: 16.2464, time taken: 1.9109468460083008s\n",
      "68/174, train_loss: 16.0241, time taken: 1.907837152481079s\n",
      "69/174, train_loss: 16.4820, time taken: 2.0011937618255615s\n",
      "70/174, train_loss: 18.6207, time taken: 2.060805082321167s\n",
      "71/174, train_loss: 17.1893, time taken: 1.908179521560669s\n",
      "72/174, train_loss: 15.0183, time taken: 1.9869515895843506s\n",
      "73/174, train_loss: 17.3371, time taken: 1.7271575927734375s\n",
      "74/174, train_loss: 18.1045, time taken: 1.7825970649719238s\n",
      "75/174, train_loss: 20.6603, time taken: 1.80413818359375s\n",
      "76/174, train_loss: 17.7631, time taken: 1.7986013889312744s\n",
      "77/174, train_loss: 18.1586, time taken: 1.8800899982452393s\n",
      "78/174, train_loss: 15.3761, time taken: 1.9111509323120117s\n",
      "79/174, train_loss: 12.5966, time taken: 1.800628423690796s\n",
      "80/174, train_loss: 16.4461, time taken: 1.810190200805664s\n",
      "81/174, train_loss: 18.7922, time taken: 2.1549530029296875s\n",
      "82/174, train_loss: 15.6370, time taken: 2.028873920440674s\n",
      "83/174, train_loss: 18.5128, time taken: 2.0940165519714355s\n",
      "84/174, train_loss: 16.8919, time taken: 1.8968284130096436s\n",
      "85/174, train_loss: 21.8295, time taken: 2.0698790550231934s\n",
      "86/174, train_loss: 13.7988, time taken: 2.0147712230682373s\n",
      "87/174, train_loss: 23.6134, time taken: 1.8852601051330566s\n",
      "88/174, train_loss: 18.5161, time taken: 1.8013708591461182s\n",
      "89/174, train_loss: 12.7036, time taken: 1.6934401988983154s\n",
      "90/174, train_loss: 18.4538, time taken: 1.7063839435577393s\n",
      "91/174, train_loss: 24.3322, time taken: 1.7733638286590576s\n",
      "92/174, train_loss: 19.3650, time taken: 1.8208882808685303s\n",
      "93/174, train_loss: 12.9118, time taken: 1.901740312576294s\n",
      "94/174, train_loss: 16.8562, time taken: 1.699556827545166s\n",
      "95/174, train_loss: 16.5239, time taken: 2.1729259490966797s\n",
      "96/174, train_loss: 16.1253, time taken: 1.8223233222961426s\n",
      "97/174, train_loss: 17.1376, time taken: 3.086224317550659s\n",
      "98/174, train_loss: 16.0814, time taken: 1.6746392250061035s\n",
      "99/174, train_loss: 19.0541, time taken: 1.8378300666809082s\n",
      "100/174, train_loss: 14.8664, time taken: 1.6787171363830566s\n",
      "101/174, train_loss: 20.0278, time taken: 1.9012093544006348s\n",
      "102/174, train_loss: 14.1839, time taken: 1.7992994785308838s\n",
      "103/174, train_loss: 16.3164, time taken: 1.9861245155334473s\n",
      "104/174, train_loss: 15.4665, time taken: 2.0134708881378174s\n",
      "105/174, train_loss: 18.9163, time taken: 1.8009190559387207s\n",
      "106/174, train_loss: 17.2136, time taken: 1.8227300643920898s\n",
      "107/174, train_loss: 19.0575, time taken: 1.7753925323486328s\n",
      "108/174, train_loss: 18.9448, time taken: 1.8242406845092773s\n",
      "109/174, train_loss: 13.6425, time taken: 2.4724161624908447s\n",
      "110/174, train_loss: 12.7908, time taken: 2.107009172439575s\n",
      "111/174, train_loss: 12.8355, time taken: 1.8960762023925781s\n",
      "112/174, train_loss: 17.4308, time taken: 2.0150821208953857s\n",
      "113/174, train_loss: 15.8918, time taken: 1.899122953414917s\n",
      "114/174, train_loss: 13.1540, time taken: 2.10068678855896s\n",
      "115/174, train_loss: 18.2172, time taken: 1.725022315979004s\n",
      "116/174, train_loss: 17.9660, time taken: 1.8001794815063477s\n",
      "117/174, train_loss: 18.1667, time taken: 1.9671235084533691s\n",
      "118/174, train_loss: 21.7814, time taken: 1.630272626876831s\n",
      "119/174, train_loss: 17.9047, time taken: 1.8902640342712402s\n",
      "120/174, train_loss: 19.3969, time taken: 1.8728504180908203s\n",
      "121/174, train_loss: 14.1856, time taken: 1.8109405040740967s\n",
      "122/174, train_loss: 17.9973, time taken: 1.7053444385528564s\n",
      "123/174, train_loss: 15.6263, time taken: 1.7935974597930908s\n",
      "124/174, train_loss: 21.5239, time taken: 2.1054904460906982s\n",
      "125/174, train_loss: 24.3504, time taken: 1.8014564514160156s\n",
      "126/174, train_loss: 12.4241, time taken: 1.8772532939910889s\n",
      "127/174, train_loss: 14.9062, time taken: 1.820953130722046s\n",
      "128/174, train_loss: 16.7839, time taken: 1.853060245513916s\n",
      "129/174, train_loss: 20.0621, time taken: 1.7208313941955566s\n",
      "130/174, train_loss: 23.1862, time taken: 1.8083617687225342s\n",
      "131/174, train_loss: 17.5855, time taken: 1.9596202373504639s\n",
      "132/174, train_loss: 15.8010, time taken: 1.7916545867919922s\n",
      "133/174, train_loss: 16.0905, time taken: 1.8069710731506348s\n",
      "134/174, train_loss: 15.7881, time taken: 3.2150444984436035s\n",
      "135/174, train_loss: 22.2770, time taken: 1.8966479301452637s\n",
      "136/174, train_loss: 16.1850, time taken: 1.8060276508331299s\n",
      "137/174, train_loss: 15.5560, time taken: 2.2964816093444824s\n",
      "138/174, train_loss: 13.6970, time taken: 2.193317174911499s\n",
      "139/174, train_loss: 15.1157, time taken: 1.865312099456787s\n",
      "140/174, train_loss: 15.8279, time taken: 1.7393429279327393s\n",
      "141/174, train_loss: 19.2725, time taken: 2.282437801361084s\n",
      "142/174, train_loss: 16.5417, time taken: 1.7941319942474365s\n",
      "143/174, train_loss: 14.1112, time taken: 2.0764753818511963s\n",
      "144/174, train_loss: 17.6763, time taken: 1.8195610046386719s\n",
      "145/174, train_loss: 18.3919, time taken: 1.8678832054138184s\n",
      "146/174, train_loss: 21.9367, time taken: 1.80875825881958s\n",
      "147/174, train_loss: 13.6387, time taken: 1.8225808143615723s\n",
      "148/174, train_loss: 17.2861, time taken: 2.0114364624023438s\n",
      "149/174, train_loss: 14.6439, time taken: 1.9892125129699707s\n",
      "150/174, train_loss: 17.1822, time taken: 1.806316614151001s\n",
      "151/174, train_loss: 16.9369, time taken: 1.9130330085754395s\n",
      "152/174, train_loss: 20.4725, time taken: 1.771324872970581s\n",
      "153/174, train_loss: 15.5280, time taken: 1.6220951080322266s\n",
      "154/174, train_loss: 16.1256, time taken: 1.7752583026885986s\n",
      "155/174, train_loss: 18.2091, time taken: 1.997734785079956s\n",
      "156/174, train_loss: 15.9030, time taken: 2.21744704246521s\n",
      "157/174, train_loss: 18.9548, time taken: 1.7869141101837158s\n",
      "158/174, train_loss: 18.0289, time taken: 1.7901716232299805s\n",
      "159/174, train_loss: 18.6087, time taken: 1.7211275100708008s\n",
      "160/174, train_loss: 18.9949, time taken: 1.9857585430145264s\n",
      "161/174, train_loss: 19.0039, time taken: 1.6966743469238281s\n",
      "162/174, train_loss: 18.2600, time taken: 1.8723156452178955s\n",
      "163/174, train_loss: 23.9886, time taken: 1.7963881492614746s\n",
      "164/174, train_loss: 23.7166, time taken: 1.8202497959136963s\n",
      "165/174, train_loss: 14.1664, time taken: 1.7843749523162842s\n",
      "166/174, train_loss: 17.8748, time taken: 1.7064249515533447s\n",
      "167/174, train_loss: 19.1963, time taken: 1.8765935897827148s\n",
      "168/174, train_loss: 18.4833, time taken: 1.909515619277954s\n",
      "169/174, train_loss: 20.0747, time taken: 1.705251693725586s\n",
      "170/174, train_loss: 20.4274, time taken: 1.684737205505371s\n",
      "171/174, train_loss: 15.6291, time taken: 1.615800142288208s\n",
      "172/174, train_loss: 19.6704, time taken: 1.9727814197540283s\n",
      "173/174, train_loss: 23.7143, time taken: 1.9015607833862305s\n",
      "174/174, train_loss: 17.4388, time taken: 1.8104467391967773s\n",
      "175/174, train_loss: 16.4932, time taken: 1.3064024448394775s\n",
      "epoch 19 average loss: 17.4807\n",
      "Entering Validation for epoch: 19\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 19 Validation avg loss: 11.7966, time taken: 1.1422405242919922s\n",
      "----------\n",
      "epoch 20/2000\n",
      "1/174, train_loss: 17.1978, time taken: 8.2152259349823s\n",
      "2/174, train_loss: 17.8798, time taken: 2.420628786087036s\n",
      "3/174, train_loss: 19.9594, time taken: 1.887455701828003s\n",
      "4/174, train_loss: 16.8543, time taken: 1.98594331741333s\n",
      "5/174, train_loss: 16.1331, time taken: 1.815535545349121s\n",
      "6/174, train_loss: 21.9025, time taken: 2.1825978755950928s\n",
      "7/174, train_loss: 17.6248, time taken: 1.7851943969726562s\n",
      "8/174, train_loss: 14.5139, time taken: 1.8200836181640625s\n",
      "9/174, train_loss: 19.0100, time taken: 1.9030869007110596s\n",
      "10/174, train_loss: 16.3184, time taken: 1.7146563529968262s\n",
      "11/174, train_loss: 21.0152, time taken: 1.939399003982544s\n",
      "12/174, train_loss: 13.5313, time taken: 1.8714573383331299s\n",
      "13/174, train_loss: 17.8750, time taken: 1.795344591140747s\n",
      "14/174, train_loss: 18.9375, time taken: 1.799220085144043s\n",
      "15/174, train_loss: 12.5534, time taken: 2.198275327682495s\n",
      "16/174, train_loss: 16.6720, time taken: 2.0105316638946533s\n",
      "17/174, train_loss: 14.8443, time taken: 1.8749678134918213s\n",
      "18/174, train_loss: 18.1680, time taken: 2.2137110233306885s\n",
      "19/174, train_loss: 19.0472, time taken: 1.8863608837127686s\n",
      "20/174, train_loss: 18.3501, time taken: 2.109807252883911s\n",
      "21/174, train_loss: 20.1409, time taken: 2.097491979598999s\n",
      "22/174, train_loss: 17.1691, time taken: 1.7940468788146973s\n",
      "23/174, train_loss: 15.6316, time taken: 1.8702211380004883s\n",
      "24/174, train_loss: 18.0113, time taken: 1.9105567932128906s\n",
      "25/174, train_loss: 15.8754, time taken: 1.800288200378418s\n",
      "26/174, train_loss: 19.6027, time taken: 1.7941322326660156s\n",
      "27/174, train_loss: 19.8125, time taken: 1.890559196472168s\n",
      "28/174, train_loss: 16.5485, time taken: 1.992556095123291s\n",
      "29/174, train_loss: 21.7366, time taken: 1.7141141891479492s\n",
      "30/174, train_loss: 15.8302, time taken: 1.7710514068603516s\n",
      "31/174, train_loss: 19.3458, time taken: 1.699655294418335s\n",
      "32/174, train_loss: 18.2613, time taken: 1.7237532138824463s\n",
      "33/174, train_loss: 14.2043, time taken: 1.784639835357666s\n",
      "34/174, train_loss: 15.0897, time taken: 1.8077278137207031s\n",
      "35/174, train_loss: 16.7810, time taken: 1.991391658782959s\n",
      "36/174, train_loss: 19.5825, time taken: 1.8764021396636963s\n",
      "37/174, train_loss: 21.1134, time taken: 1.7982399463653564s\n",
      "38/174, train_loss: 17.1491, time taken: 1.9893972873687744s\n",
      "39/174, train_loss: 21.1197, time taken: 1.790900468826294s\n",
      "40/174, train_loss: 21.1168, time taken: 1.8239338397979736s\n",
      "41/174, train_loss: 16.5603, time taken: 1.9642813205718994s\n",
      "42/174, train_loss: 15.8738, time taken: 2.008388042449951s\n",
      "43/174, train_loss: 22.1307, time taken: 1.8102123737335205s\n",
      "44/174, train_loss: 16.3238, time taken: 1.7996702194213867s\n",
      "45/174, train_loss: 20.6986, time taken: 1.713491439819336s\n",
      "46/174, train_loss: 17.3768, time taken: 1.9866619110107422s\n",
      "47/174, train_loss: 21.6970, time taken: 1.8717620372772217s\n",
      "48/174, train_loss: 14.8304, time taken: 1.7149999141693115s\n",
      "49/174, train_loss: 19.2066, time taken: 1.7001824378967285s\n",
      "50/174, train_loss: 18.3389, time taken: 3.59100604057312s\n",
      "51/174, train_loss: 15.5290, time taken: 1.7754065990447998s\n",
      "52/174, train_loss: 13.5208, time taken: 1.7205090522766113s\n",
      "53/174, train_loss: 18.9907, time taken: 2.5935351848602295s\n",
      "54/174, train_loss: 18.5789, time taken: 2.1203536987304688s\n",
      "55/174, train_loss: 19.9283, time taken: 1.9418725967407227s\n",
      "56/174, train_loss: 17.3429, time taken: 1.959862470626831s\n",
      "57/174, train_loss: 17.5966, time taken: 1.7091913223266602s\n",
      "58/174, train_loss: 26.8723, time taken: 2.0077550411224365s\n",
      "59/174, train_loss: 18.2211, time taken: 1.7812182903289795s\n",
      "60/174, train_loss: 20.9595, time taken: 2.1065292358398438s\n",
      "61/174, train_loss: 14.2610, time taken: 2.495192289352417s\n",
      "62/174, train_loss: 20.6667, time taken: 2.397087812423706s\n",
      "63/174, train_loss: 14.4201, time taken: 2.111079216003418s\n",
      "64/174, train_loss: 18.0476, time taken: 2.2698347568511963s\n",
      "65/174, train_loss: 16.2336, time taken: 2.2208547592163086s\n",
      "66/174, train_loss: 20.3386, time taken: 1.883751630783081s\n",
      "67/174, train_loss: 13.6691, time taken: 1.9860296249389648s\n",
      "68/174, train_loss: 15.3176, time taken: 2.02538800239563s\n",
      "69/174, train_loss: 15.9830, time taken: 2.0496814250946045s\n",
      "70/174, train_loss: 12.5309, time taken: 1.936434030532837s\n",
      "71/174, train_loss: 30.6668, time taken: 1.8780720233917236s\n",
      "72/174, train_loss: 20.8505, time taken: 1.8829989433288574s\n",
      "73/174, train_loss: 17.5338, time taken: 1.8221426010131836s\n",
      "74/174, train_loss: 16.6430, time taken: 1.799903392791748s\n",
      "75/174, train_loss: 15.4411, time taken: 1.9008448123931885s\n",
      "76/174, train_loss: 14.5800, time taken: 1.9162523746490479s\n",
      "77/174, train_loss: 18.1246, time taken: 1.817626953125s\n",
      "78/174, train_loss: 16.2987, time taken: 1.9793860912322998s\n",
      "79/174, train_loss: 24.0982, time taken: 1.7795045375823975s\n",
      "80/174, train_loss: 21.1796, time taken: 1.8031351566314697s\n",
      "81/174, train_loss: 17.0386, time taken: 1.7346899509429932s\n",
      "82/174, train_loss: 17.0472, time taken: 1.9143931865692139s\n",
      "83/174, train_loss: 18.5879, time taken: 1.905395746231079s\n",
      "84/174, train_loss: 19.4197, time taken: 1.7757093906402588s\n",
      "85/174, train_loss: 19.0808, time taken: 1.8058841228485107s\n",
      "86/174, train_loss: 15.9308, time taken: 1.7950637340545654s\n",
      "87/174, train_loss: 14.9074, time taken: 2.124661684036255s\n",
      "88/174, train_loss: 19.7151, time taken: 2.1915016174316406s\n",
      "89/174, train_loss: 16.1364, time taken: 2.0004236698150635s\n",
      "90/174, train_loss: 15.9064, time taken: 2.089055061340332s\n",
      "91/174, train_loss: 21.0850, time taken: 2.3871326446533203s\n",
      "92/174, train_loss: 17.6256, time taken: 2.0102860927581787s\n",
      "93/174, train_loss: 14.9849, time taken: 2.302138566970825s\n",
      "94/174, train_loss: 16.8207, time taken: 2.3492252826690674s\n",
      "95/174, train_loss: 18.7899, time taken: 2.201042890548706s\n",
      "96/174, train_loss: 23.3078, time taken: 2.092783212661743s\n",
      "97/174, train_loss: 17.0189, time taken: 1.83347487449646s\n",
      "98/174, train_loss: 20.2787, time taken: 1.8843276500701904s\n",
      "99/174, train_loss: 18.5232, time taken: 1.89613938331604s\n",
      "100/174, train_loss: 16.1634, time taken: 1.9603338241577148s\n",
      "101/174, train_loss: 14.9520, time taken: 1.7350809574127197s\n",
      "102/174, train_loss: 17.1591, time taken: 1.993563175201416s\n",
      "103/174, train_loss: 16.5459, time taken: 2.078674077987671s\n",
      "104/174, train_loss: 17.6733, time taken: 1.9805586338043213s\n",
      "105/174, train_loss: 14.1599, time taken: 1.8301522731781006s\n",
      "106/174, train_loss: 17.3728, time taken: 1.885450839996338s\n",
      "107/174, train_loss: 23.1251, time taken: 1.800663948059082s\n",
      "108/174, train_loss: 14.6988, time taken: 1.897292137145996s\n",
      "109/174, train_loss: 16.4415, time taken: 1.8049280643463135s\n",
      "110/174, train_loss: 16.5035, time taken: 2.174729585647583s\n",
      "111/174, train_loss: 20.2494, time taken: 2.190046787261963s\n",
      "112/174, train_loss: 16.2531, time taken: 1.9262328147888184s\n",
      "113/174, train_loss: 21.2140, time taken: 1.9065971374511719s\n",
      "114/174, train_loss: 18.7222, time taken: 1.8335702419281006s\n",
      "115/174, train_loss: 18.5874, time taken: 2.0071682929992676s\n",
      "116/174, train_loss: 16.1466, time taken: 2.032986640930176s\n",
      "117/174, train_loss: 20.3084, time taken: 4.0008790493011475s\n",
      "118/174, train_loss: 14.4901, time taken: 2.368368148803711s\n",
      "119/174, train_loss: 22.9616, time taken: 1.8032197952270508s\n",
      "120/174, train_loss: 14.7952, time taken: 1.8087167739868164s\n",
      "121/174, train_loss: 15.2719, time taken: 1.8846917152404785s\n",
      "122/174, train_loss: 16.2884, time taken: 2.019946813583374s\n",
      "123/174, train_loss: 14.8301, time taken: 1.880084753036499s\n",
      "124/174, train_loss: 15.7502, time taken: 1.9088945388793945s\n",
      "125/174, train_loss: 22.4142, time taken: 2.178044319152832s\n",
      "126/174, train_loss: 19.0770, time taken: 2.0217771530151367s\n",
      "127/174, train_loss: 13.2931, time taken: 1.7982800006866455s\n",
      "128/174, train_loss: 24.5158, time taken: 2.103631019592285s\n",
      "129/174, train_loss: 15.5795, time taken: 2.0053634643554688s\n",
      "130/174, train_loss: 15.7516, time taken: 2.0218822956085205s\n",
      "131/174, train_loss: 20.1836, time taken: 1.8916511535644531s\n",
      "132/174, train_loss: 19.5283, time taken: 2.1838362216949463s\n",
      "133/174, train_loss: 19.4099, time taken: 1.9132637977600098s\n",
      "134/174, train_loss: 16.0177, time taken: 1.9609220027923584s\n",
      "135/174, train_loss: 22.1519, time taken: 2.0075933933258057s\n",
      "136/174, train_loss: 16.0798, time taken: 1.9802939891815186s\n",
      "137/174, train_loss: 18.4356, time taken: 1.9001286029815674s\n",
      "138/174, train_loss: 15.8500, time taken: 1.8114619255065918s\n",
      "139/174, train_loss: 17.8663, time taken: 1.8086915016174316s\n",
      "140/174, train_loss: 14.8352, time taken: 1.9053480625152588s\n",
      "141/174, train_loss: 18.9128, time taken: 1.9302523136138916s\n",
      "142/174, train_loss: 13.3404, time taken: 1.8688149452209473s\n",
      "143/174, train_loss: 13.9464, time taken: 1.9198896884918213s\n",
      "144/174, train_loss: 16.5290, time taken: 1.9080736637115479s\n",
      "145/174, train_loss: 15.3564, time taken: 1.9527764320373535s\n",
      "146/174, train_loss: 17.4003, time taken: 1.807471752166748s\n",
      "147/174, train_loss: 26.1998, time taken: 1.8171889781951904s\n",
      "148/174, train_loss: 16.7871, time taken: 1.8977179527282715s\n",
      "149/174, train_loss: 12.8378, time taken: 1.9986114501953125s\n",
      "150/174, train_loss: 19.2351, time taken: 1.885058879852295s\n",
      "151/174, train_loss: 18.9821, time taken: 1.8716495037078857s\n",
      "152/174, train_loss: 12.0361, time taken: 1.8457529544830322s\n",
      "153/174, train_loss: 19.0638, time taken: 2.4036829471588135s\n",
      "154/174, train_loss: 16.4370, time taken: 1.72459077835083s\n",
      "155/174, train_loss: 17.9132, time taken: 1.6837339401245117s\n",
      "156/174, train_loss: 20.5923, time taken: 1.9793875217437744s\n",
      "157/174, train_loss: 20.3003, time taken: 2.000844717025757s\n",
      "158/174, train_loss: 12.8270, time taken: 1.7224819660186768s\n",
      "159/174, train_loss: 18.3020, time taken: 1.7976293563842773s\n",
      "160/174, train_loss: 20.5725, time taken: 2.098418712615967s\n",
      "161/174, train_loss: 19.2531, time taken: 2.0081965923309326s\n",
      "162/174, train_loss: 17.7352, time taken: 2.4406471252441406s\n",
      "163/174, train_loss: 17.9788, time taken: 1.9681813716888428s\n",
      "164/174, train_loss: 19.5952, time taken: 1.9181175231933594s\n",
      "165/174, train_loss: 19.2517, time taken: 1.9919400215148926s\n",
      "166/174, train_loss: 19.0867, time taken: 1.978844404220581s\n",
      "167/174, train_loss: 20.6960, time taken: 2.2891972064971924s\n",
      "168/174, train_loss: 18.8795, time taken: 1.7073955535888672s\n",
      "169/174, train_loss: 18.0657, time taken: 1.7932100296020508s\n",
      "170/174, train_loss: 16.7812, time taken: 1.8225646018981934s\n",
      "171/174, train_loss: 15.4159, time taken: 1.6896929740905762s\n",
      "172/174, train_loss: 15.9918, time taken: 2.0205154418945312s\n",
      "173/174, train_loss: 18.3856, time taken: 1.7046587467193604s\n",
      "174/174, train_loss: 15.3995, time taken: 2.0726516246795654s\n",
      "175/174, train_loss: 12.6921, time taken: 1.5036818981170654s\n",
      "epoch 20 average loss: 17.8092\n",
      "----------\n",
      "epoch 21/2000\n",
      "1/174, train_loss: 13.9530, time taken: 8.627043008804321s\n",
      "2/174, train_loss: 17.3052, time taken: 2.370788812637329s\n",
      "3/174, train_loss: 17.7578, time taken: 1.9850280284881592s\n",
      "4/174, train_loss: 15.4777, time taken: 2.1118052005767822s\n",
      "5/174, train_loss: 14.2301, time taken: 1.8085789680480957s\n",
      "6/174, train_loss: 15.5598, time taken: 1.79713773727417s\n",
      "7/174, train_loss: 16.8489, time taken: 1.8907601833343506s\n",
      "8/174, train_loss: 16.2182, time taken: 1.9870522022247314s\n",
      "9/174, train_loss: 17.2213, time taken: 1.8880009651184082s\n",
      "10/174, train_loss: 12.4701, time taken: 1.7155494689941406s\n",
      "11/174, train_loss: 17.8432, time taken: 1.7864878177642822s\n",
      "12/174, train_loss: 18.2663, time taken: 1.8883287906646729s\n",
      "13/174, train_loss: 15.2931, time taken: 1.9960956573486328s\n",
      "14/174, train_loss: 20.7886, time taken: 2.397188186645508s\n",
      "15/174, train_loss: 23.7166, time taken: 1.826169729232788s\n",
      "16/174, train_loss: 15.4375, time taken: 1.7837016582489014s\n",
      "17/174, train_loss: 15.2129, time taken: 1.8061487674713135s\n",
      "18/174, train_loss: 16.7335, time taken: 2.0561110973358154s\n",
      "19/174, train_loss: 14.6042, time taken: 2.007770538330078s\n",
      "20/174, train_loss: 18.8788, time taken: 1.7010254859924316s\n",
      "21/174, train_loss: 16.4179, time taken: 1.8107733726501465s\n",
      "22/174, train_loss: 17.3245, time taken: 1.8991680145263672s\n",
      "23/174, train_loss: 18.7045, time taken: 1.973240852355957s\n",
      "24/174, train_loss: 15.7723, time taken: 1.9065830707550049s\n",
      "25/174, train_loss: 17.8572, time taken: 1.8012151718139648s\n",
      "26/174, train_loss: 14.2680, time taken: 1.7915241718292236s\n",
      "27/174, train_loss: 16.5513, time taken: 3.808088779449463s\n",
      "28/174, train_loss: 14.4801, time taken: 2.291297435760498s\n",
      "29/174, train_loss: 22.2423, time taken: 2.1966969966888428s\n",
      "30/174, train_loss: 19.7036, time taken: 1.9716336727142334s\n",
      "31/174, train_loss: 16.0234, time taken: 2.025071620941162s\n",
      "32/174, train_loss: 11.7304, time taken: 1.8861746788024902s\n",
      "33/174, train_loss: 13.2495, time taken: 2.0879247188568115s\n",
      "34/174, train_loss: 20.6302, time taken: 2.293581008911133s\n",
      "35/174, train_loss: 18.2178, time taken: 2.1130356788635254s\n",
      "36/174, train_loss: 17.2873, time taken: 1.994922161102295s\n",
      "37/174, train_loss: 19.5385, time taken: 1.8015501499176025s\n",
      "38/174, train_loss: 17.3318, time taken: 1.7850799560546875s\n",
      "39/174, train_loss: 21.7414, time taken: 2.1753995418548584s\n",
      "40/174, train_loss: 19.0548, time taken: 1.7968804836273193s\n",
      "41/174, train_loss: 18.8533, time taken: 1.8138751983642578s\n",
      "42/174, train_loss: 11.9197, time taken: 1.9963221549987793s\n",
      "43/174, train_loss: 20.0476, time taken: 1.8009841442108154s\n",
      "44/174, train_loss: 19.3196, time taken: 1.7938694953918457s\n",
      "45/174, train_loss: 18.4351, time taken: 1.708054780960083s\n",
      "46/174, train_loss: 20.0473, time taken: 1.9592692852020264s\n",
      "47/174, train_loss: 19.2003, time taken: 1.7975695133209229s\n",
      "48/174, train_loss: 16.4018, time taken: 1.726088047027588s\n",
      "49/174, train_loss: 17.9189, time taken: 1.9579496383666992s\n",
      "50/174, train_loss: 17.4875, time taken: 1.8954236507415771s\n",
      "51/174, train_loss: 21.5220, time taken: 1.8144276142120361s\n",
      "52/174, train_loss: 15.1494, time taken: 1.7211174964904785s\n",
      "53/174, train_loss: 13.2839, time taken: 1.959104299545288s\n",
      "54/174, train_loss: 17.1540, time taken: 1.9159431457519531s\n",
      "55/174, train_loss: 16.4753, time taken: 1.7988085746765137s\n",
      "56/174, train_loss: 16.0355, time taken: 1.8006203174591064s\n",
      "57/174, train_loss: 18.6265, time taken: 1.7830750942230225s\n",
      "58/174, train_loss: 16.3834, time taken: 1.8054990768432617s\n",
      "59/174, train_loss: 15.4446, time taken: 1.9105758666992188s\n",
      "60/174, train_loss: 22.5141, time taken: 1.8862721920013428s\n",
      "61/174, train_loss: 15.2688, time taken: 1.784782886505127s\n",
      "62/174, train_loss: 17.7425, time taken: 1.7240898609161377s\n",
      "63/174, train_loss: 18.7191, time taken: 1.9145121574401855s\n",
      "64/174, train_loss: 12.6205, time taken: 1.8061282634735107s\n",
      "65/174, train_loss: 15.3183, time taken: 2.0829312801361084s\n",
      "66/174, train_loss: 14.3474, time taken: 1.8099291324615479s\n",
      "67/174, train_loss: 17.5293, time taken: 1.994591236114502s\n",
      "68/174, train_loss: 15.5150, time taken: 1.9067997932434082s\n",
      "69/174, train_loss: 20.3156, time taken: 1.8945410251617432s\n",
      "70/174, train_loss: 16.5277, time taken: 1.793229579925537s\n",
      "71/174, train_loss: 17.2362, time taken: 1.799238920211792s\n",
      "72/174, train_loss: 11.7328, time taken: 1.983696699142456s\n",
      "73/174, train_loss: 14.3675, time taken: 1.9899744987487793s\n",
      "74/174, train_loss: 16.7089, time taken: 2.1229028701782227s\n",
      "75/174, train_loss: 18.1646, time taken: 1.894484281539917s\n",
      "76/174, train_loss: 17.0004, time taken: 1.9680061340332031s\n",
      "77/174, train_loss: 15.5222, time taken: 1.92356276512146s\n",
      "78/174, train_loss: 20.2501, time taken: 1.7739224433898926s\n",
      "79/174, train_loss: 23.3510, time taken: 1.7878694534301758s\n",
      "80/174, train_loss: 13.9253, time taken: 2.226545572280884s\n",
      "81/174, train_loss: 14.9423, time taken: 1.7650518417358398s\n",
      "82/174, train_loss: 14.1622, time taken: 1.8046417236328125s\n",
      "83/174, train_loss: 18.6875, time taken: 2.0339243412017822s\n",
      "84/174, train_loss: 16.6009, time taken: 1.9326531887054443s\n",
      "85/174, train_loss: 20.1985, time taken: 1.9734902381896973s\n",
      "86/174, train_loss: 16.5283, time taken: 1.9185552597045898s\n",
      "87/174, train_loss: 16.5021, time taken: 1.8020782470703125s\n",
      "88/174, train_loss: 21.4944, time taken: 1.9814023971557617s\n",
      "89/174, train_loss: 14.2258, time taken: 1.874769687652588s\n",
      "90/174, train_loss: 19.7852, time taken: 1.8184709548950195s\n",
      "91/174, train_loss: 15.7142, time taken: 2.0930607318878174s\n",
      "92/174, train_loss: 17.1197, time taken: 2.097217559814453s\n",
      "93/174, train_loss: 11.8464, time taken: 1.8013441562652588s\n",
      "94/174, train_loss: 19.1000, time taken: 2.002498149871826s\n",
      "95/174, train_loss: 19.8170, time taken: 2.097066640853882s\n",
      "96/174, train_loss: 16.6958, time taken: 1.9041688442230225s\n",
      "97/174, train_loss: 20.5576, time taken: 1.7914388179779053s\n",
      "98/174, train_loss: 15.3081, time taken: 1.8819782733917236s\n",
      "99/174, train_loss: 16.3171, time taken: 1.80131196975708s\n",
      "100/174, train_loss: 17.9531, time taken: 1.7003118991851807s\n",
      "101/174, train_loss: 16.5626, time taken: 1.8843357563018799s\n",
      "102/174, train_loss: 20.9060, time taken: 1.921823263168335s\n",
      "103/174, train_loss: 19.9355, time taken: 1.7291133403778076s\n",
      "104/174, train_loss: 14.0168, time taken: 1.889477014541626s\n",
      "105/174, train_loss: 13.3196, time taken: 1.9942564964294434s\n",
      "106/174, train_loss: 10.4888, time taken: 1.9930641651153564s\n",
      "107/174, train_loss: 20.1457, time taken: 1.7114498615264893s\n",
      "108/174, train_loss: 20.5053, time taken: 1.9007213115692139s\n",
      "109/174, train_loss: 16.5994, time taken: 1.9731614589691162s\n",
      "110/174, train_loss: 14.8341, time taken: 1.8109846115112305s\n",
      "111/174, train_loss: 19.0156, time taken: 1.8928344249725342s\n",
      "112/174, train_loss: 15.0179, time taken: 1.922086238861084s\n",
      "113/174, train_loss: 21.0345, time taken: 1.8152763843536377s\n",
      "114/174, train_loss: 14.0716, time taken: 1.901374340057373s\n",
      "115/174, train_loss: 17.8758, time taken: 1.6988658905029297s\n",
      "116/174, train_loss: 16.1952, time taken: 1.8011550903320312s\n",
      "117/174, train_loss: 15.9902, time taken: 2.7760231494903564s\n",
      "118/174, train_loss: 16.8275, time taken: 1.8112449645996094s\n",
      "119/174, train_loss: 17.3044, time taken: 2.007779598236084s\n",
      "120/174, train_loss: 18.6909, time taken: 1.99713134765625s\n",
      "121/174, train_loss: 15.1202, time taken: 1.8980975151062012s\n",
      "122/174, train_loss: 16.5890, time taken: 2.0735023021698s\n",
      "123/174, train_loss: 19.8497, time taken: 1.9890425205230713s\n",
      "124/174, train_loss: 17.4026, time taken: 1.8126277923583984s\n",
      "125/174, train_loss: 22.0419, time taken: 2.0849416255950928s\n",
      "126/174, train_loss: 12.4903, time taken: 1.812206506729126s\n",
      "127/174, train_loss: 20.4521, time taken: 1.881110668182373s\n",
      "128/174, train_loss: 13.9295, time taken: 1.7066283226013184s\n",
      "129/174, train_loss: 20.6991, time taken: 1.7037684917449951s\n",
      "130/174, train_loss: 17.5126, time taken: 1.9695420265197754s\n",
      "131/174, train_loss: 16.1853, time taken: 1.8223400115966797s\n",
      "132/174, train_loss: 14.8839, time taken: 1.9718644618988037s\n",
      "133/174, train_loss: 17.2708, time taken: 1.987417459487915s\n",
      "134/174, train_loss: 16.4594, time taken: 1.8082053661346436s\n",
      "135/174, train_loss: 17.3781, time taken: 1.8189857006072998s\n",
      "136/174, train_loss: 22.1326, time taken: 1.785426378250122s\n",
      "137/174, train_loss: 16.7468, time taken: 1.9813768863677979s\n",
      "138/174, train_loss: 21.1307, time taken: 1.806016206741333s\n",
      "139/174, train_loss: 15.6614, time taken: 1.6896345615386963s\n",
      "140/174, train_loss: 12.3660, time taken: 1.8237512111663818s\n",
      "141/174, train_loss: 20.2601, time taken: 1.8682327270507812s\n",
      "142/174, train_loss: 12.8923, time taken: 1.9908459186553955s\n",
      "143/174, train_loss: 12.0236, time taken: 1.7925338745117188s\n",
      "144/174, train_loss: 20.9568, time taken: 1.8244638442993164s\n",
      "145/174, train_loss: 16.1859, time taken: 1.893712043762207s\n",
      "146/174, train_loss: 15.0932, time taken: 1.8961853981018066s\n",
      "147/174, train_loss: 13.9551, time taken: 1.9942710399627686s\n",
      "148/174, train_loss: 12.8083, time taken: 1.8138787746429443s\n",
      "149/174, train_loss: 17.7326, time taken: 1.7804539203643799s\n",
      "150/174, train_loss: 11.2858, time taken: 1.9982991218566895s\n",
      "151/174, train_loss: 16.4615, time taken: 1.9879231452941895s\n",
      "152/174, train_loss: 17.3802, time taken: 1.8874027729034424s\n",
      "153/174, train_loss: 14.8869, time taken: 1.9299921989440918s\n",
      "154/174, train_loss: 18.7032, time taken: 1.822209358215332s\n",
      "155/174, train_loss: 17.2743, time taken: 1.8754339218139648s\n",
      "156/174, train_loss: 16.1824, time taken: 2.4200704097747803s\n",
      "157/174, train_loss: 14.9820, time taken: 1.8091719150543213s\n",
      "158/174, train_loss: 14.9255, time taken: 1.799088716506958s\n",
      "159/174, train_loss: 18.6849, time taken: 2.3683669567108154s\n",
      "160/174, train_loss: 21.2158, time taken: 1.995767593383789s\n",
      "161/174, train_loss: 15.2866, time taken: 1.920030117034912s\n",
      "162/174, train_loss: 16.4869, time taken: 1.9890975952148438s\n",
      "163/174, train_loss: 18.5254, time taken: 1.7762532234191895s\n",
      "164/174, train_loss: 19.8100, time taken: 1.8199975490570068s\n",
      "165/174, train_loss: 19.8464, time taken: 1.9725990295410156s\n",
      "166/174, train_loss: 15.6774, time taken: 1.7202110290527344s\n",
      "167/174, train_loss: 18.2456, time taken: 2.0899505615234375s\n",
      "168/174, train_loss: 16.8630, time taken: 2.0984389781951904s\n",
      "169/174, train_loss: 18.5432, time taken: 1.98506760597229s\n",
      "170/174, train_loss: 16.9716, time taken: 1.9007987976074219s\n",
      "171/174, train_loss: 17.2757, time taken: 1.6245338916778564s\n",
      "172/174, train_loss: 18.6950, time taken: 2.026702642440796s\n",
      "173/174, train_loss: 16.1535, time taken: 1.8049061298370361s\n",
      "174/174, train_loss: 17.8624, time taken: 1.790858507156372s\n",
      "175/174, train_loss: 18.0356, time taken: 1.389855146408081s\n",
      "epoch 21 average loss: 17.0497\n",
      "Entering Validation for epoch: 21\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 21 Validation avg loss: 10.5876, time taken: 1.0444917678833008s\n",
      "----------\n",
      "epoch 22/2000\n",
      "1/174, train_loss: 18.9162, time taken: 8.764673948287964s\n",
      "2/174, train_loss: 15.2207, time taken: 2.1546895503997803s\n",
      "3/174, train_loss: 13.9199, time taken: 2.0097744464874268s\n",
      "4/174, train_loss: 14.2729, time taken: 1.9038243293762207s\n",
      "5/174, train_loss: 12.9166, time taken: 1.893005132675171s\n",
      "6/174, train_loss: 18.5191, time taken: 1.8092894554138184s\n",
      "7/174, train_loss: 15.0398, time taken: 1.9122495651245117s\n",
      "8/174, train_loss: 15.6007, time taken: 1.8259260654449463s\n",
      "9/174, train_loss: 16.3342, time taken: 1.7797608375549316s\n",
      "10/174, train_loss: 21.3775, time taken: 1.8964316844940186s\n",
      "11/174, train_loss: 15.7353, time taken: 2.0903942584991455s\n",
      "12/174, train_loss: 13.6581, time taken: 1.80519437789917s\n",
      "13/174, train_loss: 14.6990, time taken: 1.900041103363037s\n",
      "14/174, train_loss: 15.7726, time taken: 2.1076598167419434s\n",
      "15/174, train_loss: 19.2645, time taken: 1.9623775482177734s\n",
      "16/174, train_loss: 16.1014, time taken: 1.8311176300048828s\n",
      "17/174, train_loss: 18.1209, time taken: 2.079211473464966s\n",
      "18/174, train_loss: 16.6116, time taken: 2.0994715690612793s\n",
      "19/174, train_loss: 17.3540, time taken: 2.1685521602630615s\n",
      "20/174, train_loss: 21.8116, time taken: 1.830946683883667s\n",
      "21/174, train_loss: 19.4073, time taken: 2.096228837966919s\n",
      "22/174, train_loss: 15.9375, time taken: 2.0744340419769287s\n",
      "23/174, train_loss: 16.0752, time taken: 1.9232661724090576s\n",
      "24/174, train_loss: 17.0041, time taken: 1.9577898979187012s\n",
      "25/174, train_loss: 17.4784, time taken: 1.904045820236206s\n",
      "26/174, train_loss: 17.6068, time taken: 1.7072679996490479s\n",
      "27/174, train_loss: 17.6279, time taken: 2.2773876190185547s\n",
      "28/174, train_loss: 17.7505, time taken: 2.2302205562591553s\n",
      "29/174, train_loss: 17.9070, time taken: 1.903376817703247s\n",
      "30/174, train_loss: 15.5683, time taken: 2.1778857707977295s\n",
      "31/174, train_loss: 16.3632, time taken: 2.1000137329101562s\n",
      "32/174, train_loss: 13.9833, time taken: 1.9803028106689453s\n",
      "33/174, train_loss: 14.2384, time taken: 1.8366034030914307s\n",
      "34/174, train_loss: 16.5918, time taken: 1.821808099746704s\n",
      "35/174, train_loss: 15.9325, time taken: 1.8914744853973389s\n",
      "36/174, train_loss: 14.3809, time taken: 1.813697099685669s\n",
      "37/174, train_loss: 14.8445, time taken: 1.8909196853637695s\n",
      "38/174, train_loss: 21.4207, time taken: 1.9773468971252441s\n",
      "39/174, train_loss: 17.1385, time taken: 1.8150362968444824s\n",
      "40/174, train_loss: 13.6405, time taken: 2.006882429122925s\n",
      "41/174, train_loss: 15.2866, time taken: 1.7916922569274902s\n",
      "42/174, train_loss: 24.4133, time taken: 1.7664453983306885s\n",
      "43/174, train_loss: 14.8640, time taken: 1.7904589176177979s\n",
      "44/174, train_loss: 15.1540, time taken: 1.9171276092529297s\n",
      "45/174, train_loss: 16.4706, time taken: 1.9999029636383057s\n",
      "46/174, train_loss: 17.8337, time taken: 1.7871410846710205s\n",
      "47/174, train_loss: 19.9490, time taken: 1.7158043384552002s\n",
      "48/174, train_loss: 21.4910, time taken: 1.887242078781128s\n",
      "49/174, train_loss: 17.1106, time taken: 1.7867639064788818s\n",
      "50/174, train_loss: 19.8768, time taken: 1.7964191436767578s\n",
      "51/174, train_loss: 14.2105, time taken: 2.1972224712371826s\n",
      "52/174, train_loss: 18.5000, time taken: 1.9048116207122803s\n",
      "53/174, train_loss: 16.3202, time taken: 1.7134833335876465s\n",
      "54/174, train_loss: 20.0207, time taken: 1.9242877960205078s\n",
      "55/174, train_loss: 22.3676, time taken: 1.900432825088501s\n",
      "56/174, train_loss: 20.7774, time taken: 1.794980525970459s\n",
      "57/174, train_loss: 19.0060, time taken: 1.91432523727417s\n",
      "58/174, train_loss: 17.9161, time taken: 1.7227325439453125s\n",
      "59/174, train_loss: 16.0073, time taken: 1.8950810432434082s\n",
      "60/174, train_loss: 15.6535, time taken: 1.9880554676055908s\n",
      "61/174, train_loss: 20.8295, time taken: 3.6800098419189453s\n",
      "62/174, train_loss: 18.1545, time taken: 2.30551815032959s\n",
      "63/174, train_loss: 13.6922, time taken: 2.11946177482605s\n",
      "64/174, train_loss: 10.8120, time taken: 1.795790672302246s\n",
      "65/174, train_loss: 21.2739, time taken: 1.9938511848449707s\n",
      "66/174, train_loss: 18.7903, time taken: 1.8777556419372559s\n",
      "67/174, train_loss: 13.9723, time taken: 2.3835291862487793s\n",
      "68/174, train_loss: 19.8509, time taken: 2.136046886444092s\n",
      "69/174, train_loss: 18.3853, time taken: 1.885770559310913s\n",
      "70/174, train_loss: 18.6908, time taken: 1.978867530822754s\n",
      "71/174, train_loss: 15.9277, time taken: 1.8254640102386475s\n",
      "72/174, train_loss: 17.8084, time taken: 1.8102691173553467s\n",
      "73/174, train_loss: 17.3343, time taken: 1.8251440525054932s\n",
      "74/174, train_loss: 17.6500, time taken: 1.8577122688293457s\n",
      "75/174, train_loss: 15.3045, time taken: 1.900373935699463s\n",
      "76/174, train_loss: 20.8406, time taken: 1.996089220046997s\n",
      "77/174, train_loss: 14.2913, time taken: 1.7286138534545898s\n",
      "78/174, train_loss: 19.8355, time taken: 1.8669066429138184s\n",
      "79/174, train_loss: 15.8332, time taken: 2.1183183193206787s\n",
      "80/174, train_loss: 20.5207, time taken: 2.176567316055298s\n",
      "81/174, train_loss: 21.3061, time taken: 2.008084535598755s\n",
      "82/174, train_loss: 18.1551, time taken: 1.7821691036224365s\n",
      "83/174, train_loss: 15.0563, time taken: 3.2024452686309814s\n",
      "84/174, train_loss: 13.0995, time taken: 2.5784008502960205s\n",
      "85/174, train_loss: 14.8288, time taken: 2.4164364337921143s\n",
      "86/174, train_loss: 22.8207, time taken: 2.1137545108795166s\n",
      "87/174, train_loss: 16.3559, time taken: 1.7317416667938232s\n",
      "88/174, train_loss: 17.7613, time taken: 2.2737009525299072s\n",
      "89/174, train_loss: 13.6762, time taken: 2.0922935009002686s\n",
      "90/174, train_loss: 14.5953, time taken: 1.916217565536499s\n",
      "91/174, train_loss: 13.7287, time taken: 1.8848061561584473s\n",
      "92/174, train_loss: 15.5552, time taken: 2.102595090866089s\n",
      "93/174, train_loss: 16.9410, time taken: 2.176717758178711s\n",
      "94/174, train_loss: 10.4559, time taken: 1.8363134860992432s\n",
      "95/174, train_loss: 20.4882, time taken: 1.9623713493347168s\n",
      "96/174, train_loss: 15.7137, time taken: 1.8073151111602783s\n",
      "97/174, train_loss: 11.8446, time taken: 1.7874610424041748s\n",
      "98/174, train_loss: 20.9903, time taken: 2.1145265102386475s\n",
      "99/174, train_loss: 16.0359, time taken: 1.7887091636657715s\n",
      "100/174, train_loss: 15.3651, time taken: 1.7940678596496582s\n",
      "101/174, train_loss: 19.5760, time taken: 1.8976235389709473s\n",
      "102/174, train_loss: 16.3341, time taken: 1.8774309158325195s\n",
      "103/174, train_loss: 16.2428, time taken: 2.010319948196411s\n",
      "104/174, train_loss: 19.0061, time taken: 1.9810335636138916s\n",
      "105/174, train_loss: 19.1389, time taken: 1.811037302017212s\n",
      "106/174, train_loss: 23.6211, time taken: 1.7878351211547852s\n",
      "107/174, train_loss: 18.0098, time taken: 1.9171142578125s\n",
      "108/174, train_loss: 16.9487, time taken: 1.8959693908691406s\n",
      "109/174, train_loss: 17.5875, time taken: 1.7915914058685303s\n",
      "110/174, train_loss: 15.7286, time taken: 1.8998816013336182s\n",
      "111/174, train_loss: 19.0880, time taken: 1.867124319076538s\n",
      "112/174, train_loss: 17.7829, time taken: 1.8152461051940918s\n",
      "113/174, train_loss: 16.2145, time taken: 1.6884956359863281s\n",
      "114/174, train_loss: 18.0959, time taken: 1.9094507694244385s\n",
      "115/174, train_loss: 17.8713, time taken: 3.4765920639038086s\n",
      "116/174, train_loss: 20.9949, time taken: 1.8218953609466553s\n",
      "117/174, train_loss: 18.0528, time taken: 1.770451545715332s\n",
      "118/174, train_loss: 15.2532, time taken: 1.8218703269958496s\n",
      "119/174, train_loss: 20.1715, time taken: 1.799736499786377s\n",
      "120/174, train_loss: 13.8217, time taken: 2.048283576965332s\n",
      "121/174, train_loss: 19.3564, time taken: 2.1068437099456787s\n",
      "122/174, train_loss: 14.6898, time taken: 1.9015793800354004s\n",
      "123/174, train_loss: 11.0490, time taken: 1.805696725845337s\n",
      "124/174, train_loss: 20.7565, time taken: 1.7756924629211426s\n",
      "125/174, train_loss: 31.3720, time taken: 1.8008582592010498s\n",
      "126/174, train_loss: 15.3405, time taken: 2.017486810684204s\n",
      "127/174, train_loss: 21.7437, time taken: 1.8924658298492432s\n",
      "128/174, train_loss: 16.8901, time taken: 2.0775506496429443s\n",
      "129/174, train_loss: 21.3336, time taken: 1.7215025424957275s\n",
      "130/174, train_loss: 19.4339, time taken: 1.873793601989746s\n",
      "131/174, train_loss: 15.2613, time taken: 1.729630708694458s\n",
      "132/174, train_loss: 11.5964, time taken: 1.7858233451843262s\n",
      "133/174, train_loss: 19.9650, time taken: 1.7849912643432617s\n",
      "134/174, train_loss: 16.8923, time taken: 2.2798330783843994s\n",
      "135/174, train_loss: 17.6556, time taken: 2.020580291748047s\n",
      "136/174, train_loss: 20.4055, time taken: 1.793013334274292s\n",
      "137/174, train_loss: 15.2158, time taken: 1.8154633045196533s\n",
      "138/174, train_loss: 14.1408, time taken: 2.0568008422851562s\n",
      "139/174, train_loss: 15.8549, time taken: 1.7303919792175293s\n",
      "140/174, train_loss: 14.5425, time taken: 1.7681076526641846s\n",
      "141/174, train_loss: 24.0494, time taken: 1.795328140258789s\n",
      "142/174, train_loss: 19.1044, time taken: 1.9362666606903076s\n",
      "143/174, train_loss: 19.1842, time taken: 1.991316795349121s\n",
      "144/174, train_loss: 15.8915, time taken: 1.8760499954223633s\n",
      "145/174, train_loss: 11.7300, time taken: 1.7247142791748047s\n",
      "146/174, train_loss: 17.2244, time taken: 1.7732243537902832s\n",
      "147/174, train_loss: 17.2831, time taken: 1.8896396160125732s\n",
      "148/174, train_loss: 19.8416, time taken: 1.8124890327453613s\n",
      "149/174, train_loss: 16.0671, time taken: 1.9741320610046387s\n",
      "150/174, train_loss: 12.3169, time taken: 1.80072021484375s\n",
      "151/174, train_loss: 17.4994, time taken: 2.1091272830963135s\n",
      "152/174, train_loss: 14.8995, time taken: 1.9913084506988525s\n",
      "153/174, train_loss: 20.2864, time taken: 1.7170395851135254s\n",
      "154/174, train_loss: 16.2167, time taken: 1.8035106658935547s\n",
      "155/174, train_loss: 16.6547, time taken: 1.709805965423584s\n",
      "156/174, train_loss: 18.0274, time taken: 1.9847893714904785s\n",
      "157/174, train_loss: 21.2051, time taken: 1.797236680984497s\n",
      "158/174, train_loss: 17.1682, time taken: 2.1900603771209717s\n",
      "159/174, train_loss: 16.5692, time taken: 1.8169362545013428s\n",
      "160/174, train_loss: 15.1395, time taken: 1.9098048210144043s\n",
      "161/174, train_loss: 14.8619, time taken: 2.071580410003662s\n",
      "162/174, train_loss: 15.2026, time taken: 1.9286105632781982s\n",
      "163/174, train_loss: 15.6542, time taken: 1.7667510509490967s\n",
      "164/174, train_loss: 16.5742, time taken: 1.8968071937561035s\n",
      "165/174, train_loss: 16.1360, time taken: 1.9201161861419678s\n",
      "166/174, train_loss: 15.4310, time taken: 1.8997552394866943s\n",
      "167/174, train_loss: 15.5316, time taken: 1.8884363174438477s\n",
      "168/174, train_loss: 13.1445, time taken: 1.7663614749908447s\n",
      "169/174, train_loss: 22.7231, time taken: 2.204939842224121s\n",
      "170/174, train_loss: 24.4621, time taken: 1.92354416847229s\n",
      "171/174, train_loss: 18.3654, time taken: 1.876023292541504s\n",
      "172/174, train_loss: 20.7898, time taken: 1.7029767036437988s\n",
      "173/174, train_loss: 15.4016, time taken: 1.7045719623565674s\n",
      "174/174, train_loss: 13.1610, time taken: 1.771986722946167s\n",
      "175/174, train_loss: 13.7805, time taken: 1.7931852340698242s\n",
      "epoch 22 average loss: 17.2032\n",
      "----------\n",
      "epoch 23/2000\n",
      "1/174, train_loss: 13.8899, time taken: 8.65370488166809s\n",
      "2/174, train_loss: 20.0561, time taken: 2.2039318084716797s\n",
      "3/174, train_loss: 19.5920, time taken: 1.9688668251037598s\n",
      "4/174, train_loss: 16.3341, time taken: 2.185939311981201s\n",
      "5/174, train_loss: 19.5916, time taken: 1.8241844177246094s\n",
      "6/174, train_loss: 15.9596, time taken: 1.7879695892333984s\n",
      "7/174, train_loss: 14.9623, time taken: 1.7052881717681885s\n",
      "8/174, train_loss: 19.5324, time taken: 1.8603813648223877s\n",
      "9/174, train_loss: 16.7558, time taken: 1.8244116306304932s\n",
      "10/174, train_loss: 19.2492, time taken: 1.798285961151123s\n",
      "11/174, train_loss: 15.0448, time taken: 1.7995879650115967s\n",
      "12/174, train_loss: 13.6026, time taken: 2.2848732471466064s\n",
      "13/174, train_loss: 17.7542, time taken: 1.773073434829712s\n",
      "14/174, train_loss: 11.9841, time taken: 1.8333804607391357s\n",
      "15/174, train_loss: 13.8325, time taken: 2.1629140377044678s\n",
      "16/174, train_loss: 11.7733, time taken: 1.7274532318115234s\n",
      "17/174, train_loss: 18.1152, time taken: 1.872828483581543s\n",
      "18/174, train_loss: 20.2588, time taken: 1.8841843605041504s\n",
      "19/174, train_loss: 15.9700, time taken: 1.9384195804595947s\n",
      "20/174, train_loss: 18.6716, time taken: 1.9641952514648438s\n",
      "21/174, train_loss: 16.8063, time taken: 2.0959155559539795s\n",
      "22/174, train_loss: 16.3186, time taken: 1.8165810108184814s\n",
      "23/174, train_loss: 20.0812, time taken: 1.769646167755127s\n",
      "24/174, train_loss: 21.6522, time taken: 1.8220994472503662s\n",
      "25/174, train_loss: 17.6352, time taken: 1.8911571502685547s\n",
      "26/174, train_loss: 21.2338, time taken: 1.8158650398254395s\n",
      "27/174, train_loss: 16.9478, time taken: 1.914445161819458s\n",
      "28/174, train_loss: 21.5707, time taken: 1.8161485195159912s\n",
      "29/174, train_loss: 15.8920, time taken: 2.002396821975708s\n",
      "30/174, train_loss: 22.1521, time taken: 2.010592222213745s\n",
      "31/174, train_loss: 23.1831, time taken: 1.8296475410461426s\n",
      "32/174, train_loss: 16.7703, time taken: 2.8695566654205322s\n",
      "33/174, train_loss: 18.7106, time taken: 1.8996610641479492s\n",
      "34/174, train_loss: 20.3061, time taken: 1.9992799758911133s\n",
      "35/174, train_loss: 12.8317, time taken: 2.2009689807891846s\n",
      "36/174, train_loss: 17.4114, time taken: 2.2864301204681396s\n",
      "37/174, train_loss: 11.0252, time taken: 2.286616563796997s\n",
      "38/174, train_loss: 18.7415, time taken: 1.8191876411437988s\n",
      "39/174, train_loss: 18.5272, time taken: 1.9753446578979492s\n",
      "40/174, train_loss: 14.2046, time taken: 1.7940928936004639s\n",
      "41/174, train_loss: 19.8782, time taken: 1.9150669574737549s\n",
      "42/174, train_loss: 16.5409, time taken: 1.9824597835540771s\n",
      "43/174, train_loss: 22.9976, time taken: 1.912306785583496s\n",
      "44/174, train_loss: 14.6882, time taken: 2.0893454551696777s\n",
      "45/174, train_loss: 13.2606, time taken: 1.6977522373199463s\n",
      "46/174, train_loss: 17.8170, time taken: 1.8976476192474365s\n",
      "47/174, train_loss: 16.8675, time taken: 1.8027184009552002s\n",
      "48/174, train_loss: 14.7656, time taken: 1.983140230178833s\n",
      "49/174, train_loss: 18.5087, time taken: 1.8167235851287842s\n",
      "50/174, train_loss: 21.4441, time taken: 2.086589813232422s\n",
      "51/174, train_loss: 21.2421, time taken: 1.86954665184021s\n",
      "52/174, train_loss: 15.7998, time taken: 1.7995948791503906s\n",
      "53/174, train_loss: 15.0042, time taken: 1.983663558959961s\n",
      "54/174, train_loss: 18.7630, time taken: 2.0155298709869385s\n",
      "55/174, train_loss: 15.1966, time taken: 1.9857723712921143s\n",
      "56/174, train_loss: 20.3386, time taken: 1.812272071838379s\n",
      "57/174, train_loss: 15.3748, time taken: 1.707472324371338s\n",
      "58/174, train_loss: 17.4182, time taken: 1.8724040985107422s\n",
      "59/174, train_loss: 18.5689, time taken: 2.0935068130493164s\n",
      "60/174, train_loss: 17.0050, time taken: 1.9109513759613037s\n",
      "61/174, train_loss: 16.6137, time taken: 2.0059621334075928s\n",
      "62/174, train_loss: 16.2299, time taken: 1.877162218093872s\n",
      "63/174, train_loss: 15.4144, time taken: 1.9027085304260254s\n",
      "64/174, train_loss: 18.2964, time taken: 1.9211335182189941s\n",
      "65/174, train_loss: 23.0059, time taken: 1.7280075550079346s\n",
      "66/174, train_loss: 17.5810, time taken: 1.963697910308838s\n",
      "67/174, train_loss: 17.9559, time taken: 2.1053035259246826s\n",
      "68/174, train_loss: 11.1217, time taken: 2.016920328140259s\n",
      "69/174, train_loss: 13.5065, time taken: 1.780487298965454s\n",
      "70/174, train_loss: 16.9868, time taken: 1.707648754119873s\n",
      "71/174, train_loss: 18.1673, time taken: 1.71195650100708s\n",
      "72/174, train_loss: 15.4216, time taken: 1.7414042949676514s\n",
      "73/174, train_loss: 18.5625, time taken: 2.0796401500701904s\n",
      "74/174, train_loss: 17.3826, time taken: 1.870250940322876s\n",
      "75/174, train_loss: 17.8188, time taken: 1.8303618431091309s\n",
      "76/174, train_loss: 13.9475, time taken: 1.8794169425964355s\n",
      "77/174, train_loss: 16.7051, time taken: 2.205711841583252s\n",
      "78/174, train_loss: 13.0292, time taken: 3.4048359394073486s\n",
      "79/174, train_loss: 18.9527, time taken: 2.1597256660461426s\n",
      "80/174, train_loss: 14.4104, time taken: 2.042019844055176s\n",
      "81/174, train_loss: 19.1851, time taken: 2.3652970790863037s\n",
      "82/174, train_loss: 15.6044, time taken: 1.9807093143463135s\n",
      "83/174, train_loss: 16.0883, time taken: 2.0341551303863525s\n",
      "84/174, train_loss: 20.5856, time taken: 1.9860618114471436s\n",
      "85/174, train_loss: 15.4571, time taken: 1.904815912246704s\n",
      "86/174, train_loss: 17.2960, time taken: 2.367178440093994s\n",
      "87/174, train_loss: 15.5832, time taken: 2.005223512649536s\n",
      "88/174, train_loss: 19.7867, time taken: 2.1805102825164795s\n",
      "89/174, train_loss: 12.9193, time taken: 2.0339348316192627s\n",
      "90/174, train_loss: 16.8600, time taken: 1.7758557796478271s\n",
      "91/174, train_loss: 12.0371, time taken: 1.891815185546875s\n",
      "92/174, train_loss: 20.2502, time taken: 1.793139934539795s\n",
      "93/174, train_loss: 15.4633, time taken: 1.80214262008667s\n",
      "94/174, train_loss: 17.3594, time taken: 1.6290552616119385s\n",
      "95/174, train_loss: 16.9569, time taken: 1.8344383239746094s\n",
      "96/174, train_loss: 18.6305, time taken: 1.8013691902160645s\n",
      "97/174, train_loss: 12.5913, time taken: 1.861875295639038s\n",
      "98/174, train_loss: 16.9942, time taken: 1.8125028610229492s\n",
      "99/174, train_loss: 15.6730, time taken: 1.983058214187622s\n",
      "100/174, train_loss: 14.3683, time taken: 1.9193899631500244s\n",
      "101/174, train_loss: 18.2749, time taken: 1.7996783256530762s\n",
      "102/174, train_loss: 19.0183, time taken: 1.7556483745574951s\n",
      "103/174, train_loss: 18.2375, time taken: 1.7034025192260742s\n",
      "104/174, train_loss: 15.7532, time taken: 1.787731409072876s\n",
      "105/174, train_loss: 19.6335, time taken: 1.721952199935913s\n",
      "106/174, train_loss: 20.5878, time taken: 1.8786516189575195s\n",
      "107/174, train_loss: 15.9739, time taken: 1.9044134616851807s\n",
      "108/174, train_loss: 20.2001, time taken: 1.815643310546875s\n",
      "109/174, train_loss: 18.3378, time taken: 1.7840521335601807s\n",
      "110/174, train_loss: 21.9357, time taken: 3.48101544380188s\n",
      "111/174, train_loss: 17.3858, time taken: 1.7266249656677246s\n",
      "112/174, train_loss: 20.2552, time taken: 2.081590414047241s\n",
      "113/174, train_loss: 18.5777, time taken: 1.7882118225097656s\n",
      "114/174, train_loss: 17.0194, time taken: 1.7944855690002441s\n",
      "115/174, train_loss: 19.8688, time taken: 1.7071444988250732s\n",
      "116/174, train_loss: 14.1736, time taken: 1.8895807266235352s\n",
      "117/174, train_loss: 17.2692, time taken: 1.9043223857879639s\n",
      "118/174, train_loss: 15.9497, time taken: 1.793010950088501s\n",
      "119/174, train_loss: 20.4850, time taken: 1.6932570934295654s\n",
      "120/174, train_loss: 13.5177, time taken: 1.8131496906280518s\n",
      "121/174, train_loss: 19.3531, time taken: 1.6944563388824463s\n",
      "122/174, train_loss: 15.6109, time taken: 1.8003244400024414s\n",
      "123/174, train_loss: 15.8526, time taken: 1.7162516117095947s\n",
      "124/174, train_loss: 11.6940, time taken: 1.8897244930267334s\n",
      "125/174, train_loss: 13.2642, time taken: 1.7997589111328125s\n",
      "126/174, train_loss: 16.1867, time taken: 1.792576551437378s\n",
      "127/174, train_loss: 15.4115, time taken: 2.0029444694519043s\n",
      "128/174, train_loss: 18.7246, time taken: 1.7942755222320557s\n",
      "129/174, train_loss: 17.0117, time taken: 1.981889009475708s\n",
      "130/174, train_loss: 12.3598, time taken: 2.217015504837036s\n",
      "131/174, train_loss: 15.3432, time taken: 2.1096198558807373s\n",
      "132/174, train_loss: 17.9944, time taken: 1.9004552364349365s\n",
      "133/174, train_loss: 20.5081, time taken: 1.8164570331573486s\n",
      "134/174, train_loss: 18.2651, time taken: 1.7968535423278809s\n",
      "135/174, train_loss: 13.5646, time taken: 1.6836719512939453s\n",
      "136/174, train_loss: 19.4325, time taken: 1.6999459266662598s\n",
      "137/174, train_loss: 13.3120, time taken: 1.8243656158447266s\n",
      "138/174, train_loss: 17.1997, time taken: 1.892638921737671s\n",
      "139/174, train_loss: 17.9657, time taken: 1.7940351963043213s\n",
      "140/174, train_loss: 19.5370, time taken: 1.9081001281738281s\n",
      "141/174, train_loss: 13.0398, time taken: 1.8034698963165283s\n",
      "142/174, train_loss: 17.6549, time taken: 1.9047200679779053s\n",
      "143/174, train_loss: 15.2774, time taken: 1.901120662689209s\n",
      "144/174, train_loss: 19.7686, time taken: 1.712172269821167s\n",
      "145/174, train_loss: 15.7527, time taken: 1.883504867553711s\n",
      "146/174, train_loss: 16.2237, time taken: 1.8039376735687256s\n",
      "147/174, train_loss: 21.3176, time taken: 1.7663764953613281s\n",
      "148/174, train_loss: 16.5154, time taken: 1.7054414749145508s\n",
      "149/174, train_loss: 15.8793, time taken: 1.7204759120941162s\n",
      "150/174, train_loss: 13.8565, time taken: 1.669055700302124s\n",
      "151/174, train_loss: 20.0444, time taken: 1.8117196559906006s\n",
      "152/174, train_loss: 15.0796, time taken: 1.9799907207489014s\n",
      "153/174, train_loss: 17.2507, time taken: 1.9833781719207764s\n",
      "154/174, train_loss: 16.6141, time taken: 1.7186720371246338s\n",
      "155/174, train_loss: 17.8569, time taken: 3.402982234954834s\n",
      "156/174, train_loss: 17.3853, time taken: 1.8782727718353271s\n",
      "157/174, train_loss: 14.9549, time taken: 1.7920184135437012s\n",
      "158/174, train_loss: 14.8454, time taken: 1.790494680404663s\n",
      "159/174, train_loss: 15.4748, time taken: 1.8211524486541748s\n",
      "160/174, train_loss: 19.2930, time taken: 1.8851423263549805s\n",
      "161/174, train_loss: 16.4528, time taken: 1.993790626525879s\n",
      "162/174, train_loss: 16.6646, time taken: 1.802433729171753s\n",
      "163/174, train_loss: 15.7508, time taken: 1.7229135036468506s\n",
      "164/174, train_loss: 17.9067, time taken: 1.974820852279663s\n",
      "165/174, train_loss: 14.4531, time taken: 1.9149303436279297s\n",
      "166/174, train_loss: 16.0243, time taken: 1.9323453903198242s\n",
      "167/174, train_loss: 17.1501, time taken: 1.8791234493255615s\n",
      "168/174, train_loss: 19.2076, time taken: 1.8137753009796143s\n",
      "169/174, train_loss: 20.8879, time taken: 2.1675326824188232s\n",
      "170/174, train_loss: 15.2925, time taken: 1.8149731159210205s\n",
      "171/174, train_loss: 19.3186, time taken: 1.9987761974334717s\n",
      "172/174, train_loss: 12.7130, time taken: 1.8092567920684814s\n",
      "173/174, train_loss: 17.4157, time taken: 1.8963077068328857s\n",
      "174/174, train_loss: 15.7791, time taken: 1.8872814178466797s\n",
      "175/174, train_loss: 19.1656, time taken: 1.579721450805664s\n",
      "epoch 23 average loss: 17.0659\n",
      "Entering Validation for epoch: 23\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 23 Validation avg loss: 12.5937, time taken: 1.171367883682251s\n",
      "----------\n",
      "epoch 24/2000\n",
      "1/174, train_loss: 17.9561, time taken: 7.865879774093628s\n",
      "2/174, train_loss: 17.1824, time taken: 2.092592716217041s\n",
      "3/174, train_loss: 20.1140, time taken: 1.9802274703979492s\n",
      "4/174, train_loss: 22.7475, time taken: 1.986445426940918s\n",
      "5/174, train_loss: 16.1890, time taken: 1.8885557651519775s\n",
      "6/174, train_loss: 17.5718, time taken: 1.896052598953247s\n",
      "7/174, train_loss: 15.8509, time taken: 1.7264046669006348s\n",
      "8/174, train_loss: 17.4258, time taken: 2.1853201389312744s\n",
      "9/174, train_loss: 16.3879, time taken: 2.0807197093963623s\n",
      "10/174, train_loss: 18.8848, time taken: 1.9081799983978271s\n",
      "11/174, train_loss: 19.4723, time taken: 1.7909331321716309s\n",
      "12/174, train_loss: 14.9394, time taken: 1.7140185832977295s\n",
      "13/174, train_loss: 18.9953, time taken: 1.6881601810455322s\n",
      "14/174, train_loss: 18.6591, time taken: 1.6936028003692627s\n",
      "15/174, train_loss: 20.2110, time taken: 1.6777245998382568s\n",
      "16/174, train_loss: 19.2905, time taken: 1.7338809967041016s\n",
      "17/174, train_loss: 18.7411, time taken: 1.7329504489898682s\n",
      "18/174, train_loss: 17.8937, time taken: 1.6976265907287598s\n",
      "19/174, train_loss: 16.7564, time taken: 1.774043321609497s\n",
      "20/174, train_loss: 21.8512, time taken: 2.102006435394287s\n",
      "21/174, train_loss: 14.9852, time taken: 1.7142457962036133s\n",
      "22/174, train_loss: 12.1556, time taken: 1.791243314743042s\n",
      "23/174, train_loss: 19.0477, time taken: 1.8083558082580566s\n",
      "24/174, train_loss: 15.1664, time taken: 3.270838737487793s\n",
      "25/174, train_loss: 13.8198, time taken: 1.7293353080749512s\n",
      "26/174, train_loss: 18.1327, time taken: 1.72043776512146s\n",
      "27/174, train_loss: 15.7346, time taken: 2.010563373565674s\n",
      "28/174, train_loss: 15.8084, time taken: 2.365966558456421s\n",
      "29/174, train_loss: 15.5635, time taken: 1.719428300857544s\n",
      "30/174, train_loss: 15.7703, time taken: 2.0052075386047363s\n",
      "31/174, train_loss: 14.1711, time taken: 1.8689672946929932s\n",
      "32/174, train_loss: 17.0490, time taken: 1.7920794486999512s\n",
      "33/174, train_loss: 15.8256, time taken: 1.804851770401001s\n",
      "34/174, train_loss: 16.8017, time taken: 1.6972689628601074s\n",
      "35/174, train_loss: 21.7462, time taken: 1.804908275604248s\n",
      "36/174, train_loss: 12.8796, time taken: 1.8026819229125977s\n",
      "37/174, train_loss: 15.5186, time taken: 1.865422248840332s\n",
      "38/174, train_loss: 22.0038, time taken: 1.9176712036132812s\n",
      "39/174, train_loss: 12.3506, time taken: 1.7243452072143555s\n",
      "40/174, train_loss: 17.8139, time taken: 1.834900140762329s\n",
      "41/174, train_loss: 18.4045, time taken: 1.9775662422180176s\n",
      "42/174, train_loss: 16.2678, time taken: 2.3920376300811768s\n",
      "43/174, train_loss: 13.6867, time taken: 1.8900606632232666s\n",
      "44/174, train_loss: 18.2663, time taken: 1.8095366954803467s\n",
      "45/174, train_loss: 16.5903, time taken: 1.783423900604248s\n",
      "46/174, train_loss: 19.2653, time taken: 1.9076929092407227s\n",
      "47/174, train_loss: 12.7516, time taken: 1.8214797973632812s\n",
      "48/174, train_loss: 16.9053, time taken: 1.9939885139465332s\n",
      "49/174, train_loss: 15.1206, time taken: 1.9955251216888428s\n",
      "50/174, train_loss: 14.9760, time taken: 2.162996292114258s\n",
      "51/174, train_loss: 17.4011, time taken: 1.8214209079742432s\n",
      "52/174, train_loss: 18.2735, time taken: 1.7819693088531494s\n",
      "53/174, train_loss: 22.8967, time taken: 1.706655740737915s\n",
      "54/174, train_loss: 19.8595, time taken: 1.7679638862609863s\n",
      "55/174, train_loss: 16.4707, time taken: 1.728132963180542s\n",
      "56/174, train_loss: 21.5396, time taken: 1.8945012092590332s\n",
      "57/174, train_loss: 17.0672, time taken: 1.8695387840270996s\n",
      "58/174, train_loss: 26.3560, time taken: 1.9053471088409424s\n",
      "59/174, train_loss: 13.8400, time taken: 1.8109209537506104s\n",
      "60/174, train_loss: 14.6791, time taken: 1.9896326065063477s\n",
      "61/174, train_loss: 19.2170, time taken: 1.9858152866363525s\n",
      "62/174, train_loss: 18.2528, time taken: 1.7160420417785645s\n",
      "63/174, train_loss: 18.6137, time taken: 1.6874854564666748s\n",
      "64/174, train_loss: 16.5992, time taken: 1.6963918209075928s\n",
      "65/174, train_loss: 17.4948, time taken: 1.709824562072754s\n",
      "66/174, train_loss: 20.5594, time taken: 1.8830420970916748s\n",
      "67/174, train_loss: 16.7994, time taken: 1.7995827198028564s\n",
      "68/174, train_loss: 14.5821, time taken: 1.801975965499878s\n",
      "69/174, train_loss: 15.3439, time taken: 1.8858551979064941s\n",
      "70/174, train_loss: 15.6989, time taken: 1.8056652545928955s\n",
      "71/174, train_loss: 15.6546, time taken: 1.7786281108856201s\n",
      "72/174, train_loss: 19.4372, time taken: 2.0023186206817627s\n",
      "73/174, train_loss: 16.0316, time taken: 1.9147446155548096s\n",
      "74/174, train_loss: 18.6013, time taken: 1.99141263961792s\n",
      "75/174, train_loss: 17.3437, time taken: 1.9126975536346436s\n",
      "76/174, train_loss: 22.0229, time taken: 1.7839250564575195s\n",
      "77/174, train_loss: 14.7975, time taken: 3.0070879459381104s\n",
      "78/174, train_loss: 15.8692, time taken: 1.9943654537200928s\n",
      "79/174, train_loss: 19.0692, time taken: 2.103548765182495s\n",
      "80/174, train_loss: 18.1715, time taken: 1.794560432434082s\n",
      "81/174, train_loss: 15.4426, time taken: 2.164108991622925s\n",
      "82/174, train_loss: 19.3056, time taken: 1.9145801067352295s\n",
      "83/174, train_loss: 13.2789, time taken: 2.093154191970825s\n",
      "84/174, train_loss: 17.4283, time taken: 1.801919937133789s\n",
      "85/174, train_loss: 18.9338, time taken: 1.9096617698669434s\n",
      "86/174, train_loss: 18.7130, time taken: 1.7658262252807617s\n",
      "87/174, train_loss: 16.2169, time taken: 2.431199312210083s\n",
      "88/174, train_loss: 18.2765, time taken: 2.102529525756836s\n",
      "89/174, train_loss: 13.2511, time taken: 1.9071707725524902s\n",
      "90/174, train_loss: 21.4023, time taken: 1.7337374687194824s\n",
      "91/174, train_loss: 15.9212, time taken: 1.7743310928344727s\n",
      "92/174, train_loss: 21.2484, time taken: 2.0159037113189697s\n",
      "93/174, train_loss: 17.0443, time taken: 1.9256994724273682s\n",
      "94/174, train_loss: 14.9768, time taken: 1.8889501094818115s\n",
      "95/174, train_loss: 17.9930, time taken: 1.7052857875823975s\n",
      "96/174, train_loss: 18.0966, time taken: 1.8109753131866455s\n",
      "97/174, train_loss: 15.1371, time taken: 1.9275033473968506s\n",
      "98/174, train_loss: 18.0830, time taken: 2.0876498222351074s\n",
      "99/174, train_loss: 14.7051, time taken: 1.973313808441162s\n",
      "100/174, train_loss: 14.7570, time taken: 1.7143893241882324s\n",
      "101/174, train_loss: 17.8775, time taken: 1.8789582252502441s\n",
      "102/174, train_loss: 21.0764, time taken: 1.7233197689056396s\n",
      "103/174, train_loss: 17.1148, time taken: 1.788658618927002s\n",
      "104/174, train_loss: 19.8333, time taken: 1.8047916889190674s\n",
      "105/174, train_loss: 17.2140, time taken: 1.8926315307617188s\n",
      "106/174, train_loss: 12.7635, time taken: 1.815185785293579s\n",
      "107/174, train_loss: 15.3028, time taken: 1.7684905529022217s\n",
      "108/174, train_loss: 21.1226, time taken: 1.9915106296539307s\n",
      "109/174, train_loss: 15.0116, time taken: 2.0208795070648193s\n",
      "110/174, train_loss: 17.4921, time taken: 1.878312587738037s\n",
      "111/174, train_loss: 15.0103, time taken: 1.7235147953033447s\n",
      "112/174, train_loss: 16.9087, time taken: 2.3923730850219727s\n",
      "113/174, train_loss: 14.8094, time taken: 1.9001336097717285s\n",
      "114/174, train_loss: 12.4323, time taken: 1.9720566272735596s\n",
      "115/174, train_loss: 13.2856, time taken: 2.113778829574585s\n",
      "116/174, train_loss: 16.9948, time taken: 2.0843505859375s\n",
      "117/174, train_loss: 17.5376, time taken: 2.0011415481567383s\n",
      "118/174, train_loss: 14.5910, time taken: 1.996330976486206s\n",
      "119/174, train_loss: 19.3887, time taken: 1.985752820968628s\n",
      "120/174, train_loss: 15.0254, time taken: 2.079824686050415s\n",
      "121/174, train_loss: 16.5466, time taken: 2.0071334838867188s\n",
      "122/174, train_loss: 14.4518, time taken: 2.399343252182007s\n",
      "123/174, train_loss: 19.9161, time taken: 2.0163495540618896s\n",
      "124/174, train_loss: 16.3618, time taken: 2.102057695388794s\n",
      "125/174, train_loss: 16.4032, time taken: 2.1996147632598877s\n",
      "126/174, train_loss: 14.4949, time taken: 1.8243491649627686s\n",
      "127/174, train_loss: 16.1060, time taken: 1.790281057357788s\n",
      "128/174, train_loss: 16.3524, time taken: 1.978691577911377s\n",
      "129/174, train_loss: 15.7985, time taken: 1.913985252380371s\n",
      "130/174, train_loss: 15.8471, time taken: 1.9919896125793457s\n",
      "131/174, train_loss: 16.0338, time taken: 1.8836541175842285s\n",
      "132/174, train_loss: 16.4629, time taken: 1.9148869514465332s\n",
      "133/174, train_loss: 19.0718, time taken: 1.8981223106384277s\n",
      "134/174, train_loss: 19.9703, time taken: 1.9984524250030518s\n",
      "135/174, train_loss: 14.7895, time taken: 1.8072073459625244s\n",
      "136/174, train_loss: 16.9899, time taken: 1.8801555633544922s\n",
      "137/174, train_loss: 17.8250, time taken: 1.8992478847503662s\n",
      "138/174, train_loss: 12.7123, time taken: 1.997462272644043s\n",
      "139/174, train_loss: 14.0397, time taken: 1.7951390743255615s\n",
      "140/174, train_loss: 14.6331, time taken: 3.4023585319519043s\n",
      "141/174, train_loss: 16.8097, time taken: 1.699979543685913s\n",
      "142/174, train_loss: 19.7722, time taken: 1.963338851928711s\n",
      "143/174, train_loss: 13.0292, time taken: 1.9170339107513428s\n",
      "144/174, train_loss: 11.7669, time taken: 1.8094942569732666s\n",
      "145/174, train_loss: 15.5269, time taken: 1.772160291671753s\n",
      "146/174, train_loss: 18.6641, time taken: 1.99306058883667s\n",
      "147/174, train_loss: 10.4283, time taken: 1.895993709564209s\n",
      "148/174, train_loss: 22.5052, time taken: 1.9186389446258545s\n",
      "149/174, train_loss: 20.5597, time taken: 1.8902242183685303s\n",
      "150/174, train_loss: 16.2546, time taken: 2.1934869289398193s\n",
      "151/174, train_loss: 17.0643, time taken: 1.9828133583068848s\n",
      "152/174, train_loss: 13.3999, time taken: 1.9001860618591309s\n",
      "153/174, train_loss: 16.7661, time taken: 1.822981595993042s\n",
      "154/174, train_loss: 18.2972, time taken: 1.867934226989746s\n",
      "155/174, train_loss: 20.4579, time taken: 2.1229047775268555s\n",
      "156/174, train_loss: 13.9426, time taken: 2.174232244491577s\n",
      "157/174, train_loss: 18.4472, time taken: 1.7236576080322266s\n",
      "158/174, train_loss: 17.2932, time taken: 2.067826509475708s\n",
      "159/174, train_loss: 17.1920, time taken: 1.9126403331756592s\n",
      "160/174, train_loss: 19.2710, time taken: 1.8889904022216797s\n",
      "161/174, train_loss: 19.5070, time taken: 2.0161292552948s\n",
      "162/174, train_loss: 21.8186, time taken: 1.8731753826141357s\n",
      "163/174, train_loss: 19.1601, time taken: 1.9106614589691162s\n",
      "164/174, train_loss: 18.4133, time taken: 1.9030778408050537s\n",
      "165/174, train_loss: 16.6452, time taken: 1.765993356704712s\n",
      "166/174, train_loss: 17.0375, time taken: 1.907317876815796s\n",
      "167/174, train_loss: 13.9615, time taken: 1.8128149509429932s\n",
      "168/174, train_loss: 13.7431, time taken: 1.9997153282165527s\n",
      "169/174, train_loss: 18.2313, time taken: 1.8948969841003418s\n",
      "170/174, train_loss: 18.8174, time taken: 2.089357852935791s\n",
      "171/174, train_loss: 12.5992, time taken: 1.7175285816192627s\n",
      "172/174, train_loss: 23.2272, time taken: 1.824568271636963s\n",
      "173/174, train_loss: 15.9126, time taken: 1.8143126964569092s\n",
      "174/174, train_loss: 19.6301, time taken: 1.7691326141357422s\n",
      "175/174, train_loss: 18.4161, time taken: 1.616565227508545s\n",
      "epoch 24 average loss: 17.0893\n",
      "----------\n",
      "epoch 25/2000\n",
      "1/174, train_loss: 19.6500, time taken: 8.14906930923462s\n",
      "2/174, train_loss: 16.5697, time taken: 2.5968897342681885s\n",
      "3/174, train_loss: 20.1087, time taken: 2.276695966720581s\n",
      "4/174, train_loss: 15.4139, time taken: 1.7135975360870361s\n",
      "5/174, train_loss: 16.7478, time taken: 2.29742693901062s\n",
      "6/174, train_loss: 15.8628, time taken: 2.1890997886657715s\n",
      "7/174, train_loss: 14.2699, time taken: 1.8060243129730225s\n",
      "8/174, train_loss: 17.3709, time taken: 2.0673668384552s\n",
      "9/174, train_loss: 14.7107, time taken: 2.0217673778533936s\n",
      "10/174, train_loss: 19.7312, time taken: 1.8696377277374268s\n",
      "11/174, train_loss: 17.4509, time taken: 1.7175605297088623s\n",
      "12/174, train_loss: 15.1093, time taken: 2.4783031940460205s\n",
      "13/174, train_loss: 20.8342, time taken: 2.0001087188720703s\n",
      "14/174, train_loss: 14.5265, time taken: 1.7970631122589111s\n",
      "15/174, train_loss: 15.4431, time taken: 1.9339568614959717s\n",
      "16/174, train_loss: 12.2647, time taken: 1.7220041751861572s\n",
      "17/174, train_loss: 18.3279, time taken: 1.911849021911621s\n",
      "18/174, train_loss: 16.6521, time taken: 1.8816633224487305s\n",
      "19/174, train_loss: 17.8032, time taken: 2.118145704269409s\n",
      "20/174, train_loss: 16.9726, time taken: 1.8607544898986816s\n",
      "21/174, train_loss: 15.2120, time taken: 1.808635950088501s\n",
      "22/174, train_loss: 18.3778, time taken: 2.2781217098236084s\n",
      "23/174, train_loss: 17.9939, time taken: 2.00539231300354s\n",
      "24/174, train_loss: 17.8070, time taken: 1.8889844417572021s\n",
      "25/174, train_loss: 20.8109, time taken: 2.1995933055877686s\n",
      "26/174, train_loss: 18.6920, time taken: 2.1079699993133545s\n",
      "27/174, train_loss: 15.3291, time taken: 1.805401086807251s\n",
      "28/174, train_loss: 17.1434, time taken: 1.8749186992645264s\n",
      "29/174, train_loss: 18.4992, time taken: 2.113719940185547s\n",
      "30/174, train_loss: 17.6165, time taken: 2.0888493061065674s\n",
      "31/174, train_loss: 19.4548, time taken: 2.001643657684326s\n",
      "32/174, train_loss: 17.7850, time taken: 1.7959208488464355s\n",
      "33/174, train_loss: 22.2664, time taken: 1.7798213958740234s\n",
      "34/174, train_loss: 23.1410, time taken: 1.820167064666748s\n",
      "35/174, train_loss: 12.7104, time taken: 1.903947114944458s\n",
      "36/174, train_loss: 15.0663, time taken: 1.8605270385742188s\n",
      "37/174, train_loss: 19.7315, time taken: 1.893996238708496s\n",
      "38/174, train_loss: 14.3047, time taken: 1.787609577178955s\n",
      "39/174, train_loss: 18.3309, time taken: 2.1202797889709473s\n",
      "40/174, train_loss: 23.2186, time taken: 2.084719657897949s\n",
      "41/174, train_loss: 14.6991, time taken: 1.6839549541473389s\n",
      "42/174, train_loss: 17.4110, time taken: 1.8066730499267578s\n",
      "43/174, train_loss: 14.3893, time taken: 1.8125495910644531s\n",
      "44/174, train_loss: 16.9397, time taken: 1.8029983043670654s\n",
      "45/174, train_loss: 14.5456, time taken: 1.869389533996582s\n",
      "46/174, train_loss: 12.8224, time taken: 1.8063971996307373s\n",
      "47/174, train_loss: 16.7894, time taken: 1.8121790885925293s\n",
      "48/174, train_loss: 18.5742, time taken: 1.8874130249023438s\n",
      "49/174, train_loss: 15.4936, time taken: 1.8097214698791504s\n",
      "50/174, train_loss: 18.7499, time taken: 1.8772621154785156s\n",
      "51/174, train_loss: 10.8247, time taken: 1.7965898513793945s\n",
      "52/174, train_loss: 18.1270, time taken: 1.9779675006866455s\n",
      "53/174, train_loss: 16.1826, time taken: 1.8227455615997314s\n",
      "54/174, train_loss: 14.7725, time taken: 1.8990695476531982s\n",
      "55/174, train_loss: 19.9274, time taken: 1.8700785636901855s\n",
      "56/174, train_loss: 15.0769, time taken: 1.825491189956665s\n",
      "57/174, train_loss: 19.8922, time taken: 3.470998764038086s\n",
      "58/174, train_loss: 14.0969, time taken: 1.779871940612793s\n",
      "59/174, train_loss: 17.8407, time taken: 1.7352840900421143s\n",
      "60/174, train_loss: 18.2943, time taken: 1.7596604824066162s\n",
      "61/174, train_loss: 18.1977, time taken: 2.0038678646087646s\n",
      "62/174, train_loss: 14.7302, time taken: 1.9087133407592773s\n",
      "63/174, train_loss: 19.0028, time taken: 1.9902610778808594s\n",
      "64/174, train_loss: 16.8151, time taken: 1.9146158695220947s\n",
      "65/174, train_loss: 20.8443, time taken: 1.8948140144348145s\n",
      "66/174, train_loss: 20.7775, time taken: 1.9657351970672607s\n",
      "67/174, train_loss: 17.1052, time taken: 2.2318994998931885s\n",
      "68/174, train_loss: 19.3424, time taken: 2.2685182094573975s\n",
      "69/174, train_loss: 23.0151, time taken: 2.2177793979644775s\n",
      "70/174, train_loss: 21.4743, time taken: 1.9854941368103027s\n",
      "71/174, train_loss: 15.7526, time taken: 2.107668876647949s\n",
      "72/174, train_loss: 14.6040, time taken: 2.0673584938049316s\n",
      "73/174, train_loss: 19.5504, time taken: 1.996889352798462s\n",
      "74/174, train_loss: 15.2606, time taken: 2.234748601913452s\n",
      "75/174, train_loss: 18.0945, time taken: 2.0716049671173096s\n",
      "76/174, train_loss: 18.9975, time taken: 2.015627384185791s\n",
      "77/174, train_loss: 15.3196, time taken: 1.8573691844940186s\n",
      "78/174, train_loss: 15.5814, time taken: 1.7343547344207764s\n",
      "79/174, train_loss: 16.8967, time taken: 1.9799768924713135s\n",
      "80/174, train_loss: 15.5585, time taken: 2.2030832767486572s\n",
      "81/174, train_loss: 19.0175, time taken: 1.9874470233917236s\n",
      "82/174, train_loss: 17.3236, time taken: 1.9780657291412354s\n",
      "83/174, train_loss: 18.9400, time taken: 1.8347148895263672s\n",
      "84/174, train_loss: 21.3473, time taken: 1.7997734546661377s\n",
      "85/174, train_loss: 19.5555, time taken: 1.887162446975708s\n",
      "86/174, train_loss: 18.3727, time taken: 1.966702938079834s\n",
      "87/174, train_loss: 18.3365, time taken: 2.0036978721618652s\n",
      "88/174, train_loss: 14.6232, time taken: 1.6984410285949707s\n",
      "89/174, train_loss: 23.8904, time taken: 1.8109869956970215s\n",
      "90/174, train_loss: 17.3683, time taken: 3.8973777294158936s\n",
      "91/174, train_loss: 17.7302, time taken: 2.0983896255493164s\n",
      "92/174, train_loss: 14.6069, time taken: 1.9156062602996826s\n",
      "93/174, train_loss: 15.7715, time taken: 1.7336094379425049s\n",
      "94/174, train_loss: 19.7590, time taken: 1.970127820968628s\n",
      "95/174, train_loss: 19.1654, time taken: 1.834444522857666s\n",
      "96/174, train_loss: 18.0311, time taken: 1.9630773067474365s\n",
      "97/174, train_loss: 16.8856, time taken: 1.8905549049377441s\n",
      "98/174, train_loss: 16.3852, time taken: 1.907228708267212s\n",
      "99/174, train_loss: 17.7952, time taken: 1.894792079925537s\n",
      "100/174, train_loss: 29.8063, time taken: 2.196075916290283s\n",
      "101/174, train_loss: 17.6070, time taken: 2.2148499488830566s\n",
      "102/174, train_loss: 21.4060, time taken: 2.1676056385040283s\n",
      "103/174, train_loss: 17.1168, time taken: 1.8166563510894775s\n",
      "104/174, train_loss: 17.9360, time taken: 2.27897047996521s\n",
      "105/174, train_loss: 13.3868, time taken: 2.0058116912841797s\n",
      "106/174, train_loss: 17.5911, time taken: 1.8982508182525635s\n",
      "107/174, train_loss: 19.0217, time taken: 1.9197115898132324s\n",
      "108/174, train_loss: 13.8739, time taken: 1.8715946674346924s\n",
      "109/174, train_loss: 17.6233, time taken: 2.0114586353302s\n",
      "110/174, train_loss: 15.5899, time taken: 1.869396686553955s\n",
      "111/174, train_loss: 17.6038, time taken: 1.813560962677002s\n",
      "112/174, train_loss: 16.8734, time taken: 1.6983578205108643s\n",
      "113/174, train_loss: 19.8132, time taken: 2.1905462741851807s\n",
      "114/174, train_loss: 14.7822, time taken: 1.8137717247009277s\n",
      "115/174, train_loss: 18.1345, time taken: 1.784745454788208s\n",
      "116/174, train_loss: 13.2579, time taken: 1.8921091556549072s\n",
      "117/174, train_loss: 14.4984, time taken: 1.9050686359405518s\n",
      "118/174, train_loss: 15.8916, time taken: 1.800126314163208s\n",
      "119/174, train_loss: 15.8383, time taken: 2.0936949253082275s\n",
      "120/174, train_loss: 13.3983, time taken: 1.977069616317749s\n",
      "121/174, train_loss: 15.9514, time taken: 1.99263596534729s\n",
      "122/174, train_loss: 13.5433, time taken: 1.8056745529174805s\n",
      "123/174, train_loss: 16.2275, time taken: 2.1173620223999023s\n",
      "124/174, train_loss: 13.8376, time taken: 1.8738455772399902s\n",
      "125/174, train_loss: 12.6029, time taken: 1.9965026378631592s\n",
      "126/174, train_loss: 20.3624, time taken: 1.8061437606811523s\n",
      "127/174, train_loss: 15.8236, time taken: 1.8838934898376465s\n",
      "128/174, train_loss: 11.9376, time taken: 2.08955454826355s\n",
      "129/174, train_loss: 15.8287, time taken: 1.9214470386505127s\n",
      "130/174, train_loss: 15.5718, time taken: 1.7925214767456055s\n",
      "131/174, train_loss: 16.3909, time taken: 1.7831957340240479s\n",
      "132/174, train_loss: 18.0727, time taken: 2.2088985443115234s\n",
      "133/174, train_loss: 15.8986, time taken: 1.891495704650879s\n",
      "134/174, train_loss: 17.7703, time taken: 1.7940313816070557s\n",
      "135/174, train_loss: 17.2601, time taken: 2.0903754234313965s\n",
      "136/174, train_loss: 18.2470, time taken: 1.8831164836883545s\n",
      "137/174, train_loss: 17.1392, time taken: 1.8216772079467773s\n",
      "138/174, train_loss: 18.2157, time taken: 1.984570026397705s\n",
      "139/174, train_loss: 17.7261, time taken: 2.1164190769195557s\n",
      "140/174, train_loss: 19.6629, time taken: 1.9600036144256592s\n",
      "141/174, train_loss: 18.2270, time taken: 1.9266023635864258s\n",
      "142/174, train_loss: 14.2747, time taken: 2.1954569816589355s\n",
      "143/174, train_loss: 17.6367, time taken: 1.7996916770935059s\n",
      "144/174, train_loss: 14.9002, time taken: 1.7758138179779053s\n",
      "145/174, train_loss: 14.2930, time taken: 1.9962291717529297s\n",
      "146/174, train_loss: 14.7748, time taken: 1.9266064167022705s\n",
      "147/174, train_loss: 13.7645, time taken: 1.966156244277954s\n",
      "148/174, train_loss: 19.1434, time taken: 1.9210493564605713s\n",
      "149/174, train_loss: 17.9999, time taken: 1.8688876628875732s\n",
      "150/174, train_loss: 15.4470, time taken: 1.9024691581726074s\n",
      "151/174, train_loss: 22.5825, time taken: 1.7800638675689697s\n",
      "152/174, train_loss: 18.2275, time taken: 1.7147693634033203s\n",
      "153/174, train_loss: 15.1637, time taken: 1.9952306747436523s\n",
      "154/174, train_loss: 14.4600, time taken: 1.9922218322753906s\n",
      "155/174, train_loss: 16.9625, time taken: 1.8967430591583252s\n",
      "156/174, train_loss: 18.6555, time taken: 1.7116165161132812s\n",
      "157/174, train_loss: 12.7557, time taken: 1.7595834732055664s\n",
      "158/174, train_loss: 17.0384, time taken: 1.733006238937378s\n",
      "159/174, train_loss: 15.5527, time taken: 1.7804515361785889s\n",
      "160/174, train_loss: 17.0600, time taken: 1.6953356266021729s\n",
      "161/174, train_loss: 19.2970, time taken: 1.903416633605957s\n",
      "162/174, train_loss: 15.3198, time taken: 2.01045298576355s\n",
      "163/174, train_loss: 18.4715, time taken: 1.9611318111419678s\n",
      "164/174, train_loss: 15.9783, time taken: 1.7084283828735352s\n",
      "165/174, train_loss: 19.9068, time taken: 1.7204995155334473s\n",
      "166/174, train_loss: 16.0619, time taken: 1.962327003479004s\n",
      "167/174, train_loss: 17.3662, time taken: 1.7313816547393799s\n",
      "168/174, train_loss: 15.6210, time taken: 1.7770397663116455s\n",
      "169/174, train_loss: 13.8562, time taken: 1.8993003368377686s\n",
      "170/174, train_loss: 19.9905, time taken: 1.7881488800048828s\n",
      "171/174, train_loss: 18.7045, time taken: 1.7839767932891846s\n",
      "172/174, train_loss: 18.3494, time taken: 1.8149333000183105s\n",
      "173/174, train_loss: 20.4490, time taken: 1.902841329574585s\n",
      "174/174, train_loss: 20.8657, time taken: 3.877389907836914s\n",
      "175/174, train_loss: 14.7371, time taken: 1.5159249305725098s\n",
      "epoch 25 average loss: 17.1977\n",
      "Entering Validation for epoch: 25\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 25 Validation avg loss: 11.8707, time taken: 1.0555956363677979s\n",
      "----------\n",
      "epoch 26/2000\n",
      "1/174, train_loss: 12.0391, time taken: 8.551807165145874s\n",
      "2/174, train_loss: 13.2080, time taken: 2.489299774169922s\n",
      "3/174, train_loss: 19.2241, time taken: 1.7999515533447266s\n",
      "4/174, train_loss: 22.5158, time taken: 1.8915822505950928s\n",
      "5/174, train_loss: 16.7276, time taken: 1.9794998168945312s\n",
      "6/174, train_loss: 21.0077, time taken: 2.598221778869629s\n",
      "7/174, train_loss: 18.1337, time taken: 2.2284598350524902s\n",
      "8/174, train_loss: 17.3149, time taken: 1.8851864337921143s\n",
      "9/174, train_loss: 19.2350, time taken: 2.372727870941162s\n",
      "10/174, train_loss: 14.8764, time taken: 1.8114986419677734s\n",
      "11/174, train_loss: 21.3555, time taken: 1.9864189624786377s\n",
      "12/174, train_loss: 16.6161, time taken: 1.8150787353515625s\n",
      "13/174, train_loss: 15.8297, time taken: 2.1060872077941895s\n",
      "14/174, train_loss: 16.9320, time taken: 1.9936838150024414s\n",
      "15/174, train_loss: 18.6511, time taken: 2.1080970764160156s\n",
      "16/174, train_loss: 16.7144, time taken: 1.8888726234436035s\n",
      "17/174, train_loss: 15.5927, time taken: 1.7829420566558838s\n",
      "18/174, train_loss: 15.2554, time taken: 2.2810611724853516s\n",
      "19/174, train_loss: 16.7088, time taken: 1.819603681564331s\n",
      "20/174, train_loss: 22.2472, time taken: 1.890167236328125s\n",
      "21/174, train_loss: 18.5785, time taken: 1.8015327453613281s\n",
      "22/174, train_loss: 20.0686, time taken: 1.8796133995056152s\n",
      "23/174, train_loss: 16.6458, time taken: 1.7961771488189697s\n",
      "24/174, train_loss: 14.5158, time taken: 1.9981329441070557s\n",
      "25/174, train_loss: 19.9668, time taken: 1.8059992790222168s\n",
      "26/174, train_loss: 19.5934, time taken: 1.7751407623291016s\n",
      "27/174, train_loss: 16.0244, time taken: 1.7237308025360107s\n",
      "28/174, train_loss: 20.6945, time taken: 1.8935048580169678s\n",
      "29/174, train_loss: 19.7144, time taken: 1.779078483581543s\n",
      "30/174, train_loss: 21.7410, time taken: 1.8114559650421143s\n",
      "31/174, train_loss: 16.9227, time taken: 1.9006297588348389s\n",
      "32/174, train_loss: 17.3717, time taken: 2.0803093910217285s\n",
      "33/174, train_loss: 16.7683, time taken: 2.0013904571533203s\n",
      "34/174, train_loss: 16.5676, time taken: 1.9177463054656982s\n",
      "35/174, train_loss: 16.2191, time taken: 1.8935115337371826s\n",
      "36/174, train_loss: 18.1032, time taken: 1.784604787826538s\n",
      "37/174, train_loss: 17.6301, time taken: 1.8079805374145508s\n",
      "38/174, train_loss: 15.0103, time taken: 1.8721249103546143s\n",
      "39/174, train_loss: 16.0376, time taken: 1.994373083114624s\n",
      "40/174, train_loss: 17.3835, time taken: 1.9932971000671387s\n",
      "41/174, train_loss: 13.1313, time taken: 1.9120962619781494s\n",
      "42/174, train_loss: 15.4351, time taken: 1.905919075012207s\n",
      "43/174, train_loss: 16.7533, time taken: 1.8946723937988281s\n",
      "44/174, train_loss: 15.5198, time taken: 1.81154465675354s\n",
      "45/174, train_loss: 20.0552, time taken: 1.7980523109436035s\n",
      "46/174, train_loss: 24.2237, time taken: 2.202791213989258s\n",
      "47/174, train_loss: 15.9346, time taken: 1.928311824798584s\n",
      "48/174, train_loss: 17.9920, time taken: 1.7782981395721436s\n",
      "49/174, train_loss: 18.1052, time taken: 1.817807912826538s\n",
      "50/174, train_loss: 19.8038, time taken: 1.9889702796936035s\n",
      "51/174, train_loss: 19.9699, time taken: 2.0034267902374268s\n",
      "52/174, train_loss: 16.4969, time taken: 1.8616926670074463s\n",
      "53/174, train_loss: 17.5612, time taken: 1.8954987525939941s\n",
      "54/174, train_loss: 21.3852, time taken: 1.8277063369750977s\n",
      "55/174, train_loss: 13.0382, time taken: 1.9889838695526123s\n",
      "56/174, train_loss: 16.4698, time taken: 1.8812341690063477s\n",
      "57/174, train_loss: 18.4297, time taken: 2.207529067993164s\n",
      "58/174, train_loss: 23.2587, time taken: 2.4938995838165283s\n",
      "59/174, train_loss: 17.9936, time taken: 1.9063191413879395s\n",
      "60/174, train_loss: 15.6307, time taken: 1.9009451866149902s\n",
      "61/174, train_loss: 17.4555, time taken: 2.068953514099121s\n",
      "62/174, train_loss: 14.3439, time taken: 2.090391159057617s\n",
      "63/174, train_loss: 15.4821, time taken: 1.9097533226013184s\n",
      "64/174, train_loss: 15.1104, time taken: 1.9227638244628906s\n",
      "65/174, train_loss: 15.8813, time taken: 1.736534595489502s\n",
      "66/174, train_loss: 18.6577, time taken: 1.7637386322021484s\n",
      "67/174, train_loss: 16.9523, time taken: 2.017194986343384s\n",
      "68/174, train_loss: 17.7747, time taken: 2.080695152282715s\n",
      "69/174, train_loss: 19.8161, time taken: 2.015728235244751s\n",
      "70/174, train_loss: 23.9521, time taken: 2.1734073162078857s\n",
      "71/174, train_loss: 15.7468, time taken: 1.7905902862548828s\n",
      "72/174, train_loss: 10.1195, time taken: 2.0236434936523438s\n",
      "73/174, train_loss: 18.6708, time taken: 2.0964016914367676s\n",
      "74/174, train_loss: 17.8288, time taken: 2.26466965675354s\n",
      "75/174, train_loss: 16.5079, time taken: 1.828676462173462s\n",
      "76/174, train_loss: 16.8296, time taken: 1.993086338043213s\n",
      "77/174, train_loss: 22.1185, time taken: 1.9997191429138184s\n",
      "78/174, train_loss: 18.4248, time taken: 1.9773621559143066s\n",
      "79/174, train_loss: 19.4962, time taken: 1.7890665531158447s\n",
      "80/174, train_loss: 19.1746, time taken: 1.9110896587371826s\n",
      "81/174, train_loss: 17.7073, time taken: 1.8174536228179932s\n",
      "82/174, train_loss: 15.1365, time taken: 1.7041800022125244s\n",
      "83/174, train_loss: 16.1958, time taken: 1.8015406131744385s\n",
      "84/174, train_loss: 13.2580, time taken: 1.7122321128845215s\n",
      "85/174, train_loss: 15.8371, time taken: 1.8057594299316406s\n",
      "86/174, train_loss: 14.0737, time taken: 1.8989920616149902s\n",
      "87/174, train_loss: 18.2003, time taken: 1.8029277324676514s\n",
      "88/174, train_loss: 14.5674, time taken: 1.8167724609375s\n",
      "89/174, train_loss: 18.9821, time taken: 1.7827999591827393s\n",
      "90/174, train_loss: 19.9077, time taken: 1.7158405780792236s\n",
      "91/174, train_loss: 21.8992, time taken: 2.0023176670074463s\n",
      "92/174, train_loss: 14.1372, time taken: 2.082361936569214s\n",
      "93/174, train_loss: 14.5934, time taken: 1.8945775032043457s\n",
      "94/174, train_loss: 21.3929, time taken: 3.60440731048584s\n",
      "95/174, train_loss: 16.3701, time taken: 1.8101506233215332s\n",
      "96/174, train_loss: 19.0118, time taken: 1.7892465591430664s\n",
      "97/174, train_loss: 15.3194, time taken: 1.8924193382263184s\n",
      "98/174, train_loss: 12.7643, time taken: 2.265137195587158s\n",
      "99/174, train_loss: 16.2629, time taken: 2.009273052215576s\n",
      "100/174, train_loss: 14.5721, time taken: 1.922206163406372s\n",
      "101/174, train_loss: 16.8049, time taken: 1.9903573989868164s\n",
      "102/174, train_loss: 21.8628, time taken: 1.9085299968719482s\n",
      "103/174, train_loss: 15.4704, time taken: 2.188715934753418s\n",
      "104/174, train_loss: 19.1768, time taken: 2.1892502307891846s\n",
      "105/174, train_loss: 15.7851, time taken: 2.3896324634552s\n",
      "106/174, train_loss: 18.5578, time taken: 2.001328706741333s\n",
      "107/174, train_loss: 20.1433, time taken: 1.8886334896087646s\n",
      "108/174, train_loss: 16.1447, time taken: 1.9858314990997314s\n",
      "109/174, train_loss: 12.7860, time taken: 1.7941944599151611s\n",
      "110/174, train_loss: 13.4294, time taken: 1.7224769592285156s\n",
      "111/174, train_loss: 17.6813, time taken: 2.0009005069732666s\n",
      "112/174, train_loss: 15.5621, time taken: 1.9639601707458496s\n",
      "113/174, train_loss: 17.4665, time taken: 1.816795825958252s\n",
      "114/174, train_loss: 17.9126, time taken: 2.0827419757843018s\n",
      "115/174, train_loss: 19.2303, time taken: 1.7017569541931152s\n",
      "116/174, train_loss: 18.1503, time taken: 1.8025360107421875s\n",
      "117/174, train_loss: 15.4806, time taken: 1.8736319541931152s\n",
      "118/174, train_loss: 18.1278, time taken: 1.7208251953125s\n",
      "119/174, train_loss: 19.6425, time taken: 1.6928133964538574s\n",
      "120/174, train_loss: 15.3952, time taken: 1.8740606307983398s\n",
      "121/174, train_loss: 15.6327, time taken: 1.7399365901947021s\n",
      "122/174, train_loss: 18.1886, time taken: 1.8817496299743652s\n",
      "123/174, train_loss: 10.9285, time taken: 1.7047474384307861s\n",
      "124/174, train_loss: 19.4752, time taken: 1.7991139888763428s\n",
      "125/174, train_loss: 17.5832, time taken: 1.8793296813964844s\n",
      "126/174, train_loss: 12.9582, time taken: 1.6821181774139404s\n",
      "127/174, train_loss: 13.7430, time taken: 1.7203583717346191s\n",
      "128/174, train_loss: 12.0705, time taken: 3.4895153045654297s\n",
      "129/174, train_loss: 19.3810, time taken: 1.7868962287902832s\n",
      "130/174, train_loss: 11.5690, time taken: 2.0241832733154297s\n",
      "131/174, train_loss: 19.2300, time taken: 1.819495439529419s\n",
      "132/174, train_loss: 20.5458, time taken: 1.8980424404144287s\n",
      "133/174, train_loss: 18.3639, time taken: 1.8951668739318848s\n",
      "134/174, train_loss: 13.2560, time taken: 1.9955039024353027s\n",
      "135/174, train_loss: 16.3216, time taken: 1.813202142715454s\n",
      "136/174, train_loss: 17.2582, time taken: 1.9744963645935059s\n",
      "137/174, train_loss: 15.6176, time taken: 1.7984850406646729s\n",
      "138/174, train_loss: 20.1194, time taken: 2.111647844314575s\n",
      "139/174, train_loss: 22.2788, time taken: 1.8873519897460938s\n",
      "140/174, train_loss: 18.3181, time taken: 1.811812162399292s\n",
      "141/174, train_loss: 15.1147, time taken: 2.1844518184661865s\n",
      "142/174, train_loss: 14.0664, time taken: 2.3992159366607666s\n",
      "143/174, train_loss: 15.5213, time taken: 1.801243543624878s\n",
      "144/174, train_loss: 18.7668, time taken: 2.163863182067871s\n",
      "145/174, train_loss: 17.0376, time taken: 1.9995465278625488s\n",
      "146/174, train_loss: 17.0723, time taken: 1.917215347290039s\n",
      "147/174, train_loss: 17.9349, time taken: 1.800276756286621s\n",
      "148/174, train_loss: 17.8419, time taken: 1.8039195537567139s\n",
      "149/174, train_loss: 17.8551, time taken: 1.9979124069213867s\n",
      "150/174, train_loss: 19.8190, time taken: 1.8786163330078125s\n",
      "151/174, train_loss: 18.6550, time taken: 1.894636631011963s\n",
      "152/174, train_loss: 15.4012, time taken: 1.9937243461608887s\n",
      "153/174, train_loss: 14.4752, time taken: 1.8972630500793457s\n",
      "154/174, train_loss: 18.0813, time taken: 1.9158828258514404s\n",
      "155/174, train_loss: 18.4171, time taken: 2.097228527069092s\n",
      "156/174, train_loss: 20.5792, time taken: 1.8102447986602783s\n",
      "157/174, train_loss: 14.5536, time taken: 2.0651440620422363s\n",
      "158/174, train_loss: 15.1500, time taken: 1.833465337753296s\n",
      "159/174, train_loss: 20.5820, time taken: 2.0136756896972656s\n",
      "160/174, train_loss: 18.2226, time taken: 1.9230742454528809s\n",
      "161/174, train_loss: 20.3545, time taken: 1.8363919258117676s\n",
      "162/174, train_loss: 17.5394, time taken: 1.7873289585113525s\n",
      "163/174, train_loss: 14.1190, time taken: 1.7933154106140137s\n",
      "164/174, train_loss: 18.4888, time taken: 1.7069587707519531s\n",
      "165/174, train_loss: 15.7542, time taken: 1.7731058597564697s\n",
      "166/174, train_loss: 14.9765, time taken: 1.8128662109375s\n",
      "167/174, train_loss: 15.9162, time taken: 1.7829766273498535s\n",
      "168/174, train_loss: 18.7184, time taken: 1.8221540451049805s\n",
      "169/174, train_loss: 12.5230, time taken: 1.7839746475219727s\n",
      "170/174, train_loss: 14.0958, time taken: 1.7830772399902344s\n",
      "171/174, train_loss: 14.8302, time taken: 1.7093803882598877s\n",
      "172/174, train_loss: 14.3016, time taken: 1.7063159942626953s\n",
      "173/174, train_loss: 19.9622, time taken: 1.7047162055969238s\n",
      "174/174, train_loss: 15.9486, time taken: 3.675560235977173s\n",
      "175/174, train_loss: 24.1145, time taken: 1.62066650390625s\n",
      "epoch 26 average loss: 17.2452\n",
      "----------\n",
      "epoch 27/2000\n",
      "1/174, train_loss: 17.5531, time taken: 8.744325637817383s\n",
      "2/174, train_loss: 14.9255, time taken: 2.5521926879882812s\n",
      "3/174, train_loss: 14.1834, time taken: 1.911393404006958s\n",
      "4/174, train_loss: 15.0855, time taken: 2.2077903747558594s\n",
      "5/174, train_loss: 15.4555, time taken: 2.0068650245666504s\n",
      "6/174, train_loss: 19.7201, time taken: 1.8925135135650635s\n",
      "7/174, train_loss: 13.3983, time taken: 2.0974888801574707s\n",
      "8/174, train_loss: 12.9531, time taken: 1.8843059539794922s\n",
      "9/174, train_loss: 16.0928, time taken: 1.8954856395721436s\n",
      "10/174, train_loss: 15.3632, time taken: 2.107583522796631s\n",
      "11/174, train_loss: 20.2491, time taken: 2.309096574783325s\n",
      "12/174, train_loss: 16.8285, time taken: 2.0744495391845703s\n",
      "13/174, train_loss: 20.5400, time taken: 2.3240365982055664s\n",
      "14/174, train_loss: 20.0134, time taken: 2.063368797302246s\n",
      "15/174, train_loss: 14.4546, time taken: 1.7033195495605469s\n",
      "16/174, train_loss: 16.1993, time taken: 1.9846594333648682s\n",
      "17/174, train_loss: 19.6158, time taken: 2.031108856201172s\n",
      "18/174, train_loss: 16.5958, time taken: 1.9734299182891846s\n",
      "19/174, train_loss: 15.1124, time taken: 2.0148119926452637s\n",
      "20/174, train_loss: 15.1381, time taken: 1.7676100730895996s\n",
      "21/174, train_loss: 15.2960, time taken: 1.8144426345825195s\n",
      "22/174, train_loss: 16.7132, time taken: 1.7939178943634033s\n",
      "23/174, train_loss: 20.2032, time taken: 1.705747365951538s\n",
      "24/174, train_loss: 9.0375, time taken: 1.913203239440918s\n",
      "25/174, train_loss: 15.7921, time taken: 1.771463394165039s\n",
      "26/174, train_loss: 13.2702, time taken: 1.7157280445098877s\n",
      "27/174, train_loss: 19.3414, time taken: 2.150151014328003s\n",
      "28/174, train_loss: 12.5596, time taken: 1.7005136013031006s\n",
      "29/174, train_loss: 17.4331, time taken: 2.0203890800476074s\n",
      "30/174, train_loss: 12.5263, time taken: 2.076507091522217s\n",
      "31/174, train_loss: 19.5585, time taken: 1.8231971263885498s\n",
      "32/174, train_loss: 14.3984, time taken: 1.8020830154418945s\n",
      "33/174, train_loss: 9.7570, time taken: 1.6989283561706543s\n",
      "34/174, train_loss: 14.5111, time taken: 1.880279541015625s\n",
      "35/174, train_loss: 16.4918, time taken: 2.0929086208343506s\n",
      "36/174, train_loss: 18.7795, time taken: 1.79878568649292s\n",
      "37/174, train_loss: 17.5661, time taken: 1.8942060470581055s\n",
      "38/174, train_loss: 14.5020, time taken: 1.8789381980895996s\n",
      "39/174, train_loss: 16.8411, time taken: 2.108513116836548s\n",
      "40/174, train_loss: 21.4223, time taken: 2.0831844806671143s\n",
      "41/174, train_loss: 14.4079, time taken: 2.011343479156494s\n",
      "42/174, train_loss: 17.8254, time taken: 1.91292142868042s\n",
      "43/174, train_loss: 14.5383, time taken: 1.8850224018096924s\n",
      "44/174, train_loss: 19.6342, time taken: 2.197892904281616s\n",
      "45/174, train_loss: 16.3142, time taken: 2.0815770626068115s\n",
      "46/174, train_loss: 15.9441, time taken: 1.810546636581421s\n",
      "47/174, train_loss: 22.6645, time taken: 1.8021154403686523s\n",
      "48/174, train_loss: 17.2526, time taken: 1.778834342956543s\n",
      "49/174, train_loss: 15.2495, time taken: 1.707991123199463s\n",
      "50/174, train_loss: 19.5457, time taken: 1.8993761539459229s\n",
      "51/174, train_loss: 17.6084, time taken: 1.8851354122161865s\n",
      "52/174, train_loss: 19.6858, time taken: 1.8063926696777344s\n",
      "53/174, train_loss: 20.7337, time taken: 2.0991361141204834s\n",
      "54/174, train_loss: 17.8030, time taken: 1.7868423461914062s\n",
      "55/174, train_loss: 15.9378, time taken: 1.8895049095153809s\n",
      "56/174, train_loss: 13.3003, time taken: 1.9190587997436523s\n",
      "57/174, train_loss: 15.4345, time taken: 2.1895697116851807s\n",
      "58/174, train_loss: 18.9910, time taken: 1.8969526290893555s\n",
      "59/174, train_loss: 17.6837, time taken: 1.9130702018737793s\n",
      "60/174, train_loss: 14.3326, time taken: 1.7863972187042236s\n",
      "61/174, train_loss: 16.7188, time taken: 1.8950755596160889s\n",
      "62/174, train_loss: 15.2141, time taken: 1.7135274410247803s\n",
      "63/174, train_loss: 18.3214, time taken: 1.8748619556427002s\n",
      "64/174, train_loss: 19.6104, time taken: 1.8007328510284424s\n",
      "65/174, train_loss: 17.3763, time taken: 1.80057692527771s\n",
      "66/174, train_loss: 18.7472, time taken: 1.7161071300506592s\n",
      "67/174, train_loss: 19.6511, time taken: 1.9228477478027344s\n",
      "68/174, train_loss: 22.3091, time taken: 1.9772834777832031s\n",
      "69/174, train_loss: 13.7043, time taken: 1.790344476699829s\n",
      "70/174, train_loss: 20.4315, time taken: 1.911097526550293s\n",
      "71/174, train_loss: 17.5040, time taken: 1.922539472579956s\n",
      "72/174, train_loss: 20.2132, time taken: 1.89030122756958s\n",
      "73/174, train_loss: 17.1393, time taken: 1.8011186122894287s\n",
      "74/174, train_loss: 16.3624, time taken: 2.167994499206543s\n",
      "75/174, train_loss: 16.2130, time taken: 3.417691946029663s\n",
      "76/174, train_loss: 19.3150, time taken: 1.7689151763916016s\n",
      "77/174, train_loss: 15.3848, time taken: 2.0122392177581787s\n",
      "78/174, train_loss: 17.0309, time taken: 1.9858183860778809s\n",
      "79/174, train_loss: 16.1618, time taken: 2.226162910461426s\n",
      "80/174, train_loss: 17.4525, time taken: 1.8956961631774902s\n",
      "81/174, train_loss: 17.4832, time taken: 2.0085489749908447s\n",
      "82/174, train_loss: 19.2624, time taken: 2.0706512928009033s\n",
      "83/174, train_loss: 22.0184, time taken: 1.8942084312438965s\n",
      "84/174, train_loss: 17.5896, time taken: 1.8868515491485596s\n",
      "85/174, train_loss: 19.4447, time taken: 1.9139673709869385s\n",
      "86/174, train_loss: 18.9715, time taken: 2.1731295585632324s\n",
      "87/174, train_loss: 15.2818, time taken: 1.722595453262329s\n",
      "88/174, train_loss: 15.2806, time taken: 2.2076971530914307s\n",
      "89/174, train_loss: 18.9520, time taken: 2.1001923084259033s\n",
      "90/174, train_loss: 18.2342, time taken: 2.11017107963562s\n",
      "91/174, train_loss: 12.9917, time taken: 1.8130509853363037s\n",
      "92/174, train_loss: 18.9820, time taken: 1.979142665863037s\n",
      "93/174, train_loss: 20.5796, time taken: 2.1989142894744873s\n",
      "94/174, train_loss: 19.0091, time taken: 1.914442777633667s\n",
      "95/174, train_loss: 18.0687, time taken: 1.8006203174591064s\n",
      "96/174, train_loss: 15.3375, time taken: 1.7982139587402344s\n",
      "97/174, train_loss: 16.0360, time taken: 1.9569239616394043s\n",
      "98/174, train_loss: 19.8763, time taken: 1.7027499675750732s\n",
      "99/174, train_loss: 21.8198, time taken: 1.8220982551574707s\n",
      "100/174, train_loss: 15.8765, time taken: 1.8887381553649902s\n",
      "101/174, train_loss: 14.6831, time taken: 1.9002635478973389s\n",
      "102/174, train_loss: 18.6337, time taken: 1.857889175415039s\n",
      "103/174, train_loss: 17.3952, time taken: 1.8154230117797852s\n",
      "104/174, train_loss: 22.5974, time taken: 1.9906647205352783s\n",
      "105/174, train_loss: 19.1949, time taken: 1.794743537902832s\n",
      "106/174, train_loss: 18.0340, time taken: 1.993088722229004s\n",
      "107/174, train_loss: 18.4688, time taken: 2.0232715606689453s\n",
      "108/174, train_loss: 16.2526, time taken: 2.0626301765441895s\n",
      "109/174, train_loss: 16.8307, time taken: 2.119312286376953s\n",
      "110/174, train_loss: 15.0200, time taken: 1.9166874885559082s\n",
      "111/174, train_loss: 16.8109, time taken: 1.7186882495880127s\n",
      "112/174, train_loss: 16.3645, time taken: 1.781362771987915s\n",
      "113/174, train_loss: 16.0382, time taken: 2.812608242034912s\n",
      "114/174, train_loss: 18.6931, time taken: 1.8012113571166992s\n",
      "115/174, train_loss: 19.4747, time taken: 2.0151050090789795s\n",
      "116/174, train_loss: 15.0153, time taken: 2.3018317222595215s\n",
      "117/174, train_loss: 18.2516, time taken: 2.003812074661255s\n",
      "118/174, train_loss: 12.6660, time taken: 2.03190541267395s\n",
      "119/174, train_loss: 16.7534, time taken: 1.752359390258789s\n",
      "120/174, train_loss: 16.5079, time taken: 1.7392206192016602s\n",
      "121/174, train_loss: 15.1806, time taken: 2.209080934524536s\n",
      "122/174, train_loss: 18.1215, time taken: 2.1150357723236084s\n",
      "123/174, train_loss: 18.0145, time taken: 1.8074936866760254s\n",
      "124/174, train_loss: 16.4688, time taken: 1.8684144020080566s\n",
      "125/174, train_loss: 19.6501, time taken: 1.9049787521362305s\n",
      "126/174, train_loss: 15.5978, time taken: 1.8125998973846436s\n",
      "127/174, train_loss: 17.7599, time taken: 1.9862384796142578s\n",
      "128/174, train_loss: 16.7034, time taken: 2.188544750213623s\n",
      "129/174, train_loss: 18.3536, time taken: 1.900648593902588s\n",
      "130/174, train_loss: 14.3985, time taken: 1.781440019607544s\n",
      "131/174, train_loss: 16.9381, time taken: 2.2194089889526367s\n",
      "132/174, train_loss: 14.0036, time taken: 1.9968640804290771s\n",
      "133/174, train_loss: 14.5960, time taken: 1.785813331604004s\n",
      "134/174, train_loss: 14.4071, time taken: 1.900294303894043s\n",
      "135/174, train_loss: 14.1206, time taken: 1.8931429386138916s\n",
      "136/174, train_loss: 15.7886, time taken: 1.9092400074005127s\n",
      "137/174, train_loss: 17.1932, time taken: 1.998351812362671s\n",
      "138/174, train_loss: 16.7749, time taken: 1.8078677654266357s\n",
      "139/174, train_loss: 18.6240, time taken: 1.7729871273040771s\n",
      "140/174, train_loss: 16.0215, time taken: 1.7224552631378174s\n",
      "141/174, train_loss: 19.0173, time taken: 1.7191839218139648s\n",
      "142/174, train_loss: 18.3888, time taken: 1.7243545055389404s\n",
      "143/174, train_loss: 18.0266, time taken: 1.8617112636566162s\n",
      "144/174, train_loss: 14.2672, time taken: 1.8082365989685059s\n",
      "145/174, train_loss: 17.2824, time taken: 2.1002137660980225s\n",
      "146/174, train_loss: 18.2735, time taken: 2.0738866329193115s\n",
      "147/174, train_loss: 13.7103, time taken: 1.6366424560546875s\n",
      "148/174, train_loss: 12.1067, time taken: 1.7145323753356934s\n",
      "149/174, train_loss: 15.8581, time taken: 1.8913464546203613s\n",
      "150/174, train_loss: 14.8289, time taken: 1.907628059387207s\n",
      "151/174, train_loss: 19.2160, time taken: 1.7082905769348145s\n",
      "152/174, train_loss: 14.8721, time taken: 1.776444435119629s\n",
      "153/174, train_loss: 15.9898, time taken: 1.9278168678283691s\n",
      "154/174, train_loss: 14.3950, time taken: 1.8456752300262451s\n",
      "155/174, train_loss: 15.2377, time taken: 1.8673973083496094s\n",
      "156/174, train_loss: 18.7348, time taken: 2.08841609954834s\n",
      "157/174, train_loss: 18.2418, time taken: 1.9140665531158447s\n",
      "158/174, train_loss: 16.7424, time taken: 2.0018820762634277s\n",
      "159/174, train_loss: 17.4017, time taken: 1.7054462432861328s\n",
      "160/174, train_loss: 17.9374, time taken: 1.9699485301971436s\n",
      "161/174, train_loss: 16.1764, time taken: 1.8175225257873535s\n",
      "162/174, train_loss: 19.1896, time taken: 1.7651758193969727s\n",
      "163/174, train_loss: 18.2843, time taken: 3.234708547592163s\n",
      "164/174, train_loss: 20.6884, time taken: 2.1911375522613525s\n",
      "165/174, train_loss: 19.8598, time taken: 1.8659718036651611s\n",
      "166/174, train_loss: 14.4265, time taken: 1.8951399326324463s\n",
      "167/174, train_loss: 18.8933, time taken: 1.8363769054412842s\n",
      "168/174, train_loss: 20.7945, time taken: 1.7665963172912598s\n",
      "169/174, train_loss: 12.3505, time taken: 1.996697187423706s\n",
      "170/174, train_loss: 18.8446, time taken: 1.7177698612213135s\n",
      "171/174, train_loss: 17.1489, time taken: 1.8895432949066162s\n",
      "172/174, train_loss: 16.3862, time taken: 1.7161734104156494s\n",
      "173/174, train_loss: 18.7042, time taken: 2.061633586883545s\n",
      "174/174, train_loss: 19.2251, time taken: 1.816406011581421s\n",
      "175/174, train_loss: 20.0194, time taken: 1.490886926651001s\n",
      "epoch 27 average loss: 17.0300\n",
      "Entering Validation for epoch: 27\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 27 Validation avg loss: 12.8949, time taken: 1.0765764713287354s\n",
      "----------\n",
      "epoch 28/2000\n",
      "1/174, train_loss: 19.2005, time taken: 8.521050691604614s\n",
      "2/174, train_loss: 16.9932, time taken: 2.7806873321533203s\n",
      "3/174, train_loss: 18.2633, time taken: 2.090125560760498s\n",
      "4/174, train_loss: 14.4322, time taken: 1.9344167709350586s\n",
      "5/174, train_loss: 16.2466, time taken: 2.082761764526367s\n",
      "6/174, train_loss: 14.6450, time taken: 2.080575942993164s\n",
      "7/174, train_loss: 17.6754, time taken: 1.8092539310455322s\n",
      "8/174, train_loss: 17.0061, time taken: 1.8781578540802002s\n",
      "9/174, train_loss: 17.1091, time taken: 1.9989213943481445s\n",
      "10/174, train_loss: 14.2828, time taken: 1.7131097316741943s\n",
      "11/174, train_loss: 18.5288, time taken: 1.688871145248413s\n",
      "12/174, train_loss: 12.6347, time taken: 1.715977430343628s\n",
      "13/174, train_loss: 17.7498, time taken: 1.7717609405517578s\n",
      "14/174, train_loss: 17.9661, time taken: 1.7217278480529785s\n",
      "15/174, train_loss: 16.6141, time taken: 2.0735855102539062s\n",
      "16/174, train_loss: 13.8634, time taken: 2.005189895629883s\n",
      "17/174, train_loss: 13.3199, time taken: 1.8138716220855713s\n",
      "18/174, train_loss: 17.9758, time taken: 1.8012919425964355s\n",
      "19/174, train_loss: 19.1122, time taken: 1.7773146629333496s\n",
      "20/174, train_loss: 16.0037, time taken: 1.9887337684631348s\n",
      "21/174, train_loss: 19.3685, time taken: 2.0024940967559814s\n",
      "22/174, train_loss: 18.5452, time taken: 1.900862693786621s\n",
      "23/174, train_loss: 19.0807, time taken: 1.7941749095916748s\n",
      "24/174, train_loss: 21.0389, time taken: 2.205867052078247s\n",
      "25/174, train_loss: 16.8571, time taken: 1.8706321716308594s\n",
      "26/174, train_loss: 15.4889, time taken: 1.8128325939178467s\n",
      "27/174, train_loss: 13.9578, time taken: 1.9848170280456543s\n",
      "28/174, train_loss: 18.4957, time taken: 1.797135591506958s\n",
      "29/174, train_loss: 15.7767, time taken: 1.8107082843780518s\n",
      "30/174, train_loss: 16.0365, time taken: 1.7766830921173096s\n",
      "31/174, train_loss: 16.7752, time taken: 1.921518087387085s\n",
      "32/174, train_loss: 18.5616, time taken: 2.0714592933654785s\n",
      "33/174, train_loss: 20.5652, time taken: 1.8202080726623535s\n",
      "34/174, train_loss: 13.1083, time taken: 1.7838163375854492s\n",
      "35/174, train_loss: 16.7505, time taken: 1.7817983627319336s\n",
      "36/174, train_loss: 17.6241, time taken: 1.9028136730194092s\n",
      "37/174, train_loss: 18.5648, time taken: 1.9073100090026855s\n",
      "38/174, train_loss: 16.2801, time taken: 1.879800796508789s\n",
      "39/174, train_loss: 18.3637, time taken: 1.7309627532958984s\n",
      "40/174, train_loss: 19.8842, time taken: 1.875389575958252s\n",
      "41/174, train_loss: 18.2204, time taken: 1.9187359809875488s\n",
      "42/174, train_loss: 14.9970, time taken: 1.7857346534729004s\n",
      "43/174, train_loss: 17.2059, time taken: 1.8890035152435303s\n",
      "44/174, train_loss: 17.5059, time taken: 2.1129233837127686s\n",
      "45/174, train_loss: 15.2878, time taken: 2.385525941848755s\n",
      "46/174, train_loss: 16.6524, time taken: 2.00105357170105s\n",
      "47/174, train_loss: 18.8165, time taken: 1.8950698375701904s\n",
      "48/174, train_loss: 15.9359, time taken: 1.9131906032562256s\n",
      "49/174, train_loss: 17.4139, time taken: 2.3156251907348633s\n",
      "50/174, train_loss: 17.4994, time taken: 1.880516529083252s\n",
      "51/174, train_loss: 15.5809, time taken: 1.9009180068969727s\n",
      "52/174, train_loss: 18.0232, time taken: 1.809464931488037s\n",
      "53/174, train_loss: 16.7477, time taken: 2.284564256668091s\n",
      "54/174, train_loss: 16.2271, time taken: 2.096679449081421s\n",
      "55/174, train_loss: 17.7900, time taken: 1.8043334484100342s\n",
      "56/174, train_loss: 14.6506, time taken: 2.0253376960754395s\n",
      "57/174, train_loss: 17.6062, time taken: 2.0272951126098633s\n",
      "58/174, train_loss: 17.5640, time taken: 1.9088237285614014s\n",
      "59/174, train_loss: 19.0088, time taken: 1.8957219123840332s\n",
      "60/174, train_loss: 15.4965, time taken: 1.9770121574401855s\n",
      "61/174, train_loss: 11.4924, time taken: 1.80977201461792s\n",
      "62/174, train_loss: 17.2124, time taken: 1.907547950744629s\n",
      "63/174, train_loss: 18.9501, time taken: 1.9541053771972656s\n",
      "64/174, train_loss: 19.8726, time taken: 2.407958745956421s\n",
      "65/174, train_loss: 14.8356, time taken: 1.7118699550628662s\n",
      "66/174, train_loss: 16.5991, time taken: 1.8917295932769775s\n",
      "67/174, train_loss: 22.3528, time taken: 1.8027873039245605s\n",
      "68/174, train_loss: 17.0146, time taken: 1.7793323993682861s\n",
      "69/174, train_loss: 14.4298, time taken: 1.8884944915771484s\n",
      "70/174, train_loss: 18.8429, time taken: 1.9319233894348145s\n",
      "71/174, train_loss: 10.1585, time taken: 1.980271577835083s\n",
      "72/174, train_loss: 15.2655, time taken: 1.7848563194274902s\n",
      "73/174, train_loss: 13.5052, time taken: 1.8276009559631348s\n",
      "74/174, train_loss: 17.2250, time taken: 2.071998119354248s\n",
      "75/174, train_loss: 19.7247, time taken: 1.8951349258422852s\n",
      "76/174, train_loss: 17.0121, time taken: 1.8091745376586914s\n",
      "77/174, train_loss: 17.9995, time taken: 1.9972987174987793s\n",
      "78/174, train_loss: 15.9238, time taken: 2.0697407722473145s\n",
      "79/174, train_loss: 14.7844, time taken: 1.920464038848877s\n",
      "80/174, train_loss: 21.2580, time taken: 1.8872108459472656s\n",
      "81/174, train_loss: 17.7103, time taken: 2.079343557357788s\n",
      "82/174, train_loss: 16.3398, time taken: 2.4400634765625s\n",
      "83/174, train_loss: 19.7634, time taken: 1.960075855255127s\n",
      "84/174, train_loss: 12.8868, time taken: 1.8248875141143799s\n",
      "85/174, train_loss: 16.8550, time taken: 1.8014874458312988s\n",
      "86/174, train_loss: 18.7228, time taken: 1.787839651107788s\n",
      "87/174, train_loss: 21.0755, time taken: 2.7125356197357178s\n",
      "88/174, train_loss: 19.5957, time taken: 2.0118298530578613s\n",
      "89/174, train_loss: 14.2245, time taken: 2.019663095474243s\n",
      "90/174, train_loss: 18.5020, time taken: 1.9660134315490723s\n",
      "91/174, train_loss: 15.4019, time taken: 2.0301597118377686s\n",
      "92/174, train_loss: 17.8244, time taken: 1.8851075172424316s\n",
      "93/174, train_loss: 17.9077, time taken: 1.6963841915130615s\n",
      "94/174, train_loss: 13.5842, time taken: 1.8809845447540283s\n",
      "95/174, train_loss: 18.4790, time taken: 1.9133639335632324s\n",
      "96/174, train_loss: 16.7192, time taken: 2.192355155944824s\n",
      "97/174, train_loss: 17.4157, time taken: 2.099222421646118s\n",
      "98/174, train_loss: 20.0850, time taken: 2.4093194007873535s\n",
      "99/174, train_loss: 16.5632, time taken: 2.1035256385803223s\n",
      "100/174, train_loss: 18.4238, time taken: 2.009369373321533s\n",
      "101/174, train_loss: 16.7436, time taken: 1.8129711151123047s\n",
      "102/174, train_loss: 19.8726, time taken: 2.2732250690460205s\n",
      "103/174, train_loss: 15.6544, time taken: 2.1935954093933105s\n",
      "104/174, train_loss: 17.2109, time taken: 2.092432737350464s\n",
      "105/174, train_loss: 16.9840, time taken: 1.8264923095703125s\n",
      "106/174, train_loss: 16.6424, time taken: 1.7981696128845215s\n",
      "107/174, train_loss: 13.3449, time taken: 1.785571813583374s\n",
      "108/174, train_loss: 19.0152, time taken: 1.7701780796051025s\n",
      "109/174, train_loss: 17.2604, time taken: 1.8213167190551758s\n",
      "110/174, train_loss: 17.4315, time taken: 1.7003331184387207s\n",
      "111/174, train_loss: 21.2793, time taken: 1.769040822982788s\n",
      "112/174, train_loss: 17.6882, time taken: 1.921863079071045s\n",
      "113/174, train_loss: 16.5761, time taken: 1.9865810871124268s\n",
      "114/174, train_loss: 20.3594, time taken: 1.9897351264953613s\n",
      "115/174, train_loss: 14.1157, time taken: 1.8079571723937988s\n",
      "116/174, train_loss: 15.8420, time taken: 1.9123096466064453s\n",
      "117/174, train_loss: 18.4522, time taken: 2.177354097366333s\n",
      "118/174, train_loss: 24.0340, time taken: 2.0785717964172363s\n",
      "119/174, train_loss: 19.1643, time taken: 1.9060683250427246s\n",
      "120/174, train_loss: 17.8375, time taken: 2.022801160812378s\n",
      "121/174, train_loss: 19.9644, time taken: 4.293748378753662s\n",
      "122/174, train_loss: 19.6512, time taken: 2.0036227703094482s\n",
      "123/174, train_loss: 18.8715, time taken: 2.060129404067993s\n",
      "124/174, train_loss: 17.6302, time taken: 1.922562599182129s\n",
      "125/174, train_loss: 16.4915, time taken: 2.2774569988250732s\n",
      "126/174, train_loss: 14.9485, time taken: 2.020456314086914s\n",
      "127/174, train_loss: 13.3625, time taken: 1.8807132244110107s\n",
      "128/174, train_loss: 14.9924, time taken: 2.3110361099243164s\n",
      "129/174, train_loss: 16.8164, time taken: 1.7067506313323975s\n",
      "130/174, train_loss: 15.9283, time taken: 1.7254917621612549s\n",
      "131/174, train_loss: 15.3355, time taken: 2.1018786430358887s\n",
      "132/174, train_loss: 13.7952, time taken: 1.8966176509857178s\n",
      "133/174, train_loss: 14.8411, time taken: 2.0894691944122314s\n",
      "134/174, train_loss: 18.3412, time taken: 1.7153782844543457s\n",
      "135/174, train_loss: 25.5208, time taken: 1.8532869815826416s\n",
      "136/174, train_loss: 16.2207, time taken: 1.7167589664459229s\n",
      "137/174, train_loss: 19.8478, time taken: 1.995542287826538s\n",
      "138/174, train_loss: 19.0976, time taken: 1.878617286682129s\n",
      "139/174, train_loss: 17.3434, time taken: 1.903916597366333s\n",
      "140/174, train_loss: 23.5487, time taken: 1.7175705432891846s\n",
      "141/174, train_loss: 15.5032, time taken: 1.9682013988494873s\n",
      "142/174, train_loss: 14.3536, time taken: 2.003161668777466s\n",
      "143/174, train_loss: 20.0320, time taken: 1.904181718826294s\n",
      "144/174, train_loss: 17.8515, time taken: 1.8094816207885742s\n",
      "145/174, train_loss: 19.4477, time taken: 1.7811927795410156s\n",
      "146/174, train_loss: 16.8949, time taken: 1.829296350479126s\n",
      "147/174, train_loss: 20.0970, time taken: 1.6885221004486084s\n",
      "148/174, train_loss: 14.6397, time taken: 1.7695448398590088s\n",
      "149/174, train_loss: 17.2087, time taken: 1.710869550704956s\n",
      "150/174, train_loss: 18.2660, time taken: 1.8179247379302979s\n",
      "151/174, train_loss: 13.6753, time taken: 1.8633201122283936s\n",
      "152/174, train_loss: 14.0490, time taken: 1.830493688583374s\n",
      "153/174, train_loss: 12.9021, time taken: 1.7756311893463135s\n",
      "154/174, train_loss: 21.5407, time taken: 1.7980635166168213s\n",
      "155/174, train_loss: 18.2007, time taken: 1.7755241394042969s\n",
      "156/174, train_loss: 17.1307, time taken: 1.9344189167022705s\n",
      "157/174, train_loss: 15.7961, time taken: 1.8836631774902344s\n",
      "158/174, train_loss: 16.1740, time taken: 1.8009343147277832s\n",
      "159/174, train_loss: 19.0452, time taken: 1.7806761264801025s\n",
      "160/174, train_loss: 18.3730, time taken: 3.4158689975738525s\n",
      "161/174, train_loss: 16.0918, time taken: 1.880795955657959s\n",
      "162/174, train_loss: 17.4060, time taken: 2.384937047958374s\n",
      "163/174, train_loss: 12.8138, time taken: 1.7006607055664062s\n",
      "164/174, train_loss: 15.2336, time taken: 2.108949661254883s\n",
      "165/174, train_loss: 23.2368, time taken: 2.0089573860168457s\n",
      "166/174, train_loss: 20.2843, time taken: 1.7800984382629395s\n",
      "167/174, train_loss: 21.3459, time taken: 2.0978636741638184s\n",
      "168/174, train_loss: 15.5815, time taken: 1.7806768417358398s\n",
      "169/174, train_loss: 18.4495, time taken: 1.7115726470947266s\n",
      "170/174, train_loss: 17.6583, time taken: 1.9019505977630615s\n",
      "171/174, train_loss: 14.4303, time taken: 1.6703693866729736s\n",
      "172/174, train_loss: 13.9141, time taken: 1.9175488948822021s\n",
      "173/174, train_loss: 19.0328, time taken: 1.9837372303009033s\n",
      "174/174, train_loss: 21.6071, time taken: 2.113391160964966s\n",
      "175/174, train_loss: 18.6025, time taken: 1.3925461769104004s\n",
      "epoch 28 average loss: 17.2368\n",
      "----------\n",
      "epoch 29/2000\n",
      "1/174, train_loss: 13.0126, time taken: 8.445771217346191s\n",
      "2/174, train_loss: 17.6371, time taken: 2.1219229698181152s\n",
      "3/174, train_loss: 19.7627, time taken: 2.0575451850891113s\n",
      "4/174, train_loss: 15.6409, time taken: 1.9870967864990234s\n",
      "5/174, train_loss: 17.3326, time taken: 1.8384308815002441s\n",
      "6/174, train_loss: 19.6334, time taken: 2.105504274368286s\n",
      "7/174, train_loss: 16.9374, time taken: 1.9378383159637451s\n",
      "8/174, train_loss: 13.6716, time taken: 1.7637605667114258s\n",
      "9/174, train_loss: 14.6249, time taken: 2.0001590251922607s\n",
      "10/174, train_loss: 16.8777, time taken: 2.1096549034118652s\n",
      "11/174, train_loss: 15.5473, time taken: 1.7884228229522705s\n",
      "12/174, train_loss: 13.6616, time taken: 1.7895028591156006s\n",
      "13/174, train_loss: 18.4534, time taken: 1.9212987422943115s\n",
      "14/174, train_loss: 15.5709, time taken: 2.031268835067749s\n",
      "15/174, train_loss: 19.3665, time taken: 1.8770556449890137s\n",
      "16/174, train_loss: 18.9138, time taken: 1.7874698638916016s\n",
      "17/174, train_loss: 16.0876, time taken: 1.7073121070861816s\n",
      "18/174, train_loss: 15.8746, time taken: 2.0087897777557373s\n",
      "19/174, train_loss: 17.3783, time taken: 2.096310615539551s\n",
      "20/174, train_loss: 19.5267, time taken: 1.7865755558013916s\n",
      "21/174, train_loss: 15.6721, time taken: 1.7829878330230713s\n",
      "22/174, train_loss: 20.3686, time taken: 3.6970582008361816s\n",
      "23/174, train_loss: 17.5220, time taken: 1.9261860847473145s\n",
      "24/174, train_loss: 17.2251, time taken: 1.7802834510803223s\n",
      "25/174, train_loss: 14.4849, time taken: 1.792924165725708s\n",
      "26/174, train_loss: 18.2085, time taken: 1.8746318817138672s\n",
      "27/174, train_loss: 12.7621, time taken: 2.2102222442626953s\n",
      "28/174, train_loss: 16.7597, time taken: 1.9256293773651123s\n",
      "29/174, train_loss: 14.6064, time taken: 2.00451922416687s\n",
      "30/174, train_loss: 17.4573, time taken: 2.2031421661376953s\n",
      "31/174, train_loss: 16.6959, time taken: 1.9219987392425537s\n",
      "32/174, train_loss: 12.6915, time taken: 1.9871819019317627s\n",
      "33/174, train_loss: 15.2404, time taken: 1.881563663482666s\n",
      "34/174, train_loss: 15.6414, time taken: 1.798841953277588s\n",
      "35/174, train_loss: 17.7953, time taken: 2.0981662273406982s\n",
      "36/174, train_loss: 22.4226, time taken: 1.9010517597198486s\n",
      "37/174, train_loss: 21.1219, time taken: 1.873697280883789s\n",
      "38/174, train_loss: 18.6134, time taken: 1.811631679534912s\n",
      "39/174, train_loss: 17.1772, time taken: 1.7983992099761963s\n",
      "40/174, train_loss: 17.8879, time taken: 1.9813034534454346s\n",
      "41/174, train_loss: 13.5687, time taken: 1.921644687652588s\n",
      "42/174, train_loss: 17.7868, time taken: 1.7998600006103516s\n",
      "43/174, train_loss: 15.8657, time taken: 1.7857487201690674s\n",
      "44/174, train_loss: 20.2634, time taken: 1.7870538234710693s\n",
      "45/174, train_loss: 13.3720, time taken: 2.023650884628296s\n",
      "46/174, train_loss: 21.5335, time taken: 1.774468183517456s\n",
      "47/174, train_loss: 15.9405, time taken: 1.873091220855713s\n",
      "48/174, train_loss: 12.2585, time taken: 2.118372917175293s\n",
      "49/174, train_loss: 16.0993, time taken: 1.792226791381836s\n",
      "50/174, train_loss: 21.0662, time taken: 1.885143518447876s\n",
      "51/174, train_loss: 15.4685, time taken: 1.9033255577087402s\n",
      "52/174, train_loss: 20.1830, time taken: 1.882246494293213s\n",
      "53/174, train_loss: 16.1351, time taken: 2.2083826065063477s\n",
      "54/174, train_loss: 11.4887, time taken: 1.8209667205810547s\n",
      "55/174, train_loss: 15.8055, time taken: 1.7911043167114258s\n",
      "56/174, train_loss: 16.9769, time taken: 2.166459083557129s\n",
      "57/174, train_loss: 17.9263, time taken: 1.9158930778503418s\n",
      "58/174, train_loss: 18.7884, time taken: 1.776174545288086s\n",
      "59/174, train_loss: 17.5692, time taken: 1.7144482135772705s\n",
      "60/174, train_loss: 11.7897, time taken: 1.8755919933319092s\n",
      "61/174, train_loss: 19.1654, time taken: 1.713625192642212s\n",
      "62/174, train_loss: 16.0540, time taken: 1.68288254737854s\n",
      "63/174, train_loss: 18.7663, time taken: 1.9141302108764648s\n",
      "64/174, train_loss: 18.3694, time taken: 1.986539363861084s\n",
      "65/174, train_loss: 20.5365, time taken: 1.9058475494384766s\n",
      "66/174, train_loss: 19.4001, time taken: 1.911811113357544s\n",
      "67/174, train_loss: 14.0509, time taken: 1.8940644264221191s\n",
      "68/174, train_loss: 16.8009, time taken: 1.975433349609375s\n",
      "69/174, train_loss: 15.8704, time taken: 1.714177131652832s\n",
      "70/174, train_loss: 14.7504, time taken: 2.0748276710510254s\n",
      "71/174, train_loss: 17.0854, time taken: 1.8874051570892334s\n",
      "72/174, train_loss: 19.9207, time taken: 2.0392050743103027s\n",
      "73/174, train_loss: 16.5454, time taken: 1.8602991104125977s\n",
      "74/174, train_loss: 17.4220, time taken: 1.806764841079712s\n",
      "75/174, train_loss: 18.8914, time taken: 1.6978859901428223s\n",
      "76/174, train_loss: 12.5232, time taken: 1.7842340469360352s\n",
      "77/174, train_loss: 17.4776, time taken: 1.8898677825927734s\n",
      "78/174, train_loss: 17.4423, time taken: 3.592752456665039s\n",
      "79/174, train_loss: 19.1182, time taken: 1.716684341430664s\n",
      "80/174, train_loss: 17.9270, time taken: 2.10927414894104s\n",
      "81/174, train_loss: 17.4494, time taken: 2.1754727363586426s\n",
      "82/174, train_loss: 13.5603, time taken: 1.9031364917755127s\n",
      "83/174, train_loss: 20.0980, time taken: 1.9189808368682861s\n",
      "84/174, train_loss: 12.7465, time taken: 1.8335027694702148s\n",
      "85/174, train_loss: 15.4407, time taken: 1.8812015056610107s\n",
      "86/174, train_loss: 17.1591, time taken: 1.8140182495117188s\n",
      "87/174, train_loss: 17.4639, time taken: 2.2681121826171875s\n",
      "88/174, train_loss: 14.3580, time taken: 1.9257659912109375s\n",
      "89/174, train_loss: 17.0323, time taken: 1.8637418746948242s\n",
      "90/174, train_loss: 16.4434, time taken: 1.9258534908294678s\n",
      "91/174, train_loss: 19.7301, time taken: 1.8765618801116943s\n",
      "92/174, train_loss: 19.1014, time taken: 1.7243359088897705s\n",
      "93/174, train_loss: 15.1306, time taken: 1.7998929023742676s\n",
      "94/174, train_loss: 15.8084, time taken: 1.7709009647369385s\n",
      "95/174, train_loss: 18.1157, time taken: 1.8037188053131104s\n",
      "96/174, train_loss: 14.7375, time taken: 1.8067419528961182s\n",
      "97/174, train_loss: 15.1561, time taken: 2.0998542308807373s\n",
      "98/174, train_loss: 15.8336, time taken: 1.7786052227020264s\n",
      "99/174, train_loss: 19.2342, time taken: 2.179276943206787s\n",
      "100/174, train_loss: 16.3840, time taken: 2.010913610458374s\n",
      "101/174, train_loss: 18.8550, time taken: 1.817765712738037s\n",
      "102/174, train_loss: 19.5173, time taken: 1.947577714920044s\n",
      "103/174, train_loss: 15.9379, time taken: 1.6320219039916992s\n",
      "104/174, train_loss: 14.5758, time taken: 1.7772116661071777s\n",
      "105/174, train_loss: 21.1709, time taken: 1.9208078384399414s\n",
      "106/174, train_loss: 14.5369, time taken: 2.0319840908050537s\n",
      "107/174, train_loss: 18.6343, time taken: 1.8850979804992676s\n",
      "108/174, train_loss: 16.7601, time taken: 1.774219274520874s\n",
      "109/174, train_loss: 18.1774, time taken: 1.8242604732513428s\n",
      "110/174, train_loss: 15.8643, time taken: 1.77907395362854s\n",
      "111/174, train_loss: 17.9510, time taken: 1.8030667304992676s\n",
      "112/174, train_loss: 17.5584, time taken: 1.7795567512512207s\n",
      "113/174, train_loss: 14.6675, time taken: 1.8228254318237305s\n",
      "114/174, train_loss: 16.4666, time taken: 1.7826087474822998s\n",
      "115/174, train_loss: 19.9038, time taken: 1.9273512363433838s\n",
      "116/174, train_loss: 19.6684, time taken: 1.8209612369537354s\n",
      "117/174, train_loss: 16.7872, time taken: 1.774780511856079s\n",
      "118/174, train_loss: 17.6751, time taken: 1.7297499179840088s\n",
      "119/174, train_loss: 18.1048, time taken: 1.7639493942260742s\n",
      "120/174, train_loss: 20.0385, time taken: 3.5311851501464844s\n",
      "121/174, train_loss: 17.5862, time taken: 1.7666645050048828s\n",
      "122/174, train_loss: 13.7161, time taken: 1.624542474746704s\n",
      "123/174, train_loss: 22.6561, time taken: 1.705796241760254s\n",
      "124/174, train_loss: 15.5564, time taken: 1.8081519603729248s\n",
      "125/174, train_loss: 15.0848, time taken: 1.9302926063537598s\n",
      "126/174, train_loss: 13.5876, time taken: 1.7590162754058838s\n",
      "127/174, train_loss: 19.2681, time taken: 1.7077734470367432s\n",
      "128/174, train_loss: 17.1299, time taken: 1.9131474494934082s\n",
      "129/174, train_loss: 16.8845, time taken: 1.7784581184387207s\n",
      "130/174, train_loss: 18.0859, time taken: 1.9029991626739502s\n",
      "131/174, train_loss: 18.9882, time taken: 1.7819278240203857s\n",
      "132/174, train_loss: 18.1605, time taken: 1.7861511707305908s\n",
      "133/174, train_loss: 15.8665, time taken: 2.29709792137146s\n",
      "134/174, train_loss: 13.5554, time taken: 1.8316688537597656s\n",
      "135/174, train_loss: 13.7536, time taken: 1.7767791748046875s\n",
      "136/174, train_loss: 15.3689, time taken: 2.0831761360168457s\n",
      "137/174, train_loss: 18.0902, time taken: 1.8256521224975586s\n",
      "138/174, train_loss: 15.6838, time taken: 1.7911722660064697s\n",
      "139/174, train_loss: 18.4243, time taken: 2.105224370956421s\n",
      "140/174, train_loss: 15.8730, time taken: 1.9589502811431885s\n",
      "141/174, train_loss: 15.5548, time taken: 1.7033238410949707s\n",
      "142/174, train_loss: 20.9944, time taken: 1.912390947341919s\n",
      "143/174, train_loss: 16.6594, time taken: 2.2957537174224854s\n",
      "144/174, train_loss: 17.1202, time taken: 2.316807985305786s\n",
      "145/174, train_loss: 15.8139, time taken: 2.0009939670562744s\n",
      "146/174, train_loss: 17.8172, time taken: 1.80098557472229s\n",
      "147/174, train_loss: 12.8847, time taken: 1.820054292678833s\n",
      "148/174, train_loss: 18.6774, time taken: 1.8069398403167725s\n",
      "149/174, train_loss: 14.0911, time taken: 2.052992582321167s\n",
      "150/174, train_loss: 18.9959, time taken: 1.742798089981079s\n",
      "151/174, train_loss: 14.6799, time taken: 1.8328492641448975s\n",
      "152/174, train_loss: 17.3308, time taken: 1.7041418552398682s\n",
      "153/174, train_loss: 20.4950, time taken: 1.6989881992340088s\n",
      "154/174, train_loss: 13.6465, time taken: 1.7129080295562744s\n",
      "155/174, train_loss: 17.2951, time taken: 1.9051523208618164s\n",
      "156/174, train_loss: 19.2773, time taken: 1.9984385967254639s\n",
      "157/174, train_loss: 12.5025, time taken: 1.9820003509521484s\n",
      "158/174, train_loss: 14.1215, time taken: 2.000898599624634s\n",
      "159/174, train_loss: 15.1503, time taken: 1.6865088939666748s\n",
      "160/174, train_loss: 16.6227, time taken: 1.7156481742858887s\n",
      "161/174, train_loss: 16.0156, time taken: 1.8889882564544678s\n",
      "162/174, train_loss: 19.9644, time taken: 1.9017572402954102s\n",
      "163/174, train_loss: 17.1120, time taken: 1.79331636428833s\n",
      "164/174, train_loss: 18.4649, time taken: 1.8845677375793457s\n",
      "165/174, train_loss: 19.2044, time taken: 1.7178175449371338s\n",
      "166/174, train_loss: 14.6277, time taken: 1.7774620056152344s\n",
      "167/174, train_loss: 13.8613, time taken: 1.8321995735168457s\n",
      "168/174, train_loss: 15.1897, time taken: 1.846304178237915s\n",
      "169/174, train_loss: 18.4118, time taken: 1.7285401821136475s\n",
      "170/174, train_loss: 18.7807, time taken: 1.7831530570983887s\n",
      "171/174, train_loss: 19.7302, time taken: 1.7979795932769775s\n",
      "172/174, train_loss: 16.5422, time taken: 1.7110998630523682s\n",
      "173/174, train_loss: 17.1767, time taken: 1.8813655376434326s\n",
      "174/174, train_loss: 17.1715, time taken: 2.001354217529297s\n",
      "175/174, train_loss: 16.3891, time taken: 1.4067714214324951s\n",
      "epoch 29 average loss: 16.9108\n",
      "Entering Validation for epoch: 29\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 29 Validation avg loss: 10.9938, time taken: 0.9876470565795898s\n",
      "----------\n",
      "epoch 30/2000\n",
      "1/174, train_loss: 16.3219, time taken: 8.902203798294067s\n",
      "2/174, train_loss: 17.9999, time taken: 2.194270372390747s\n",
      "3/174, train_loss: 15.3939, time taken: 2.096773147583008s\n",
      "4/174, train_loss: 16.8854, time taken: 1.890925407409668s\n",
      "5/174, train_loss: 17.0235, time taken: 2.1150619983673096s\n",
      "6/174, train_loss: 16.8218, time taken: 2.3678276538848877s\n",
      "7/174, train_loss: 20.0015, time taken: 1.8261964321136475s\n",
      "8/174, train_loss: 16.8266, time taken: 2.3196768760681152s\n",
      "9/174, train_loss: 15.8246, time taken: 1.7048523426055908s\n",
      "10/174, train_loss: 18.0681, time taken: 1.7852978706359863s\n",
      "11/174, train_loss: 20.0377, time taken: 1.7806897163391113s\n",
      "12/174, train_loss: 19.4995, time taken: 1.9303603172302246s\n",
      "13/174, train_loss: 19.0161, time taken: 1.8636977672576904s\n",
      "14/174, train_loss: 22.8485, time taken: 1.7323198318481445s\n",
      "15/174, train_loss: 14.0632, time taken: 1.975062608718872s\n",
      "16/174, train_loss: 19.0267, time taken: 1.810542345046997s\n",
      "17/174, train_loss: 15.7199, time taken: 1.8756334781646729s\n",
      "18/174, train_loss: 21.2184, time taken: 1.9088339805603027s\n",
      "19/174, train_loss: 18.1005, time taken: 1.7117419242858887s\n",
      "20/174, train_loss: 13.6555, time taken: 2.1864876747131348s\n",
      "21/174, train_loss: 15.8087, time taken: 1.7031872272491455s\n",
      "22/174, train_loss: 14.6494, time taken: 1.7851524353027344s\n",
      "23/174, train_loss: 19.5135, time taken: 2.0031545162200928s\n",
      "24/174, train_loss: 18.4369, time taken: 1.7065176963806152s\n",
      "25/174, train_loss: 19.0346, time taken: 1.9154691696166992s\n",
      "26/174, train_loss: 15.2011, time taken: 1.9899487495422363s\n",
      "27/174, train_loss: 13.4133, time taken: 1.895148515701294s\n",
      "28/174, train_loss: 19.3550, time taken: 1.9213612079620361s\n",
      "29/174, train_loss: 20.6115, time taken: 1.9672114849090576s\n",
      "30/174, train_loss: 17.3955, time taken: 1.9294910430908203s\n",
      "31/174, train_loss: 19.3783, time taken: 1.9624152183532715s\n",
      "32/174, train_loss: 16.5856, time taken: 3.592477321624756s\n",
      "33/174, train_loss: 20.7813, time taken: 1.8075921535491943s\n",
      "34/174, train_loss: 23.6884, time taken: 1.9113526344299316s\n",
      "35/174, train_loss: 20.9373, time taken: 2.002807855606079s\n",
      "36/174, train_loss: 16.6467, time taken: 1.973787784576416s\n",
      "37/174, train_loss: 14.3116, time taken: 1.8168466091156006s\n",
      "38/174, train_loss: 15.6757, time taken: 1.7948732376098633s\n",
      "39/174, train_loss: 17.6511, time taken: 2.0959153175354004s\n",
      "40/174, train_loss: 15.0922, time taken: 1.8668646812438965s\n",
      "41/174, train_loss: 15.6080, time taken: 1.8996989727020264s\n",
      "42/174, train_loss: 15.0540, time taken: 1.820028305053711s\n",
      "43/174, train_loss: 14.4260, time taken: 2.09552264213562s\n",
      "44/174, train_loss: 15.0862, time taken: 1.9007630348205566s\n",
      "45/174, train_loss: 15.0607, time taken: 1.9638898372650146s\n",
      "46/174, train_loss: 17.8520, time taken: 1.7306239604949951s\n",
      "47/174, train_loss: 20.1795, time taken: 1.7985858917236328s\n",
      "48/174, train_loss: 16.2927, time taken: 1.8004093170166016s\n",
      "49/174, train_loss: 20.0700, time taken: 1.7221174240112305s\n",
      "50/174, train_loss: 15.1341, time taken: 1.7014563083648682s\n",
      "51/174, train_loss: 25.1055, time taken: 1.702930212020874s\n",
      "52/174, train_loss: 17.3158, time taken: 1.802008867263794s\n",
      "53/174, train_loss: 19.8077, time taken: 1.746805191040039s\n",
      "54/174, train_loss: 16.4222, time taken: 1.9225282669067383s\n",
      "55/174, train_loss: 13.3310, time taken: 1.8851861953735352s\n",
      "56/174, train_loss: 18.6299, time taken: 1.7851710319519043s\n",
      "57/174, train_loss: 19.4097, time taken: 2.004551649093628s\n",
      "58/174, train_loss: 19.9737, time taken: 1.80452299118042s\n",
      "59/174, train_loss: 14.7181, time taken: 1.7216708660125732s\n",
      "60/174, train_loss: 12.8617, time taken: 1.9930002689361572s\n",
      "61/174, train_loss: 19.4704, time taken: 1.8909101486206055s\n",
      "62/174, train_loss: 18.8135, time taken: 1.9736628532409668s\n",
      "63/174, train_loss: 18.2619, time taken: 1.7080023288726807s\n",
      "64/174, train_loss: 15.8127, time taken: 1.80184006690979s\n",
      "65/174, train_loss: 14.6394, time taken: 1.8777496814727783s\n",
      "66/174, train_loss: 15.8655, time taken: 1.7892282009124756s\n",
      "67/174, train_loss: 17.2878, time taken: 1.716193675994873s\n",
      "68/174, train_loss: 16.2209, time taken: 1.892343282699585s\n",
      "69/174, train_loss: 24.6263, time taken: 1.9146182537078857s\n",
      "70/174, train_loss: 17.8679, time taken: 1.9696743488311768s\n",
      "71/174, train_loss: 15.3196, time taken: 1.918874979019165s\n",
      "72/174, train_loss: 22.7509, time taken: 1.9623100757598877s\n",
      "73/174, train_loss: 11.9911, time taken: 1.830812931060791s\n",
      "74/174, train_loss: 14.4444, time taken: 1.9920530319213867s\n",
      "75/174, train_loss: 15.8501, time taken: 1.8986701965332031s\n",
      "76/174, train_loss: 16.3633, time taken: 2.007741928100586s\n",
      "77/174, train_loss: 16.4071, time taken: 1.8512723445892334s\n",
      "78/174, train_loss: 15.9371, time taken: 1.7177863121032715s\n",
      "79/174, train_loss: 14.5128, time taken: 1.9969251155853271s\n",
      "80/174, train_loss: 17.9678, time taken: 2.1806225776672363s\n",
      "81/174, train_loss: 17.5835, time taken: 1.7159075736999512s\n",
      "82/174, train_loss: 17.7402, time taken: 2.0049045085906982s\n",
      "83/174, train_loss: 19.5077, time taken: 2.0988550186157227s\n",
      "84/174, train_loss: 20.1108, time taken: 2.0923352241516113s\n",
      "85/174, train_loss: 16.1922, time taken: 2.0803167819976807s\n",
      "86/174, train_loss: 20.3501, time taken: 1.7959659099578857s\n",
      "87/174, train_loss: 14.1065, time taken: 1.792572259902954s\n",
      "88/174, train_loss: 16.5994, time taken: 1.8989760875701904s\n",
      "89/174, train_loss: 11.7906, time taken: 4.085758209228516s\n",
      "90/174, train_loss: 17.7911, time taken: 2.192429780960083s\n",
      "91/174, train_loss: 18.9869, time taken: 2.114424467086792s\n",
      "92/174, train_loss: 18.3641, time taken: 1.977006435394287s\n",
      "93/174, train_loss: 16.9577, time taken: 1.7102501392364502s\n",
      "94/174, train_loss: 17.1929, time taken: 2.218784809112549s\n",
      "95/174, train_loss: 18.0680, time taken: 2.067408800125122s\n",
      "96/174, train_loss: 18.4166, time taken: 1.89349365234375s\n",
      "97/174, train_loss: 16.9376, time taken: 1.832273006439209s\n",
      "98/174, train_loss: 18.8338, time taken: 1.8247361183166504s\n",
      "99/174, train_loss: 15.4935, time taken: 1.7824859619140625s\n",
      "100/174, train_loss: 18.9475, time taken: 1.8820691108703613s\n",
      "101/174, train_loss: 17.3740, time taken: 2.1050238609313965s\n",
      "102/174, train_loss: 17.1344, time taken: 2.2084531784057617s\n",
      "103/174, train_loss: 16.1058, time taken: 2.187183380126953s\n",
      "104/174, train_loss: 17.0481, time taken: 1.8248317241668701s\n",
      "105/174, train_loss: 17.8480, time taken: 2.4738190174102783s\n",
      "106/174, train_loss: 21.3195, time taken: 1.9935967922210693s\n",
      "107/174, train_loss: 20.3505, time taken: 1.8181791305541992s\n",
      "108/174, train_loss: 21.1751, time taken: 1.8982560634613037s\n",
      "109/174, train_loss: 16.6108, time taken: 2.1640994548797607s\n",
      "110/174, train_loss: 16.9378, time taken: 2.004924774169922s\n",
      "111/174, train_loss: 12.9563, time taken: 1.8943884372711182s\n",
      "112/174, train_loss: 16.4863, time taken: 2.0089073181152344s\n",
      "113/174, train_loss: 16.5512, time taken: 2.003129005432129s\n",
      "114/174, train_loss: 16.1604, time taken: 1.8041861057281494s\n",
      "115/174, train_loss: 18.7918, time taken: 2.053624153137207s\n",
      "116/174, train_loss: 19.5339, time taken: 1.7141010761260986s\n",
      "117/174, train_loss: 17.1090, time taken: 1.803128957748413s\n",
      "118/174, train_loss: 15.0833, time taken: 1.799701452255249s\n",
      "119/174, train_loss: 17.7797, time taken: 1.7989304065704346s\n",
      "120/174, train_loss: 17.2083, time taken: 2.0865180492401123s\n",
      "121/174, train_loss: 18.0534, time taken: 1.7166328430175781s\n",
      "122/174, train_loss: 15.7880, time taken: 1.7763183116912842s\n",
      "123/174, train_loss: 16.0539, time taken: 1.7227203845977783s\n",
      "124/174, train_loss: 15.0706, time taken: 1.7288832664489746s\n",
      "125/174, train_loss: 17.1370, time taken: 1.7959480285644531s\n",
      "126/174, train_loss: 16.8087, time taken: 1.8096950054168701s\n",
      "127/174, train_loss: 15.5134, time taken: 1.9658434391021729s\n",
      "128/174, train_loss: 16.7583, time taken: 3.6021320819854736s\n",
      "129/174, train_loss: 13.4083, time taken: 1.6806344985961914s\n",
      "130/174, train_loss: 15.7884, time taken: 1.9060428142547607s\n",
      "131/174, train_loss: 14.9004, time taken: 1.9824700355529785s\n",
      "132/174, train_loss: 14.1673, time taken: 1.8188366889953613s\n",
      "133/174, train_loss: 16.3406, time taken: 2.0858190059661865s\n",
      "134/174, train_loss: 19.4120, time taken: 1.8908414840698242s\n",
      "135/174, train_loss: 12.0361, time taken: 1.8020551204681396s\n",
      "136/174, train_loss: 17.8240, time taken: 1.7113873958587646s\n",
      "137/174, train_loss: 15.5398, time taken: 1.889261245727539s\n",
      "138/174, train_loss: 19.7688, time taken: 1.985201358795166s\n",
      "139/174, train_loss: 21.1275, time taken: 1.715440273284912s\n",
      "140/174, train_loss: 15.3151, time taken: 2.3881900310516357s\n",
      "141/174, train_loss: 19.0271, time taken: 1.8987081050872803s\n",
      "142/174, train_loss: 15.8616, time taken: 1.779038667678833s\n",
      "143/174, train_loss: 16.8092, time taken: 1.836069107055664s\n",
      "144/174, train_loss: 17.3596, time taken: 2.1539723873138428s\n",
      "145/174, train_loss: 16.6241, time taken: 1.7979166507720947s\n",
      "146/174, train_loss: 17.1317, time taken: 1.8914084434509277s\n",
      "147/174, train_loss: 20.9275, time taken: 1.7311742305755615s\n",
      "148/174, train_loss: 17.2502, time taken: 1.8768937587738037s\n",
      "149/174, train_loss: 14.1189, time taken: 1.7781729698181152s\n",
      "150/174, train_loss: 17.3193, time taken: 1.7194428443908691s\n",
      "151/174, train_loss: 14.1142, time taken: 1.6897053718566895s\n",
      "152/174, train_loss: 15.8913, time taken: 1.7015080451965332s\n",
      "153/174, train_loss: 22.7200, time taken: 1.691906213760376s\n",
      "154/174, train_loss: 23.6079, time taken: 1.6848084926605225s\n",
      "155/174, train_loss: 21.0692, time taken: 1.7305529117584229s\n",
      "156/174, train_loss: 18.0447, time taken: 2.0981876850128174s\n",
      "157/174, train_loss: 16.3608, time taken: 1.8346984386444092s\n",
      "158/174, train_loss: 14.4707, time taken: 1.7905187606811523s\n",
      "159/174, train_loss: 15.9689, time taken: 1.7690682411193848s\n",
      "160/174, train_loss: 14.1027, time taken: 1.816427230834961s\n",
      "161/174, train_loss: 19.2598, time taken: 1.9112558364868164s\n",
      "162/174, train_loss: 13.4789, time taken: 1.710655689239502s\n",
      "163/174, train_loss: 16.7547, time taken: 1.707538366317749s\n",
      "164/174, train_loss: 20.3862, time taken: 3.4878575801849365s\n",
      "165/174, train_loss: 18.0264, time taken: 1.800095558166504s\n",
      "166/174, train_loss: 20.7903, time taken: 1.91499662399292s\n",
      "167/174, train_loss: 18.1312, time taken: 1.8953571319580078s\n",
      "168/174, train_loss: 17.7726, time taken: 1.7059955596923828s\n",
      "169/174, train_loss: 14.6112, time taken: 1.8945729732513428s\n",
      "170/174, train_loss: 18.4621, time taken: 1.6899819374084473s\n",
      "171/174, train_loss: 13.0080, time taken: 1.7941913604736328s\n",
      "172/174, train_loss: 9.8259, time taken: 1.97226881980896s\n",
      "173/174, train_loss: 14.3800, time taken: 1.9053311347961426s\n",
      "174/174, train_loss: 15.5458, time taken: 1.7763981819152832s\n",
      "175/174, train_loss: 14.3850, time taken: 1.8023486137390137s\n",
      "epoch 30 average loss: 17.2069\n",
      "----------\n",
      "epoch 31/2000\n",
      "1/174, train_loss: 16.4567, time taken: 8.332561492919922s\n",
      "2/174, train_loss: 18.8958, time taken: 2.384079694747925s\n",
      "3/174, train_loss: 16.4449, time taken: 1.8234460353851318s\n",
      "4/174, train_loss: 18.7629, time taken: 2.0810179710388184s\n",
      "5/174, train_loss: 18.6297, time taken: 1.7771377563476562s\n",
      "6/174, train_loss: 18.4318, time taken: 1.7152462005615234s\n",
      "7/174, train_loss: 19.7354, time taken: 1.6929998397827148s\n",
      "8/174, train_loss: 13.2752, time taken: 1.8091223239898682s\n",
      "9/174, train_loss: 22.0296, time taken: 1.7087178230285645s\n",
      "10/174, train_loss: 17.9743, time taken: 1.896526575088501s\n",
      "11/174, train_loss: 18.6746, time taken: 1.7911953926086426s\n",
      "12/174, train_loss: 18.8648, time taken: 1.7808868885040283s\n",
      "13/174, train_loss: 18.5702, time taken: 1.7061262130737305s\n",
      "14/174, train_loss: 15.0272, time taken: 1.7864694595336914s\n",
      "15/174, train_loss: 22.8062, time taken: 2.0166730880737305s\n",
      "16/174, train_loss: 16.1217, time taken: 1.8857955932617188s\n",
      "17/174, train_loss: 12.9229, time taken: 1.981602430343628s\n",
      "18/174, train_loss: 19.6882, time taken: 1.815915822982788s\n",
      "19/174, train_loss: 14.3178, time taken: 1.784245252609253s\n",
      "20/174, train_loss: 15.9720, time taken: 2.0203866958618164s\n",
      "21/174, train_loss: 24.0037, time taken: 1.8002188205718994s\n",
      "22/174, train_loss: 17.9769, time taken: 1.7210345268249512s\n",
      "23/174, train_loss: 16.0042, time taken: 1.598691701889038s\n",
      "24/174, train_loss: 15.9162, time taken: 1.9737370014190674s\n",
      "25/174, train_loss: 17.1159, time taken: 1.8991518020629883s\n",
      "26/174, train_loss: 19.6659, time taken: 1.900707483291626s\n",
      "27/174, train_loss: 12.6438, time taken: 1.7067921161651611s\n",
      "28/174, train_loss: 12.9481, time taken: 1.7895770072937012s\n",
      "29/174, train_loss: 14.7155, time taken: 1.7139945030212402s\n",
      "30/174, train_loss: 15.8793, time taken: 2.468085765838623s\n",
      "31/174, train_loss: 14.0769, time taken: 2.0018656253814697s\n",
      "32/174, train_loss: 17.9173, time taken: 2.0884294509887695s\n",
      "33/174, train_loss: 15.9809, time taken: 2.414994478225708s\n",
      "34/174, train_loss: 13.3094, time taken: 1.896648645401001s\n",
      "35/174, train_loss: 16.3915, time taken: 1.7700116634368896s\n",
      "36/174, train_loss: 16.7601, time taken: 2.019897699356079s\n",
      "37/174, train_loss: 14.8746, time taken: 1.900315284729004s\n",
      "38/174, train_loss: 17.4423, time taken: 1.6908326148986816s\n",
      "39/174, train_loss: 15.3265, time taken: 1.7981846332550049s\n",
      "40/174, train_loss: 13.3973, time taken: 2.0075814723968506s\n",
      "41/174, train_loss: 22.9528, time taken: 1.8378314971923828s\n",
      "42/174, train_loss: 19.0236, time taken: 1.8780224323272705s\n",
      "43/174, train_loss: 18.1767, time taken: 1.884547472000122s\n",
      "44/174, train_loss: 21.0195, time taken: 1.8045780658721924s\n",
      "45/174, train_loss: 16.4416, time taken: 1.789445400238037s\n",
      "46/174, train_loss: 18.9238, time taken: 1.7173664569854736s\n",
      "47/174, train_loss: 18.2971, time taken: 1.7637948989868164s\n",
      "48/174, train_loss: 14.3398, time taken: 1.7176461219787598s\n",
      "49/174, train_loss: 17.1363, time taken: 1.7977137565612793s\n",
      "50/174, train_loss: 16.4665, time taken: 1.7840805053710938s\n",
      "51/174, train_loss: 18.5198, time taken: 1.6296634674072266s\n",
      "52/174, train_loss: 18.4903, time taken: 1.7860491275787354s\n",
      "53/174, train_loss: 22.9645, time taken: 1.9964382648468018s\n",
      "54/174, train_loss: 15.8846, time taken: 1.7833056449890137s\n",
      "55/174, train_loss: 17.9908, time taken: 1.8081893920898438s\n",
      "56/174, train_loss: 18.7996, time taken: 2.083667516708374s\n",
      "57/174, train_loss: 15.8603, time taken: 1.7963907718658447s\n",
      "58/174, train_loss: 18.1160, time taken: 1.8753690719604492s\n",
      "59/174, train_loss: 18.2750, time taken: 2.1128082275390625s\n",
      "60/174, train_loss: 19.3182, time taken: 2.0192201137542725s\n",
      "61/174, train_loss: 19.2984, time taken: 1.794071912765503s\n",
      "62/174, train_loss: 13.8632, time taken: 1.8821978569030762s\n",
      "63/174, train_loss: 16.9065, time taken: 3.4818098545074463s\n",
      "64/174, train_loss: 11.7414, time taken: 1.6251158714294434s\n",
      "65/174, train_loss: 15.5638, time taken: 2.094475507736206s\n",
      "66/174, train_loss: 19.3460, time taken: 2.3897128105163574s\n",
      "67/174, train_loss: 17.4252, time taken: 2.0121216773986816s\n",
      "68/174, train_loss: 16.5593, time taken: 1.957517385482788s\n",
      "69/174, train_loss: 22.8796, time taken: 2.0198590755462646s\n",
      "70/174, train_loss: 14.4319, time taken: 1.7982652187347412s\n",
      "71/174, train_loss: 18.4850, time taken: 1.7059721946716309s\n",
      "72/174, train_loss: 19.9543, time taken: 1.9660861492156982s\n",
      "73/174, train_loss: 15.2346, time taken: 2.2892403602600098s\n",
      "74/174, train_loss: 10.9992, time taken: 2.2128520011901855s\n",
      "75/174, train_loss: 14.3365, time taken: 1.8983185291290283s\n",
      "76/174, train_loss: 15.9191, time taken: 2.3079824447631836s\n",
      "77/174, train_loss: 16.1950, time taken: 2.0803325176239014s\n",
      "78/174, train_loss: 18.2785, time taken: 1.8774354457855225s\n",
      "79/174, train_loss: 17.7537, time taken: 1.808488368988037s\n",
      "80/174, train_loss: 16.6971, time taken: 1.779709815979004s\n",
      "81/174, train_loss: 18.7958, time taken: 1.9262876510620117s\n",
      "82/174, train_loss: 16.8851, time taken: 2.166738748550415s\n",
      "83/174, train_loss: 16.8653, time taken: 1.9026942253112793s\n",
      "84/174, train_loss: 18.6425, time taken: 1.9903523921966553s\n",
      "85/174, train_loss: 18.8179, time taken: 1.7001926898956299s\n",
      "86/174, train_loss: 20.4178, time taken: 1.9157538414001465s\n",
      "87/174, train_loss: 16.7280, time taken: 2.0004844665527344s\n",
      "88/174, train_loss: 15.9102, time taken: 1.7988388538360596s\n",
      "89/174, train_loss: 12.7361, time taken: 3.4843802452087402s\n",
      "90/174, train_loss: 17.7223, time taken: 1.8933823108673096s\n",
      "91/174, train_loss: 20.1084, time taken: 1.8047094345092773s\n",
      "92/174, train_loss: 14.8986, time taken: 1.8695147037506104s\n",
      "93/174, train_loss: 15.0712, time taken: 1.8065850734710693s\n",
      "94/174, train_loss: 15.7567, time taken: 1.9970362186431885s\n",
      "95/174, train_loss: 18.4777, time taken: 1.8067896366119385s\n",
      "96/174, train_loss: 15.0660, time taken: 1.640125036239624s\n",
      "97/174, train_loss: 15.2413, time taken: 1.7941718101501465s\n",
      "98/174, train_loss: 14.6358, time taken: 2.358077049255371s\n",
      "99/174, train_loss: 14.3224, time taken: 2.229738473892212s\n",
      "100/174, train_loss: 20.6612, time taken: 1.8730034828186035s\n",
      "101/174, train_loss: 17.0294, time taken: 2.005784749984741s\n",
      "102/174, train_loss: 15.9998, time taken: 1.9214203357696533s\n",
      "103/174, train_loss: 15.9897, time taken: 2.050954818725586s\n",
      "104/174, train_loss: 16.8757, time taken: 1.8221149444580078s\n",
      "105/174, train_loss: 15.1966, time taken: 1.8725359439849854s\n",
      "106/174, train_loss: 17.3186, time taken: 1.9258482456207275s\n",
      "107/174, train_loss: 15.2327, time taken: 2.100586414337158s\n",
      "108/174, train_loss: 20.6234, time taken: 2.0746874809265137s\n",
      "109/174, train_loss: 18.6684, time taken: 2.0264363288879395s\n",
      "110/174, train_loss: 16.6966, time taken: 1.8966116905212402s\n",
      "111/174, train_loss: 17.4090, time taken: 1.808105230331421s\n",
      "112/174, train_loss: 19.5438, time taken: 1.8185780048370361s\n",
      "113/174, train_loss: 18.5352, time taken: 1.7712504863739014s\n",
      "114/174, train_loss: 16.5698, time taken: 1.8949146270751953s\n",
      "115/174, train_loss: 18.9390, time taken: 1.916194200515747s\n",
      "116/174, train_loss: 18.1973, time taken: 1.8024468421936035s\n",
      "117/174, train_loss: 15.1981, time taken: 1.7114412784576416s\n",
      "118/174, train_loss: 15.2499, time taken: 1.80540132522583s\n",
      "119/174, train_loss: 17.1599, time taken: 1.8147828578948975s\n",
      "120/174, train_loss: 16.2244, time taken: 1.7849023342132568s\n",
      "121/174, train_loss: 20.9469, time taken: 1.8081834316253662s\n",
      "122/174, train_loss: 22.3011, time taken: 1.7987046241760254s\n",
      "123/174, train_loss: 18.3897, time taken: 1.8784089088439941s\n",
      "124/174, train_loss: 18.5428, time taken: 1.7241759300231934s\n",
      "125/174, train_loss: 11.5237, time taken: 1.7695896625518799s\n",
      "126/174, train_loss: 19.3434, time taken: 3.88775372505188s\n",
      "127/174, train_loss: 19.7064, time taken: 1.9368197917938232s\n",
      "128/174, train_loss: 17.6991, time taken: 1.9840524196624756s\n",
      "129/174, train_loss: 13.8793, time taken: 1.8669028282165527s\n",
      "130/174, train_loss: 12.2388, time taken: 2.100231170654297s\n",
      "131/174, train_loss: 16.4786, time taken: 2.003711223602295s\n",
      "132/174, train_loss: 17.5982, time taken: 1.8224327564239502s\n",
      "133/174, train_loss: 19.8053, time taken: 1.6985116004943848s\n",
      "134/174, train_loss: 14.3100, time taken: 1.8241703510284424s\n",
      "135/174, train_loss: 15.8921, time taken: 1.771188497543335s\n",
      "136/174, train_loss: 17.0743, time taken: 1.6308939456939697s\n",
      "137/174, train_loss: 17.4806, time taken: 1.760298728942871s\n",
      "138/174, train_loss: 18.3338, time taken: 1.7093820571899414s\n",
      "139/174, train_loss: 19.9569, time taken: 1.90724778175354s\n",
      "140/174, train_loss: 18.3117, time taken: 1.6693809032440186s\n",
      "141/174, train_loss: 17.5778, time taken: 2.0014488697052s\n",
      "142/174, train_loss: 15.4289, time taken: 1.8179371356964111s\n",
      "143/174, train_loss: 18.1924, time taken: 1.714756727218628s\n",
      "144/174, train_loss: 15.7261, time taken: 1.7716026306152344s\n",
      "145/174, train_loss: 18.5553, time taken: 1.7832932472229004s\n",
      "146/174, train_loss: 14.8152, time taken: 1.7983677387237549s\n",
      "147/174, train_loss: 17.3894, time taken: 2.104912042617798s\n",
      "148/174, train_loss: 14.9477, time taken: 1.8835525512695312s\n",
      "149/174, train_loss: 14.2951, time taken: 2.019012212753296s\n",
      "150/174, train_loss: 16.5019, time taken: 1.8992550373077393s\n",
      "151/174, train_loss: 17.7327, time taken: 1.9016382694244385s\n",
      "152/174, train_loss: 21.5361, time taken: 1.9018032550811768s\n",
      "153/174, train_loss: 15.5027, time taken: 2.0825858116149902s\n",
      "154/174, train_loss: 15.7888, time taken: 1.9747679233551025s\n",
      "155/174, train_loss: 15.6737, time taken: 1.9036989212036133s\n",
      "156/174, train_loss: 13.1071, time taken: 1.8070805072784424s\n",
      "157/174, train_loss: 19.3136, time taken: 1.9194979667663574s\n",
      "158/174, train_loss: 19.5354, time taken: 1.7256042957305908s\n",
      "159/174, train_loss: 19.0455, time taken: 1.9942824840545654s\n",
      "160/174, train_loss: 19.2996, time taken: 1.8992345333099365s\n",
      "161/174, train_loss: 18.4908, time taken: 1.7897067070007324s\n",
      "162/174, train_loss: 21.6611, time taken: 1.8189404010772705s\n",
      "163/174, train_loss: 14.4762, time taken: 2.0969645977020264s\n",
      "164/174, train_loss: 16.9382, time taken: 1.736445665359497s\n",
      "165/174, train_loss: 19.3601, time taken: 1.8899731636047363s\n",
      "166/174, train_loss: 18.1527, time taken: 1.8747320175170898s\n",
      "167/174, train_loss: 20.1177, time taken: 1.6993803977966309s\n",
      "168/174, train_loss: 17.3351, time taken: 1.6994872093200684s\n",
      "169/174, train_loss: 22.6723, time taken: 1.7194161415100098s\n",
      "170/174, train_loss: 21.2895, time taken: 1.779311180114746s\n",
      "171/174, train_loss: 14.5040, time taken: 1.6948857307434082s\n",
      "172/174, train_loss: 15.7943, time taken: 1.7181334495544434s\n",
      "173/174, train_loss: 18.5660, time taken: 1.7607853412628174s\n",
      "174/174, train_loss: 11.8047, time taken: 1.9062550067901611s\n",
      "175/174, train_loss: 17.5328, time taken: 1.3130168914794922s\n",
      "epoch 31 average loss: 17.2212\n",
      "Entering Validation for epoch: 31\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 31 Validation avg loss: 10.3585, time taken: 1.1018257141113281s\n",
      "----------\n",
      "epoch 32/2000\n",
      "1/174, train_loss: 20.1255, time taken: 8.131596326828003s\n",
      "2/174, train_loss: 16.4360, time taken: 2.2642548084259033s\n",
      "3/174, train_loss: 16.5625, time taken: 2.1194517612457275s\n",
      "4/174, train_loss: 18.0717, time taken: 3.3121516704559326s\n",
      "5/174, train_loss: 30.5786, time taken: 1.7303669452667236s\n",
      "6/174, train_loss: 22.8727, time taken: 1.9653806686401367s\n",
      "7/174, train_loss: 13.3403, time taken: 2.3298048973083496s\n",
      "8/174, train_loss: 20.9735, time taken: 1.7786381244659424s\n",
      "9/174, train_loss: 17.6935, time taken: 1.927597999572754s\n",
      "10/174, train_loss: 13.7073, time taken: 1.8738443851470947s\n",
      "11/174, train_loss: 16.5038, time taken: 1.789482593536377s\n",
      "12/174, train_loss: 18.7187, time taken: 2.0125668048858643s\n",
      "13/174, train_loss: 21.0661, time taken: 1.8780148029327393s\n",
      "14/174, train_loss: 13.9796, time taken: 2.084867000579834s\n",
      "15/174, train_loss: 16.1423, time taken: 1.7064213752746582s\n",
      "16/174, train_loss: 17.7190, time taken: 1.799363374710083s\n",
      "17/174, train_loss: 15.8720, time taken: 2.0134661197662354s\n",
      "18/174, train_loss: 24.8278, time taken: 1.6881818771362305s\n",
      "19/174, train_loss: 13.9825, time taken: 1.9959003925323486s\n",
      "20/174, train_loss: 18.3521, time taken: 1.7084686756134033s\n",
      "21/174, train_loss: 13.6339, time taken: 1.9970333576202393s\n",
      "22/174, train_loss: 18.1327, time taken: 1.8879404067993164s\n",
      "23/174, train_loss: 18.6081, time taken: 1.8948025703430176s\n",
      "24/174, train_loss: 16.7989, time taken: 1.8030788898468018s\n",
      "25/174, train_loss: 15.6066, time taken: 2.064168691635132s\n",
      "26/174, train_loss: 18.0698, time taken: 1.6963624954223633s\n",
      "27/174, train_loss: 14.9769, time taken: 1.730151891708374s\n",
      "28/174, train_loss: 13.4522, time taken: 1.8852882385253906s\n",
      "29/174, train_loss: 15.0270, time taken: 1.6823515892028809s\n",
      "30/174, train_loss: 19.1159, time taken: 1.7312119007110596s\n",
      "31/174, train_loss: 16.2125, time taken: 1.72993803024292s\n",
      "32/174, train_loss: 18.2579, time taken: 1.9115948677062988s\n",
      "33/174, train_loss: 20.8602, time taken: 1.9411702156066895s\n",
      "34/174, train_loss: 17.0641, time taken: 1.894313097000122s\n",
      "35/174, train_loss: 21.8200, time taken: 1.913802146911621s\n",
      "36/174, train_loss: 13.9910, time taken: 1.8939523696899414s\n",
      "37/174, train_loss: 18.6100, time taken: 1.8212132453918457s\n",
      "38/174, train_loss: 18.1714, time taken: 1.9937684535980225s\n",
      "39/174, train_loss: 15.4363, time taken: 2.003441095352173s\n",
      "40/174, train_loss: 15.1987, time taken: 1.9093539714813232s\n",
      "41/174, train_loss: 20.0744, time taken: 1.917027235031128s\n",
      "42/174, train_loss: 16.2388, time taken: 1.776789903640747s\n",
      "43/174, train_loss: 14.4227, time taken: 1.6954662799835205s\n",
      "44/174, train_loss: 19.0407, time taken: 1.7040073871612549s\n",
      "45/174, train_loss: 19.0925, time taken: 1.8124523162841797s\n",
      "46/174, train_loss: 15.0751, time taken: 2.084815263748169s\n",
      "47/174, train_loss: 13.6120, time taken: 1.7960848808288574s\n",
      "48/174, train_loss: 17.3507, time taken: 1.7766966819763184s\n",
      "49/174, train_loss: 12.7210, time taken: 3.430788278579712s\n",
      "50/174, train_loss: 17.1295, time taken: 1.70582914352417s\n",
      "51/174, train_loss: 17.4900, time taken: 2.3188745975494385s\n",
      "52/174, train_loss: 17.7203, time taken: 2.101154327392578s\n",
      "53/174, train_loss: 21.0075, time taken: 1.9960262775421143s\n",
      "54/174, train_loss: 14.0731, time taken: 2.073479413986206s\n",
      "55/174, train_loss: 19.9472, time taken: 2.1969428062438965s\n",
      "56/174, train_loss: 15.6699, time taken: 1.7850940227508545s\n",
      "57/174, train_loss: 17.4973, time taken: 1.8121542930603027s\n",
      "58/174, train_loss: 15.0185, time taken: 1.7848916053771973s\n",
      "59/174, train_loss: 22.7516, time taken: 2.0258939266204834s\n",
      "60/174, train_loss: 16.1397, time taken: 2.096644639968872s\n",
      "61/174, train_loss: 13.0576, time taken: 1.9801063537597656s\n",
      "62/174, train_loss: 14.1727, time taken: 1.794426441192627s\n",
      "63/174, train_loss: 15.8353, time taken: 1.907233476638794s\n",
      "64/174, train_loss: 19.8463, time taken: 1.8024804592132568s\n",
      "65/174, train_loss: 16.4157, time taken: 1.8758726119995117s\n",
      "66/174, train_loss: 20.3443, time taken: 1.70359468460083s\n",
      "67/174, train_loss: 13.2929, time taken: 1.7748970985412598s\n",
      "68/174, train_loss: 18.7827, time taken: 1.93768310546875s\n",
      "69/174, train_loss: 21.4406, time taken: 1.8707475662231445s\n",
      "70/174, train_loss: 20.4653, time taken: 2.0961039066314697s\n",
      "71/174, train_loss: 20.6428, time taken: 1.7920787334442139s\n",
      "72/174, train_loss: 12.8335, time taken: 1.9273943901062012s\n",
      "73/174, train_loss: 14.9217, time taken: 1.8757197856903076s\n",
      "74/174, train_loss: 19.5320, time taken: 1.7819819450378418s\n",
      "75/174, train_loss: 16.3352, time taken: 3.526712417602539s\n",
      "76/174, train_loss: 16.7146, time taken: 1.99200439453125s\n",
      "77/174, train_loss: 16.8870, time taken: 2.3773884773254395s\n",
      "78/174, train_loss: 14.4534, time taken: 2.1805553436279297s\n",
      "79/174, train_loss: 13.2944, time taken: 1.8288557529449463s\n",
      "80/174, train_loss: 15.7293, time taken: 1.7638962268829346s\n",
      "81/174, train_loss: 16.3859, time taken: 1.8215479850769043s\n",
      "82/174, train_loss: 21.0235, time taken: 1.9065217971801758s\n",
      "83/174, train_loss: 14.8105, time taken: 1.8932626247406006s\n",
      "84/174, train_loss: 22.6659, time taken: 1.7622489929199219s\n",
      "85/174, train_loss: 19.2227, time taken: 1.7112326622009277s\n",
      "86/174, train_loss: 17.0229, time taken: 1.8152778148651123s\n",
      "87/174, train_loss: 20.3558, time taken: 1.7930657863616943s\n",
      "88/174, train_loss: 18.7997, time taken: 1.876610517501831s\n",
      "89/174, train_loss: 17.1006, time taken: 2.1839349269866943s\n",
      "90/174, train_loss: 20.2891, time taken: 2.13625431060791s\n",
      "91/174, train_loss: 16.5891, time taken: 1.9879937171936035s\n",
      "92/174, train_loss: 14.6020, time taken: 1.8957715034484863s\n",
      "93/174, train_loss: 14.6065, time taken: 2.1652190685272217s\n",
      "94/174, train_loss: 18.0279, time taken: 1.8096156120300293s\n",
      "95/174, train_loss: 15.6948, time taken: 1.9064719676971436s\n",
      "96/174, train_loss: 14.7563, time taken: 1.7987918853759766s\n",
      "97/174, train_loss: 19.7401, time taken: 1.908832311630249s\n",
      "98/174, train_loss: 17.1185, time taken: 1.899712085723877s\n",
      "99/174, train_loss: 20.3323, time taken: 1.8052253723144531s\n",
      "100/174, train_loss: 13.3473, time taken: 1.8349003791809082s\n",
      "101/174, train_loss: 18.5940, time taken: 1.8723411560058594s\n",
      "102/174, train_loss: 16.4705, time taken: 1.8052780628204346s\n",
      "103/174, train_loss: 18.9077, time taken: 1.7874934673309326s\n",
      "104/174, train_loss: 15.1963, time taken: 1.8162260055541992s\n",
      "105/174, train_loss: 18.9335, time taken: 2.196457862854004s\n",
      "106/174, train_loss: 16.6338, time taken: 2.0760204792022705s\n",
      "107/174, train_loss: 18.2717, time taken: 2.1111879348754883s\n",
      "108/174, train_loss: 15.6978, time taken: 1.9985451698303223s\n",
      "109/174, train_loss: 12.1591, time taken: 1.9810879230499268s\n",
      "110/174, train_loss: 21.7294, time taken: 1.7813398838043213s\n",
      "111/174, train_loss: 15.7771, time taken: 1.9044454097747803s\n",
      "112/174, train_loss: 17.6135, time taken: 4.201120138168335s\n",
      "113/174, train_loss: 14.9811, time taken: 1.7902717590332031s\n",
      "114/174, train_loss: 20.7820, time taken: 1.8942992687225342s\n",
      "115/174, train_loss: 14.1533, time taken: 1.7952866554260254s\n",
      "116/174, train_loss: 20.9693, time taken: 1.8074162006378174s\n",
      "117/174, train_loss: 15.4017, time taken: 2.077178478240967s\n",
      "118/174, train_loss: 16.3234, time taken: 1.8185124397277832s\n",
      "119/174, train_loss: 15.2402, time taken: 1.898059368133545s\n",
      "120/174, train_loss: 19.1902, time taken: 1.8178937435150146s\n",
      "121/174, train_loss: 19.0691, time taken: 1.7323813438415527s\n",
      "122/174, train_loss: 18.7605, time taken: 1.769045352935791s\n",
      "123/174, train_loss: 17.2843, time taken: 1.8997855186462402s\n",
      "124/174, train_loss: 20.5231, time taken: 2.106717824935913s\n",
      "125/174, train_loss: 23.9530, time taken: 2.180044412612915s\n",
      "126/174, train_loss: 14.0149, time taken: 1.6932086944580078s\n",
      "127/174, train_loss: 15.5223, time taken: 1.9282279014587402s\n",
      "128/174, train_loss: 16.3094, time taken: 2.007838249206543s\n",
      "129/174, train_loss: 12.7455, time taken: 2.060508966445923s\n",
      "130/174, train_loss: 16.3137, time taken: 2.01029372215271s\n",
      "131/174, train_loss: 21.4442, time taken: 1.7950375080108643s\n",
      "132/174, train_loss: 13.0350, time taken: 1.8819119930267334s\n",
      "133/174, train_loss: 17.8811, time taken: 1.9222118854522705s\n",
      "134/174, train_loss: 18.8499, time taken: 1.8051795959472656s\n",
      "135/174, train_loss: 17.3480, time taken: 1.7673585414886475s\n",
      "136/174, train_loss: 21.1657, time taken: 2.014644145965576s\n",
      "137/174, train_loss: 15.5833, time taken: 1.8863153457641602s\n",
      "138/174, train_loss: 16.7678, time taken: 1.8019301891326904s\n",
      "139/174, train_loss: 15.9190, time taken: 1.7955994606018066s\n",
      "140/174, train_loss: 16.6822, time taken: 1.8719096183776855s\n",
      "141/174, train_loss: 17.3178, time taken: 1.7977213859558105s\n",
      "142/174, train_loss: 14.5726, time taken: 1.7264330387115479s\n",
      "143/174, train_loss: 20.9914, time taken: 1.975019931793213s\n",
      "144/174, train_loss: 17.6880, time taken: 1.9217579364776611s\n",
      "145/174, train_loss: 16.0057, time taken: 1.7627942562103271s\n",
      "146/174, train_loss: 16.1943, time taken: 2.1386919021606445s\n",
      "147/174, train_loss: 19.2725, time taken: 2.1504323482513428s\n",
      "148/174, train_loss: 15.1489, time taken: 1.8375022411346436s\n",
      "149/174, train_loss: 19.2410, time taken: 1.7858407497406006s\n",
      "150/174, train_loss: 12.2085, time taken: 1.7801029682159424s\n",
      "151/174, train_loss: 13.9819, time taken: 1.894381046295166s\n",
      "152/174, train_loss: 15.3172, time taken: 2.1191728115081787s\n",
      "153/174, train_loss: 19.1547, time taken: 1.7667841911315918s\n",
      "154/174, train_loss: 17.6415, time taken: 2.300839424133301s\n",
      "155/174, train_loss: 11.9804, time taken: 1.7296240329742432s\n",
      "156/174, train_loss: 13.7020, time taken: 2.0579540729522705s\n",
      "157/174, train_loss: 16.3847, time taken: 2.0328731536865234s\n",
      "158/174, train_loss: 16.5501, time taken: 1.7792472839355469s\n",
      "159/174, train_loss: 15.7345, time taken: 1.8931374549865723s\n",
      "160/174, train_loss: 17.9491, time taken: 1.9933252334594727s\n",
      "161/174, train_loss: 21.5547, time taken: 2.009413719177246s\n",
      "162/174, train_loss: 15.7139, time taken: 1.870666265487671s\n",
      "163/174, train_loss: 14.5370, time taken: 1.8261477947235107s\n",
      "164/174, train_loss: 18.2488, time taken: 2.082857608795166s\n",
      "165/174, train_loss: 20.0982, time taken: 2.004082679748535s\n",
      "166/174, train_loss: 19.0013, time taken: 1.973555088043213s\n",
      "167/174, train_loss: 17.6848, time taken: 2.1236116886138916s\n",
      "168/174, train_loss: 19.4425, time taken: 1.8851966857910156s\n",
      "169/174, train_loss: 18.0836, time taken: 1.7909936904907227s\n",
      "170/174, train_loss: 18.1900, time taken: 1.8111329078674316s\n",
      "171/174, train_loss: 15.4418, time taken: 1.7837271690368652s\n",
      "172/174, train_loss: 17.9017, time taken: 1.712083101272583s\n",
      "173/174, train_loss: 17.2605, time taken: 1.8731915950775146s\n",
      "174/174, train_loss: 20.6568, time taken: 1.8287158012390137s\n",
      "175/174, train_loss: 12.3661, time taken: 1.3793127536773682s\n",
      "epoch 32 average loss: 17.2660\n",
      "----------\n",
      "epoch 33/2000\n",
      "1/174, train_loss: 16.7380, time taken: 8.289757251739502s\n",
      "2/174, train_loss: 18.7441, time taken: 2.2101895809173584s\n",
      "3/174, train_loss: 18.2666, time taken: 1.9003863334655762s\n",
      "4/174, train_loss: 18.0047, time taken: 2.158088445663452s\n",
      "5/174, train_loss: 16.1152, time taken: 1.7159900665283203s\n",
      "6/174, train_loss: 15.9734, time taken: 1.7015275955200195s\n",
      "7/174, train_loss: 17.5662, time taken: 1.7892355918884277s\n",
      "8/174, train_loss: 16.8263, time taken: 1.8087334632873535s\n",
      "9/174, train_loss: 14.3243, time taken: 1.8043389320373535s\n",
      "10/174, train_loss: 15.2428, time taken: 1.6428353786468506s\n",
      "11/174, train_loss: 18.7052, time taken: 1.775540828704834s\n",
      "12/174, train_loss: 16.9024, time taken: 3.5204315185546875s\n",
      "13/174, train_loss: 22.4965, time taken: 1.9842987060546875s\n",
      "14/174, train_loss: 17.7966, time taken: 2.2847793102264404s\n",
      "15/174, train_loss: 15.3981, time taken: 1.7877871990203857s\n",
      "16/174, train_loss: 15.9667, time taken: 1.7096359729766846s\n",
      "17/174, train_loss: 20.0226, time taken: 1.7751870155334473s\n",
      "18/174, train_loss: 13.4221, time taken: 1.9145073890686035s\n",
      "19/174, train_loss: 18.4263, time taken: 1.8923439979553223s\n",
      "20/174, train_loss: 12.5975, time taken: 1.8102777004241943s\n",
      "21/174, train_loss: 16.0434, time taken: 1.9926517009735107s\n",
      "22/174, train_loss: 20.5413, time taken: 1.7857904434204102s\n",
      "23/174, train_loss: 16.3420, time taken: 1.7800307273864746s\n",
      "24/174, train_loss: 16.6942, time taken: 1.9260785579681396s\n",
      "25/174, train_loss: 20.1099, time taken: 1.9756228923797607s\n",
      "26/174, train_loss: 18.6666, time taken: 1.8825814723968506s\n",
      "27/174, train_loss: 21.4315, time taken: 1.8107595443725586s\n",
      "28/174, train_loss: 13.0522, time taken: 2.0233001708984375s\n",
      "29/174, train_loss: 18.7528, time taken: 1.8767979145050049s\n",
      "30/174, train_loss: 16.7880, time taken: 2.3896892070770264s\n",
      "31/174, train_loss: 16.6856, time taken: 2.115785837173462s\n",
      "32/174, train_loss: 11.0991, time taken: 1.9728193283081055s\n",
      "33/174, train_loss: 16.2872, time taken: 2.3086071014404297s\n",
      "34/174, train_loss: 12.3988, time taken: 2.172139883041382s\n",
      "35/174, train_loss: 20.1203, time taken: 2.117150068283081s\n",
      "36/174, train_loss: 18.9318, time taken: 1.9877870082855225s\n",
      "37/174, train_loss: 15.0956, time taken: 1.925184965133667s\n",
      "38/174, train_loss: 18.7418, time taken: 2.1995134353637695s\n",
      "39/174, train_loss: 13.3008, time taken: 1.9395205974578857s\n",
      "40/174, train_loss: 14.9041, time taken: 1.762829065322876s\n",
      "41/174, train_loss: 20.9476, time taken: 1.8007969856262207s\n",
      "42/174, train_loss: 21.0458, time taken: 1.8234584331512451s\n",
      "43/174, train_loss: 19.2453, time taken: 1.8365330696105957s\n",
      "44/174, train_loss: 15.7600, time taken: 1.7849798202514648s\n",
      "45/174, train_loss: 18.0129, time taken: 1.7756593227386475s\n",
      "46/174, train_loss: 15.0946, time taken: 1.9130399227142334s\n",
      "47/174, train_loss: 20.8640, time taken: 2.017198085784912s\n",
      "48/174, train_loss: 16.3809, time taken: 1.8858697414398193s\n",
      "49/174, train_loss: 17.1791, time taken: 1.7783112525939941s\n",
      "50/174, train_loss: 14.9386, time taken: 1.8092758655548096s\n",
      "51/174, train_loss: 17.2505, time taken: 1.782686710357666s\n",
      "52/174, train_loss: 19.1782, time taken: 1.8104355335235596s\n",
      "53/174, train_loss: 16.2564, time taken: 1.994774580001831s\n",
      "54/174, train_loss: 14.4352, time taken: 1.8072621822357178s\n",
      "55/174, train_loss: 19.3022, time taken: 1.7061824798583984s\n",
      "56/174, train_loss: 16.3314, time taken: 1.7780849933624268s\n",
      "57/174, train_loss: 17.2463, time taken: 1.9084954261779785s\n",
      "58/174, train_loss: 19.9276, time taken: 1.913557529449463s\n",
      "59/174, train_loss: 24.2326, time taken: 1.7025914192199707s\n",
      "60/174, train_loss: 13.1701, time taken: 1.7814462184906006s\n",
      "61/174, train_loss: 15.1489, time taken: 1.837754487991333s\n",
      "62/174, train_loss: 16.6536, time taken: 1.7634429931640625s\n",
      "63/174, train_loss: 16.5729, time taken: 1.7947633266448975s\n",
      "64/174, train_loss: 13.9892, time taken: 1.7354366779327393s\n",
      "65/174, train_loss: 18.6912, time taken: 1.706681251525879s\n",
      "66/174, train_loss: 12.7801, time taken: 2.1095786094665527s\n",
      "67/174, train_loss: 20.6697, time taken: 1.879190444946289s\n",
      "68/174, train_loss: 15.9487, time taken: 3.3263471126556396s\n",
      "69/174, train_loss: 16.0333, time taken: 1.7992594242095947s\n",
      "70/174, train_loss: 19.1212, time taken: 1.8757126331329346s\n",
      "71/174, train_loss: 15.7899, time taken: 1.7924854755401611s\n",
      "72/174, train_loss: 14.7033, time taken: 1.9124655723571777s\n",
      "73/174, train_loss: 18.7098, time taken: 1.9772613048553467s\n",
      "74/174, train_loss: 19.4731, time taken: 2.0185587406158447s\n",
      "75/174, train_loss: 17.1545, time taken: 1.7969615459442139s\n",
      "76/174, train_loss: 22.6926, time taken: 1.8841602802276611s\n",
      "77/174, train_loss: 14.5356, time taken: 1.899275302886963s\n",
      "78/174, train_loss: 16.3084, time taken: 1.9807798862457275s\n",
      "79/174, train_loss: 17.7197, time taken: 1.8110685348510742s\n",
      "80/174, train_loss: 17.7131, time taken: 1.7983572483062744s\n",
      "81/174, train_loss: 18.8667, time taken: 1.995032548904419s\n",
      "82/174, train_loss: 20.5932, time taken: 1.8068451881408691s\n",
      "83/174, train_loss: 20.7928, time taken: 1.8773953914642334s\n",
      "84/174, train_loss: 16.1380, time taken: 2.0207059383392334s\n",
      "85/174, train_loss: 17.6320, time taken: 1.8797733783721924s\n",
      "86/174, train_loss: 15.1374, time taken: 1.7765388488769531s\n",
      "87/174, train_loss: 14.4186, time taken: 2.006270408630371s\n",
      "88/174, train_loss: 15.5661, time taken: 1.8272693157196045s\n",
      "89/174, train_loss: 16.0292, time taken: 1.8818190097808838s\n",
      "90/174, train_loss: 23.6405, time taken: 1.792048692703247s\n",
      "91/174, train_loss: 16.0836, time taken: 1.9928085803985596s\n",
      "92/174, train_loss: 14.2725, time taken: 1.882373332977295s\n",
      "93/174, train_loss: 15.9165, time taken: 2.137270450592041s\n",
      "94/174, train_loss: 16.5959, time taken: 1.6254711151123047s\n",
      "95/174, train_loss: 16.3310, time taken: 2.2836802005767822s\n",
      "96/174, train_loss: 18.2737, time taken: 1.8250775337219238s\n",
      "97/174, train_loss: 15.6372, time taken: 1.9373703002929688s\n",
      "98/174, train_loss: 15.4905, time taken: 1.8604204654693604s\n",
      "99/174, train_loss: 20.7133, time taken: 1.818972110748291s\n",
      "100/174, train_loss: 14.7225, time taken: 2.0774571895599365s\n",
      "101/174, train_loss: 22.7655, time taken: 1.9957730770111084s\n",
      "102/174, train_loss: 16.1780, time taken: 2.0276458263397217s\n",
      "103/174, train_loss: 15.5509, time taken: 2.0012457370758057s\n",
      "104/174, train_loss: 17.3236, time taken: 2.278353214263916s\n",
      "105/174, train_loss: 16.1899, time taken: 1.9078145027160645s\n",
      "106/174, train_loss: 16.5678, time taken: 2.267716884613037s\n",
      "107/174, train_loss: 14.0096, time taken: 2.224778175354004s\n",
      "108/174, train_loss: 18.3473, time taken: 1.9016640186309814s\n",
      "109/174, train_loss: 22.4736, time taken: 2.0613296031951904s\n",
      "110/174, train_loss: 18.1796, time taken: 2.094557285308838s\n",
      "111/174, train_loss: 17.8054, time taken: 2.2044334411621094s\n",
      "112/174, train_loss: 39.1799, time taken: 1.814164400100708s\n",
      "113/174, train_loss: 19.4092, time taken: 1.9947032928466797s\n",
      "114/174, train_loss: 18.9574, time taken: 2.0827670097351074s\n",
      "115/174, train_loss: 15.1682, time taken: 1.8032503128051758s\n",
      "116/174, train_loss: 16.6340, time taken: 1.9012048244476318s\n",
      "117/174, train_loss: 19.1903, time taken: 1.9155399799346924s\n",
      "118/174, train_loss: 17.3587, time taken: 1.7908613681793213s\n",
      "119/174, train_loss: 19.4988, time taken: 1.7430341243743896s\n",
      "120/174, train_loss: 17.0067, time taken: 1.8573369979858398s\n",
      "121/174, train_loss: 17.7115, time taken: 1.8150534629821777s\n",
      "122/174, train_loss: 14.0823, time taken: 1.805128812789917s\n",
      "123/174, train_loss: 17.5926, time taken: 1.7877440452575684s\n",
      "124/174, train_loss: 16.7835, time taken: 2.1763858795166016s\n",
      "125/174, train_loss: 20.9258, time taken: 2.1278486251831055s\n",
      "126/174, train_loss: 18.3878, time taken: 1.9792919158935547s\n",
      "127/174, train_loss: 22.6647, time taken: 1.917619228363037s\n",
      "128/174, train_loss: 18.5100, time taken: 1.8580734729766846s\n",
      "129/174, train_loss: 22.6021, time taken: 2.0914785861968994s\n",
      "130/174, train_loss: 17.4479, time taken: 1.924257755279541s\n",
      "131/174, train_loss: 27.6675, time taken: 1.7772867679595947s\n",
      "132/174, train_loss: 17.1256, time taken: 1.9211804866790771s\n",
      "133/174, train_loss: 20.9057, time taken: 1.796421766281128s\n",
      "134/174, train_loss: 15.7070, time taken: 1.8753256797790527s\n",
      "135/174, train_loss: 14.5349, time taken: 1.8063647747039795s\n",
      "136/174, train_loss: 16.2843, time taken: 2.2839837074279785s\n",
      "137/174, train_loss: 16.2959, time taken: 2.2887673377990723s\n",
      "138/174, train_loss: 14.7155, time taken: 1.7335178852081299s\n",
      "139/174, train_loss: 16.7753, time taken: 1.8716349601745605s\n",
      "140/174, train_loss: 14.2373, time taken: 2.079970359802246s\n",
      "141/174, train_loss: 15.9911, time taken: 2.432842969894409s\n",
      "142/174, train_loss: 17.0938, time taken: 2.074093818664551s\n",
      "143/174, train_loss: 17.9461, time taken: 1.9033994674682617s\n",
      "144/174, train_loss: 14.9906, time taken: 1.9684584140777588s\n",
      "145/174, train_loss: 17.7017, time taken: 2.0240604877471924s\n",
      "146/174, train_loss: 16.2796, time taken: 1.9022390842437744s\n",
      "147/174, train_loss: 17.3121, time taken: 1.9976181983947754s\n",
      "148/174, train_loss: 14.6003, time taken: 2.000330924987793s\n",
      "149/174, train_loss: 15.2472, time taken: 1.9252886772155762s\n",
      "150/174, train_loss: 11.6986, time taken: 2.190627336502075s\n",
      "151/174, train_loss: 14.6669, time taken: 1.904005765914917s\n",
      "152/174, train_loss: 16.1446, time taken: 1.9760968685150146s\n",
      "153/174, train_loss: 18.6268, time taken: 1.8195245265960693s\n",
      "154/174, train_loss: 14.4777, time taken: 1.8811116218566895s\n",
      "155/174, train_loss: 17.3032, time taken: 1.8835136890411377s\n",
      "156/174, train_loss: 16.4212, time taken: 1.7154772281646729s\n",
      "157/174, train_loss: 21.5643, time taken: 1.9871869087219238s\n",
      "158/174, train_loss: 22.2259, time taken: 1.8882851600646973s\n",
      "159/174, train_loss: 16.7715, time taken: 1.904611587524414s\n",
      "160/174, train_loss: 15.8903, time taken: 1.8310866355895996s\n",
      "161/174, train_loss: 14.4352, time taken: 1.8601388931274414s\n",
      "162/174, train_loss: 21.0949, time taken: 1.8200337886810303s\n",
      "163/174, train_loss: 22.4243, time taken: 2.093954086303711s\n",
      "164/174, train_loss: 15.2773, time taken: 1.8806557655334473s\n",
      "165/174, train_loss: 22.9434, time taken: 1.8214936256408691s\n",
      "166/174, train_loss: 14.0320, time taken: 1.7835028171539307s\n",
      "167/174, train_loss: 15.9622, time taken: 1.872002363204956s\n",
      "168/174, train_loss: 21.4495, time taken: 1.7094745635986328s\n",
      "169/174, train_loss: 16.3266, time taken: 1.7876813411712646s\n",
      "170/174, train_loss: 15.5369, time taken: 2.115175724029541s\n",
      "171/174, train_loss: 17.4032, time taken: 1.976149559020996s\n",
      "172/174, train_loss: 11.8086, time taken: 3.724799156188965s\n",
      "173/174, train_loss: 17.2699, time taken: 2.0148215293884277s\n",
      "174/174, train_loss: 13.9032, time taken: 1.7908568382263184s\n",
      "175/174, train_loss: 24.4512, time taken: 1.8017470836639404s\n",
      "epoch 33 average loss: 17.4377\n",
      "Entering Validation for epoch: 33\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 33 Validation avg loss: 11.1510, time taken: 1.1079967021942139s\n",
      "----------\n",
      "epoch 34/2000\n",
      "1/174, train_loss: 13.5353, time taken: 8.442246437072754s\n",
      "2/174, train_loss: 21.8065, time taken: 2.3249125480651855s\n",
      "3/174, train_loss: 21.3777, time taken: 2.1691555976867676s\n",
      "4/174, train_loss: 18.2331, time taken: 2.4929111003875732s\n",
      "5/174, train_loss: 20.4659, time taken: 2.3122901916503906s\n",
      "6/174, train_loss: 17.0959, time taken: 2.2753798961639404s\n",
      "7/174, train_loss: 22.8529, time taken: 2.006012439727783s\n",
      "8/174, train_loss: 19.3017, time taken: 1.8848683834075928s\n",
      "9/174, train_loss: 16.4661, time taken: 1.6950373649597168s\n",
      "10/174, train_loss: 16.6100, time taken: 1.917389154434204s\n",
      "11/174, train_loss: 19.3395, time taken: 2.1766791343688965s\n",
      "12/174, train_loss: 15.0472, time taken: 2.1029841899871826s\n",
      "13/174, train_loss: 18.1183, time taken: 2.2076287269592285s\n",
      "14/174, train_loss: 13.0473, time taken: 2.3002960681915283s\n",
      "15/174, train_loss: 16.1394, time taken: 1.9031174182891846s\n",
      "16/174, train_loss: 18.5276, time taken: 1.8933930397033691s\n",
      "17/174, train_loss: 20.0316, time taken: 2.002544641494751s\n",
      "18/174, train_loss: 16.9246, time taken: 1.8628044128417969s\n",
      "19/174, train_loss: 15.2440, time taken: 1.7375693321228027s\n",
      "20/174, train_loss: 13.9742, time taken: 1.8213579654693604s\n",
      "21/174, train_loss: 15.7537, time taken: 1.7932689189910889s\n",
      "22/174, train_loss: 17.7142, time taken: 1.8905398845672607s\n",
      "23/174, train_loss: 12.5439, time taken: 1.802947998046875s\n",
      "24/174, train_loss: 17.2440, time taken: 1.818483829498291s\n",
      "25/174, train_loss: 15.4065, time taken: 1.7962121963500977s\n",
      "26/174, train_loss: 18.0578, time taken: 1.856564998626709s\n",
      "27/174, train_loss: 18.8877, time taken: 1.9446544647216797s\n",
      "28/174, train_loss: 14.6225, time taken: 1.8702967166900635s\n",
      "29/174, train_loss: 16.3637, time taken: 1.6944551467895508s\n",
      "30/174, train_loss: 16.0078, time taken: 1.683307409286499s\n",
      "31/174, train_loss: 15.8581, time taken: 1.6905579566955566s\n",
      "32/174, train_loss: 17.4191, time taken: 1.7064323425292969s\n",
      "33/174, train_loss: 17.0036, time taken: 1.7048001289367676s\n",
      "34/174, train_loss: 20.7487, time taken: 1.706989049911499s\n",
      "35/174, train_loss: 16.0629, time taken: 1.8123621940612793s\n",
      "36/174, train_loss: 13.9855, time taken: 1.7380859851837158s\n",
      "37/174, train_loss: 16.9267, time taken: 1.8056352138519287s\n",
      "38/174, train_loss: 21.9852, time taken: 3.7345709800720215s\n",
      "39/174, train_loss: 19.5652, time taken: 2.19573712348938s\n",
      "40/174, train_loss: 17.3430, time taken: 2.2756218910217285s\n",
      "41/174, train_loss: 19.6038, time taken: 1.89674973487854s\n",
      "42/174, train_loss: 20.5020, time taken: 1.9985017776489258s\n",
      "43/174, train_loss: 17.7604, time taken: 1.783761739730835s\n",
      "44/174, train_loss: 14.9182, time taken: 1.826404333114624s\n",
      "45/174, train_loss: 17.9732, time taken: 1.885354995727539s\n",
      "46/174, train_loss: 15.7462, time taken: 1.8985545635223389s\n",
      "47/174, train_loss: 15.1511, time taken: 1.805666208267212s\n",
      "48/174, train_loss: 19.1786, time taken: 1.9768640995025635s\n",
      "49/174, train_loss: 16.9738, time taken: 2.1020588874816895s\n",
      "50/174, train_loss: 14.5161, time taken: 2.312289237976074s\n",
      "51/174, train_loss: 18.8142, time taken: 1.9785926342010498s\n",
      "52/174, train_loss: 13.5208, time taken: 1.9162280559539795s\n",
      "53/174, train_loss: 19.9393, time taken: 1.8709301948547363s\n",
      "54/174, train_loss: 14.1920, time taken: 1.787482500076294s\n",
      "55/174, train_loss: 19.6926, time taken: 1.8076889514923096s\n",
      "56/174, train_loss: 15.7497, time taken: 1.9922757148742676s\n",
      "57/174, train_loss: 19.7552, time taken: 1.7337234020233154s\n",
      "58/174, train_loss: 14.8578, time taken: 1.8580477237701416s\n",
      "59/174, train_loss: 17.1227, time taken: 2.022921323776245s\n",
      "60/174, train_loss: 17.4561, time taken: 1.9810264110565186s\n",
      "61/174, train_loss: 16.4662, time taken: 1.9881513118743896s\n",
      "62/174, train_loss: 18.8167, time taken: 2.3308236598968506s\n",
      "63/174, train_loss: 16.5328, time taken: 2.0656917095184326s\n",
      "64/174, train_loss: 17.4851, time taken: 1.91408371925354s\n",
      "65/174, train_loss: 13.4652, time taken: 2.0735936164855957s\n",
      "66/174, train_loss: 11.7499, time taken: 2.0182669162750244s\n",
      "67/174, train_loss: 19.7423, time taken: 3.5035760402679443s\n",
      "68/174, train_loss: 16.0553, time taken: 1.8878228664398193s\n",
      "69/174, train_loss: 14.6256, time taken: 2.1838786602020264s\n",
      "70/174, train_loss: 21.5259, time taken: 1.8997166156768799s\n",
      "71/174, train_loss: 14.1779, time taken: 2.0846524238586426s\n",
      "72/174, train_loss: 14.5541, time taken: 1.9210364818572998s\n",
      "73/174, train_loss: 16.6635, time taken: 2.0031704902648926s\n",
      "74/174, train_loss: 14.9497, time taken: 1.998023509979248s\n",
      "75/174, train_loss: 18.4438, time taken: 2.291550397872925s\n",
      "76/174, train_loss: 19.1288, time taken: 1.9948244094848633s\n",
      "77/174, train_loss: 21.5667, time taken: 2.012244462966919s\n",
      "78/174, train_loss: 15.0474, time taken: 1.8838343620300293s\n",
      "79/174, train_loss: 14.0722, time taken: 1.9144890308380127s\n",
      "80/174, train_loss: 16.2061, time taken: 1.80576753616333s\n",
      "81/174, train_loss: 20.0511, time taken: 1.806394338607788s\n",
      "82/174, train_loss: 20.4840, time taken: 1.8945584297180176s\n",
      "83/174, train_loss: 18.7932, time taken: 1.799647569656372s\n",
      "84/174, train_loss: 14.6264, time taken: 1.9204261302947998s\n",
      "85/174, train_loss: 15.3551, time taken: 2.0710668563842773s\n",
      "86/174, train_loss: 13.1288, time taken: 1.8952910900115967s\n",
      "87/174, train_loss: 16.8807, time taken: 2.014644145965576s\n",
      "88/174, train_loss: 14.0819, time taken: 1.873650312423706s\n",
      "89/174, train_loss: 17.7998, time taken: 1.7240548133850098s\n",
      "90/174, train_loss: 17.2309, time taken: 1.900679111480713s\n",
      "91/174, train_loss: 16.9250, time taken: 2.0679502487182617s\n",
      "92/174, train_loss: 17.2533, time taken: 1.8214402198791504s\n",
      "93/174, train_loss: 15.2636, time taken: 2.881997585296631s\n",
      "94/174, train_loss: 14.1500, time taken: 1.9038655757904053s\n",
      "95/174, train_loss: 15.9113, time taken: 1.9941811561584473s\n",
      "96/174, train_loss: 18.4634, time taken: 1.8751280307769775s\n",
      "97/174, train_loss: 21.2524, time taken: 1.7193968296051025s\n",
      "98/174, train_loss: 15.7715, time taken: 1.9798710346221924s\n",
      "99/174, train_loss: 20.5263, time taken: 1.7172966003417969s\n",
      "100/174, train_loss: 19.0853, time taken: 2.1872308254241943s\n",
      "101/174, train_loss: 17.0805, time taken: 1.9027745723724365s\n",
      "102/174, train_loss: 21.4085, time taken: 1.7914316654205322s\n",
      "103/174, train_loss: 15.8696, time taken: 2.2834060192108154s\n",
      "104/174, train_loss: 17.5388, time taken: 2.2229061126708984s\n",
      "105/174, train_loss: 20.9800, time taken: 1.9813282489776611s\n",
      "106/174, train_loss: 20.4763, time taken: 2.178679943084717s\n",
      "107/174, train_loss: 13.3340, time taken: 2.112757921218872s\n",
      "108/174, train_loss: 17.4125, time taken: 1.9257922172546387s\n",
      "109/174, train_loss: 16.1417, time taken: 2.036403179168701s\n",
      "110/174, train_loss: 18.3812, time taken: 1.886566162109375s\n",
      "111/174, train_loss: 15.2892, time taken: 1.9670228958129883s\n",
      "112/174, train_loss: 18.3775, time taken: 1.7049076557159424s\n",
      "113/174, train_loss: 22.2867, time taken: 1.8032474517822266s\n",
      "114/174, train_loss: 20.4755, time taken: 1.988562822341919s\n",
      "115/174, train_loss: 19.3521, time taken: 1.990030288696289s\n",
      "116/174, train_loss: 19.0466, time taken: 1.8131444454193115s\n",
      "117/174, train_loss: 17.1410, time taken: 1.8940210342407227s\n",
      "118/174, train_loss: 17.0588, time taken: 2.1240458488464355s\n",
      "119/174, train_loss: 18.0455, time taken: 1.8772976398468018s\n",
      "120/174, train_loss: 17.1837, time taken: 1.7163360118865967s\n",
      "121/174, train_loss: 16.3161, time taken: 1.799252986907959s\n",
      "122/174, train_loss: 17.9705, time taken: 1.875549077987671s\n",
      "123/174, train_loss: 18.7021, time taken: 1.9946739673614502s\n",
      "124/174, train_loss: 22.0056, time taken: 1.9002814292907715s\n",
      "125/174, train_loss: 15.3856, time taken: 1.7744178771972656s\n",
      "126/174, train_loss: 13.1724, time taken: 1.713954210281372s\n",
      "127/174, train_loss: 12.4596, time taken: 1.7914347648620605s\n",
      "128/174, train_loss: 18.0827, time taken: 1.9194037914276123s\n",
      "129/174, train_loss: 15.5654, time taken: 1.8788831233978271s\n",
      "130/174, train_loss: 17.2027, time taken: 3.3136775493621826s\n",
      "131/174, train_loss: 14.3969, time taken: 2.2874674797058105s\n",
      "132/174, train_loss: 17.2240, time taken: 1.8861753940582275s\n",
      "133/174, train_loss: 16.9985, time taken: 2.0129852294921875s\n",
      "134/174, train_loss: 17.9783, time taken: 2.185239315032959s\n",
      "135/174, train_loss: 17.5110, time taken: 1.7964417934417725s\n",
      "136/174, train_loss: 15.2194, time taken: 2.015911817550659s\n",
      "137/174, train_loss: 17.9729, time taken: 2.000154972076416s\n",
      "138/174, train_loss: 16.4618, time taken: 1.9960424900054932s\n",
      "139/174, train_loss: 14.6159, time taken: 1.8734393119812012s\n",
      "140/174, train_loss: 18.9002, time taken: 2.1964831352233887s\n",
      "141/174, train_loss: 13.7007, time taken: 2.197279453277588s\n",
      "142/174, train_loss: 20.0025, time taken: 1.9056363105773926s\n",
      "143/174, train_loss: 20.0613, time taken: 2.3851473331451416s\n",
      "144/174, train_loss: 18.9413, time taken: 1.9000015258789062s\n",
      "145/174, train_loss: 15.3001, time taken: 1.7811598777770996s\n",
      "146/174, train_loss: 12.7184, time taken: 2.0911431312561035s\n",
      "147/174, train_loss: 18.0493, time taken: 2.0256404876708984s\n",
      "148/174, train_loss: 15.7895, time taken: 2.094496488571167s\n",
      "149/174, train_loss: 16.6608, time taken: 1.882763385772705s\n",
      "150/174, train_loss: 17.4671, time taken: 1.8992769718170166s\n",
      "151/174, train_loss: 18.6638, time taken: 1.7804620265960693s\n",
      "152/174, train_loss: 19.3603, time taken: 1.8218731880187988s\n",
      "153/174, train_loss: 22.5669, time taken: 2.067847967147827s\n",
      "154/174, train_loss: 17.2278, time taken: 2.0126514434814453s\n",
      "155/174, train_loss: 18.6267, time taken: 1.7845304012298584s\n",
      "156/174, train_loss: 20.0969, time taken: 1.8265180587768555s\n",
      "157/174, train_loss: 16.8483, time taken: 1.786803960800171s\n",
      "158/174, train_loss: 18.4417, time taken: 1.776092529296875s\n",
      "159/174, train_loss: 12.0008, time taken: 1.9947688579559326s\n",
      "160/174, train_loss: 15.2502, time taken: 1.9228203296661377s\n",
      "161/174, train_loss: 13.8526, time taken: 1.8124651908874512s\n",
      "162/174, train_loss: 26.7726, time taken: 2.120820999145508s\n",
      "163/174, train_loss: 19.1975, time taken: 1.8948731422424316s\n",
      "164/174, train_loss: 17.6593, time taken: 1.7153692245483398s\n",
      "165/174, train_loss: 16.8651, time taken: 1.778972864151001s\n",
      "166/174, train_loss: 14.8010, time taken: 1.7936973571777344s\n",
      "167/174, train_loss: 17.3001, time taken: 2.196718215942383s\n",
      "168/174, train_loss: 18.6639, time taken: 1.7175943851470947s\n",
      "169/174, train_loss: 15.3491, time taken: 2.079827070236206s\n",
      "170/174, train_loss: 16.6624, time taken: 1.69179368019104s\n",
      "171/174, train_loss: 14.6172, time taken: 1.8045375347137451s\n",
      "172/174, train_loss: 15.0801, time taken: 1.6891376972198486s\n",
      "173/174, train_loss: 18.9724, time taken: 1.786414384841919s\n",
      "174/174, train_loss: 18.9568, time taken: 1.9335896968841553s\n",
      "175/174, train_loss: 14.1101, time taken: 1.4118034839630127s\n",
      "epoch 34 average loss: 17.2287\n",
      "----------\n",
      "epoch 35/2000\n",
      "1/174, train_loss: 21.3923, time taken: 8.867552757263184s\n",
      "2/174, train_loss: 19.6809, time taken: 2.893930196762085s\n",
      "3/174, train_loss: 18.7617, time taken: 2.6246562004089355s\n",
      "4/174, train_loss: 18.3291, time taken: 2.264256477355957s\n",
      "5/174, train_loss: 16.0084, time taken: 1.786550760269165s\n",
      "6/174, train_loss: 17.3851, time taken: 2.211904764175415s\n",
      "7/174, train_loss: 23.5613, time taken: 1.778998851776123s\n",
      "8/174, train_loss: 18.3046, time taken: 1.9149112701416016s\n",
      "9/174, train_loss: 14.8611, time taken: 1.9132533073425293s\n",
      "10/174, train_loss: 19.3353, time taken: 1.973902940750122s\n",
      "11/174, train_loss: 19.7121, time taken: 1.8214869499206543s\n",
      "12/174, train_loss: 21.9698, time taken: 2.1854910850524902s\n",
      "13/174, train_loss: 13.8225, time taken: 1.7938294410705566s\n",
      "14/174, train_loss: 18.4995, time taken: 1.8032903671264648s\n",
      "15/174, train_loss: 16.9099, time taken: 1.9903271198272705s\n",
      "16/174, train_loss: 15.3301, time taken: 1.7804527282714844s\n",
      "17/174, train_loss: 16.6193, time taken: 1.9984030723571777s\n",
      "18/174, train_loss: 15.7502, time taken: 1.7149598598480225s\n",
      "19/174, train_loss: 14.6976, time taken: 1.9942405223846436s\n",
      "20/174, train_loss: 20.2822, time taken: 2.073620557785034s\n",
      "21/174, train_loss: 18.5698, time taken: 2.0295028686523438s\n",
      "22/174, train_loss: 16.1059, time taken: 1.9066274166107178s\n",
      "23/174, train_loss: 17.2513, time taken: 1.7305097579956055s\n",
      "24/174, train_loss: 16.8476, time taken: 1.884268045425415s\n",
      "25/174, train_loss: 16.6879, time taken: 1.9042854309082031s\n",
      "26/174, train_loss: 16.7085, time taken: 1.8744730949401855s\n",
      "27/174, train_loss: 13.9233, time taken: 1.8264238834381104s\n",
      "28/174, train_loss: 15.4273, time taken: 1.8071141242980957s\n",
      "29/174, train_loss: 17.0866, time taken: 2.17751407623291s\n",
      "30/174, train_loss: 13.6009, time taken: 1.9878535270690918s\n",
      "31/174, train_loss: 15.9999, time taken: 1.7265779972076416s\n",
      "32/174, train_loss: 16.4434, time taken: 1.9386980533599854s\n",
      "33/174, train_loss: 15.2000, time taken: 1.771620750427246s\n",
      "34/174, train_loss: 18.0513, time taken: 1.8191196918487549s\n",
      "35/174, train_loss: 21.7039, time taken: 1.8734443187713623s\n",
      "36/174, train_loss: 20.3800, time taken: 1.8147237300872803s\n",
      "37/174, train_loss: 18.4135, time taken: 1.8842456340789795s\n",
      "38/174, train_loss: 13.0930, time taken: 1.699958086013794s\n",
      "39/174, train_loss: 14.2406, time taken: 1.7840125560760498s\n",
      "40/174, train_loss: 15.4922, time taken: 1.9211549758911133s\n",
      "41/174, train_loss: 13.9205, time taken: 1.8896243572235107s\n",
      "42/174, train_loss: 17.7577, time taken: 1.88869309425354s\n",
      "43/174, train_loss: 17.6803, time taken: 1.893148422241211s\n",
      "44/174, train_loss: 18.6657, time taken: 1.9110705852508545s\n",
      "45/174, train_loss: 14.8074, time taken: 1.79152250289917s\n",
      "46/174, train_loss: 18.7394, time taken: 1.7884597778320312s\n",
      "47/174, train_loss: 19.2839, time taken: 2.008305072784424s\n",
      "48/174, train_loss: 15.7605, time taken: 1.9858615398406982s\n",
      "49/174, train_loss: 15.2956, time taken: 1.9014933109283447s\n",
      "50/174, train_loss: 15.0506, time taken: 1.790759801864624s\n",
      "51/174, train_loss: 18.5987, time taken: 1.812828779220581s\n",
      "52/174, train_loss: 16.2924, time taken: 2.565031051635742s\n",
      "53/174, train_loss: 15.5841, time taken: 2.025412082672119s\n",
      "54/174, train_loss: 17.0241, time taken: 1.7701032161712646s\n",
      "55/174, train_loss: 34.0712, time taken: 1.83408784866333s\n",
      "56/174, train_loss: 13.6206, time taken: 2.6708874702453613s\n",
      "57/174, train_loss: 18.2934, time taken: 1.8142297267913818s\n",
      "58/174, train_loss: 15.0537, time taken: 1.797419548034668s\n",
      "59/174, train_loss: 13.2668, time taken: 1.8927500247955322s\n",
      "60/174, train_loss: 18.4390, time taken: 1.910984754562378s\n",
      "61/174, train_loss: 16.9820, time taken: 1.9755306243896484s\n",
      "62/174, train_loss: 21.8280, time taken: 1.809201717376709s\n",
      "63/174, train_loss: 19.6206, time taken: 1.9948227405548096s\n",
      "64/174, train_loss: 16.9835, time taken: 1.7818386554718018s\n",
      "65/174, train_loss: 21.9534, time taken: 1.7184357643127441s\n",
      "66/174, train_loss: 18.2754, time taken: 1.7245678901672363s\n",
      "67/174, train_loss: 16.3411, time taken: 1.7979695796966553s\n",
      "68/174, train_loss: 18.1005, time taken: 2.003857135772705s\n",
      "69/174, train_loss: 15.6821, time taken: 2.082972764968872s\n",
      "70/174, train_loss: 17.1414, time taken: 1.7001495361328125s\n",
      "71/174, train_loss: 22.7953, time taken: 3.77347731590271s\n",
      "72/174, train_loss: 21.7201, time taken: 1.9277911186218262s\n",
      "73/174, train_loss: 14.4133, time taken: 2.1017000675201416s\n",
      "74/174, train_loss: 18.0659, time taken: 2.177205801010132s\n",
      "75/174, train_loss: 18.5281, time taken: 1.9882919788360596s\n",
      "76/174, train_loss: 23.1011, time taken: 2.2033143043518066s\n",
      "77/174, train_loss: 16.7943, time taken: 1.701965093612671s\n",
      "78/174, train_loss: 15.9053, time taken: 1.8755223751068115s\n",
      "79/174, train_loss: 18.6984, time taken: 1.9998605251312256s\n",
      "80/174, train_loss: 18.7843, time taken: 1.8179607391357422s\n",
      "81/174, train_loss: 18.4959, time taken: 1.8821730613708496s\n",
      "82/174, train_loss: 16.9665, time taken: 1.7212467193603516s\n",
      "83/174, train_loss: 16.0575, time taken: 1.7899608612060547s\n",
      "84/174, train_loss: 23.0948, time taken: 1.9047744274139404s\n",
      "85/174, train_loss: 17.1669, time taken: 1.8667075634002686s\n",
      "86/174, train_loss: 14.3268, time taken: 1.719146728515625s\n",
      "87/174, train_loss: 22.7458, time taken: 1.7922275066375732s\n",
      "88/174, train_loss: 16.9100, time taken: 2.0965452194213867s\n",
      "89/174, train_loss: 16.8018, time taken: 1.983119010925293s\n",
      "90/174, train_loss: 18.1588, time taken: 1.7008411884307861s\n",
      "91/174, train_loss: 11.6897, time taken: 2.1199283599853516s\n",
      "92/174, train_loss: 11.9250, time taken: 1.8513686656951904s\n",
      "93/174, train_loss: 14.1529, time taken: 1.7245609760284424s\n",
      "94/174, train_loss: 19.9831, time taken: 1.7974257469177246s\n",
      "95/174, train_loss: 17.5337, time taken: 2.0886178016662598s\n",
      "96/174, train_loss: 11.9299, time taken: 1.799597978591919s\n",
      "97/174, train_loss: 15.8251, time taken: 1.8783066272735596s\n",
      "98/174, train_loss: 15.5875, time taken: 1.8047819137573242s\n",
      "99/174, train_loss: 15.2620, time taken: 2.1098732948303223s\n",
      "100/174, train_loss: 14.3707, time taken: 2.0150177478790283s\n",
      "101/174, train_loss: 21.4761, time taken: 1.8603901863098145s\n",
      "102/174, train_loss: 13.2733, time taken: 3.5929415225982666s\n",
      "103/174, train_loss: 19.7935, time taken: 1.8221335411071777s\n",
      "104/174, train_loss: 13.6529, time taken: 1.98213529586792s\n",
      "105/174, train_loss: 16.0280, time taken: 2.1940577030181885s\n",
      "106/174, train_loss: 16.2946, time taken: 1.8949666023254395s\n",
      "107/174, train_loss: 20.1631, time taken: 1.9236702919006348s\n",
      "108/174, train_loss: 15.9173, time taken: 2.064167022705078s\n",
      "109/174, train_loss: 19.2435, time taken: 1.9266998767852783s\n",
      "110/174, train_loss: 18.6535, time taken: 1.8855302333831787s\n",
      "111/174, train_loss: 14.2603, time taken: 1.9796886444091797s\n",
      "112/174, train_loss: 16.1431, time taken: 1.8168833255767822s\n",
      "113/174, train_loss: 18.9344, time taken: 1.7989585399627686s\n",
      "114/174, train_loss: 18.7756, time taken: 1.801769495010376s\n",
      "115/174, train_loss: 16.2252, time taken: 1.9788615703582764s\n",
      "116/174, train_loss: 19.0743, time taken: 1.891615629196167s\n",
      "117/174, train_loss: 19.7838, time taken: 1.8232343196868896s\n",
      "118/174, train_loss: 17.3543, time taken: 1.9939570426940918s\n",
      "119/174, train_loss: 16.0892, time taken: 1.8876276016235352s\n",
      "120/174, train_loss: 17.5550, time taken: 1.9066855907440186s\n",
      "121/174, train_loss: 15.5260, time taken: 2.085845470428467s\n",
      "122/174, train_loss: 16.8764, time taken: 1.711014986038208s\n",
      "123/174, train_loss: 21.3735, time taken: 1.7940335273742676s\n",
      "124/174, train_loss: 14.6018, time taken: 1.7800986766815186s\n",
      "125/174, train_loss: 24.3397, time taken: 1.8789265155792236s\n",
      "126/174, train_loss: 16.3594, time taken: 1.7306311130523682s\n",
      "127/174, train_loss: 15.6116, time taken: 1.8080990314483643s\n",
      "128/174, train_loss: 16.5633, time taken: 1.7861757278442383s\n",
      "129/174, train_loss: 18.0965, time taken: 1.7903778553009033s\n",
      "130/174, train_loss: 16.4826, time taken: 1.8726747035980225s\n",
      "131/174, train_loss: 16.5107, time taken: 2.190789222717285s\n",
      "132/174, train_loss: 16.2547, time taken: 2.0349926948547363s\n",
      "133/174, train_loss: 18.3486, time taken: 1.86905837059021s\n",
      "134/174, train_loss: 16.6885, time taken: 2.021618604660034s\n",
      "135/174, train_loss: 18.9018, time taken: 2.1049413681030273s\n",
      "136/174, train_loss: 21.7341, time taken: 1.8094768524169922s\n",
      "137/174, train_loss: 18.5558, time taken: 2.004183769226074s\n",
      "138/174, train_loss: 20.1502, time taken: 1.8222641944885254s\n",
      "139/174, train_loss: 18.4092, time taken: 1.6238951683044434s\n",
      "140/174, train_loss: 16.3175, time taken: 1.9835219383239746s\n",
      "141/174, train_loss: 18.2901, time taken: 1.9360299110412598s\n",
      "142/174, train_loss: 14.0441, time taken: 1.7986609935760498s\n",
      "143/174, train_loss: 19.0197, time taken: 1.7309808731079102s\n",
      "144/174, train_loss: 12.5710, time taken: 1.701350212097168s\n",
      "145/174, train_loss: 17.7179, time taken: 1.6874940395355225s\n",
      "146/174, train_loss: 16.7034, time taken: 1.6931321620941162s\n",
      "147/174, train_loss: 15.3932, time taken: 1.677643060684204s\n",
      "148/174, train_loss: 16.1484, time taken: 1.8958940505981445s\n",
      "149/174, train_loss: 14.5515, time taken: 1.892686128616333s\n",
      "150/174, train_loss: 19.0903, time taken: 1.8985884189605713s\n",
      "151/174, train_loss: 16.3402, time taken: 1.7935245037078857s\n",
      "152/174, train_loss: 21.7783, time taken: 1.7042393684387207s\n",
      "153/174, train_loss: 14.0064, time taken: 1.9199979305267334s\n",
      "154/174, train_loss: 18.9666, time taken: 2.20638108253479s\n",
      "155/174, train_loss: 17.2152, time taken: 1.8562850952148438s\n",
      "156/174, train_loss: 18.0831, time taken: 1.9408605098724365s\n",
      "157/174, train_loss: 16.2938, time taken: 1.931370735168457s\n",
      "158/174, train_loss: 15.2424, time taken: 1.888983964920044s\n",
      "159/174, train_loss: 16.3226, time taken: 1.8091943264007568s\n",
      "160/174, train_loss: 20.0663, time taken: 2.069507598876953s\n",
      "161/174, train_loss: 17.7337, time taken: 1.7972581386566162s\n",
      "162/174, train_loss: 19.5887, time taken: 1.8936374187469482s\n",
      "163/174, train_loss: 16.0861, time taken: 2.0202486515045166s\n",
      "164/174, train_loss: 13.3866, time taken: 1.7978410720825195s\n",
      "165/174, train_loss: 20.5071, time taken: 1.7777321338653564s\n",
      "166/174, train_loss: 17.9500, time taken: 1.7926218509674072s\n",
      "167/174, train_loss: 18.0512, time taken: 1.9185855388641357s\n",
      "168/174, train_loss: 14.1764, time taken: 1.9717204570770264s\n",
      "169/174, train_loss: 14.8214, time taken: 1.6942389011383057s\n",
      "170/174, train_loss: 15.3925, time taken: 2.009704351425171s\n",
      "171/174, train_loss: 17.8317, time taken: 1.9196910858154297s\n",
      "172/174, train_loss: 15.8260, time taken: 1.6929988861083984s\n",
      "173/174, train_loss: 13.7188, time taken: 1.8914244174957275s\n",
      "174/174, train_loss: 17.6775, time taken: 1.6930885314941406s\n",
      "175/174, train_loss: 15.6830, time taken: 1.2977917194366455s\n",
      "epoch 35 average loss: 17.3315\n",
      "Entering Validation for epoch: 35\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 35 Validation avg loss: 13.8237, time taken: 0.9637539386749268s\n",
      "----------\n",
      "epoch 36/2000\n",
      "1/174, train_loss: 17.6861, time taken: 8.17891240119934s\n",
      "2/174, train_loss: 17.9759, time taken: 2.2151567935943604s\n",
      "3/174, train_loss: 13.3108, time taken: 1.9926564693450928s\n",
      "4/174, train_loss: 17.1647, time taken: 1.8903250694274902s\n",
      "5/174, train_loss: 19.2936, time taken: 1.8636887073516846s\n",
      "6/174, train_loss: 15.2551, time taken: 1.747645616531372s\n",
      "7/174, train_loss: 14.5867, time taken: 1.786886215209961s\n",
      "8/174, train_loss: 18.0228, time taken: 2.091925859451294s\n",
      "9/174, train_loss: 16.5522, time taken: 1.7874553203582764s\n",
      "10/174, train_loss: 16.7948, time taken: 1.690746784210205s\n",
      "11/174, train_loss: 15.3471, time taken: 2.1208982467651367s\n",
      "12/174, train_loss: 16.7794, time taken: 1.8743369579315186s\n",
      "13/174, train_loss: 16.5979, time taken: 1.91060471534729s\n",
      "14/174, train_loss: 19.9747, time taken: 3.7972447872161865s\n",
      "15/174, train_loss: 17.6546, time taken: 1.9666979312896729s\n",
      "16/174, train_loss: 15.0658, time taken: 1.9103126525878906s\n",
      "17/174, train_loss: 17.9063, time taken: 2.017667293548584s\n",
      "18/174, train_loss: 12.8815, time taken: 2.3901612758636475s\n",
      "19/174, train_loss: 21.2221, time taken: 1.8969616889953613s\n",
      "20/174, train_loss: 19.5165, time taken: 2.0970242023468018s\n",
      "21/174, train_loss: 19.6517, time taken: 2.279982328414917s\n",
      "22/174, train_loss: 19.1011, time taken: 2.1083457469940186s\n",
      "23/174, train_loss: 18.1407, time taken: 2.095158338546753s\n",
      "24/174, train_loss: 15.3506, time taken: 1.8853816986083984s\n",
      "25/174, train_loss: 17.9661, time taken: 1.913644552230835s\n",
      "26/174, train_loss: 19.8786, time taken: 2.3149850368499756s\n",
      "27/174, train_loss: 16.9043, time taken: 1.8895084857940674s\n",
      "28/174, train_loss: 21.4756, time taken: 2.1819958686828613s\n",
      "29/174, train_loss: 15.1046, time taken: 2.1812703609466553s\n",
      "30/174, train_loss: 20.3097, time taken: 1.790719985961914s\n",
      "31/174, train_loss: 16.9012, time taken: 1.7219974994659424s\n",
      "32/174, train_loss: 19.1124, time taken: 2.1056764125823975s\n",
      "33/174, train_loss: 15.2628, time taken: 1.890866994857788s\n",
      "34/174, train_loss: 15.9715, time taken: 1.8840250968933105s\n",
      "35/174, train_loss: 20.5444, time taken: 2.1151492595672607s\n",
      "36/174, train_loss: 14.2185, time taken: 1.9652001857757568s\n",
      "37/174, train_loss: 17.9037, time taken: 1.7278292179107666s\n",
      "38/174, train_loss: 16.1654, time taken: 1.9751067161560059s\n",
      "39/174, train_loss: 18.6150, time taken: 1.9126219749450684s\n",
      "40/174, train_loss: 16.8477, time taken: 1.8077731132507324s\n",
      "41/174, train_loss: 13.5555, time taken: 1.7461822032928467s\n",
      "42/174, train_loss: 15.5231, time taken: 1.8527393341064453s\n",
      "43/174, train_loss: 11.6673, time taken: 1.9254436492919922s\n",
      "44/174, train_loss: 16.6868, time taken: 1.8716895580291748s\n",
      "45/174, train_loss: 12.4999, time taken: 1.715423345565796s\n",
      "46/174, train_loss: 22.3003, time taken: 1.9814558029174805s\n",
      "47/174, train_loss: 13.8757, time taken: 1.8929870128631592s\n",
      "48/174, train_loss: 19.3763, time taken: 1.8274827003479004s\n",
      "49/174, train_loss: 24.1779, time taken: 2.0977113246917725s\n",
      "50/174, train_loss: 16.4807, time taken: 1.6831974983215332s\n",
      "51/174, train_loss: 17.5794, time taken: 2.00754714012146s\n",
      "52/174, train_loss: 17.5213, time taken: 1.800138235092163s\n",
      "53/174, train_loss: 16.8442, time taken: 1.7909705638885498s\n",
      "54/174, train_loss: 15.3962, time taken: 1.8615992069244385s\n",
      "55/174, train_loss: 20.5312, time taken: 1.9319300651550293s\n",
      "56/174, train_loss: 16.5949, time taken: 1.7010676860809326s\n",
      "57/174, train_loss: 15.1022, time taken: 1.7777009010314941s\n",
      "58/174, train_loss: 21.0084, time taken: 1.8690335750579834s\n",
      "59/174, train_loss: 18.2451, time taken: 3.403364896774292s\n",
      "60/174, train_loss: 16.6663, time taken: 1.8261022567749023s\n",
      "61/174, train_loss: 22.9488, time taken: 1.8985111713409424s\n",
      "62/174, train_loss: 15.5365, time taken: 1.775651454925537s\n",
      "63/174, train_loss: 19.5396, time taken: 1.8091273307800293s\n",
      "64/174, train_loss: 18.4583, time taken: 1.9834136962890625s\n",
      "65/174, train_loss: 17.8397, time taken: 1.9089007377624512s\n",
      "66/174, train_loss: 15.6079, time taken: 2.076796531677246s\n",
      "67/174, train_loss: 17.0389, time taken: 1.8163013458251953s\n",
      "68/174, train_loss: 18.7709, time taken: 1.8876054286956787s\n",
      "69/174, train_loss: 19.1867, time taken: 1.8890955448150635s\n",
      "70/174, train_loss: 16.1611, time taken: 1.9056475162506104s\n",
      "71/174, train_loss: 18.8527, time taken: 1.894845962524414s\n",
      "72/174, train_loss: 19.0570, time taken: 2.3012778759002686s\n",
      "73/174, train_loss: 17.5099, time taken: 1.9188573360443115s\n",
      "74/174, train_loss: 17.1395, time taken: 2.017442226409912s\n",
      "75/174, train_loss: 18.7062, time taken: 2.2293782234191895s\n",
      "76/174, train_loss: 16.9000, time taken: 1.7235157489776611s\n",
      "77/174, train_loss: 15.4922, time taken: 1.7887110710144043s\n",
      "78/174, train_loss: 18.8695, time taken: 1.9227514266967773s\n",
      "79/174, train_loss: 19.2827, time taken: 1.9754748344421387s\n",
      "80/174, train_loss: 16.0566, time taken: 1.9870963096618652s\n",
      "81/174, train_loss: 15.9302, time taken: 1.7084529399871826s\n",
      "82/174, train_loss: 18.6249, time taken: 1.869490385055542s\n",
      "83/174, train_loss: 12.8312, time taken: 1.8468356132507324s\n",
      "84/174, train_loss: 19.9825, time taken: 2.0244662761688232s\n",
      "85/174, train_loss: 16.9531, time taken: 1.7891559600830078s\n",
      "86/174, train_loss: 16.9689, time taken: 2.222369909286499s\n",
      "87/174, train_loss: 19.5672, time taken: 2.0158638954162598s\n",
      "88/174, train_loss: 17.1583, time taken: 3.3806183338165283s\n",
      "89/174, train_loss: 15.7841, time taken: 2.008798122406006s\n",
      "90/174, train_loss: 13.3616, time taken: 1.7850453853607178s\n",
      "91/174, train_loss: 18.1052, time taken: 1.7996087074279785s\n",
      "92/174, train_loss: 16.4335, time taken: 2.111602306365967s\n",
      "93/174, train_loss: 13.0219, time taken: 2.1896588802337646s\n",
      "94/174, train_loss: 19.6523, time taken: 1.900071144104004s\n",
      "95/174, train_loss: 15.0484, time taken: 1.7099418640136719s\n",
      "96/174, train_loss: 16.7367, time taken: 1.9950757026672363s\n",
      "97/174, train_loss: 20.8223, time taken: 2.008574962615967s\n",
      "98/174, train_loss: 17.1375, time taken: 1.8370068073272705s\n",
      "99/174, train_loss: 16.8263, time taken: 1.8641445636749268s\n",
      "100/174, train_loss: 22.9565, time taken: 2.0049824714660645s\n",
      "101/174, train_loss: 14.6051, time taken: 2.0279178619384766s\n",
      "102/174, train_loss: 14.9144, time taken: 1.9571104049682617s\n",
      "103/174, train_loss: 15.1830, time taken: 2.031494140625s\n",
      "104/174, train_loss: 14.1756, time taken: 2.0706822872161865s\n",
      "105/174, train_loss: 12.1010, time taken: 1.9107553958892822s\n",
      "106/174, train_loss: 17.0013, time taken: 1.870164155960083s\n",
      "107/174, train_loss: 13.3539, time taken: 1.7287328243255615s\n",
      "108/174, train_loss: 19.2319, time taken: 1.7848031520843506s\n",
      "109/174, train_loss: 16.4904, time taken: 1.7189772129058838s\n",
      "110/174, train_loss: 13.5308, time taken: 1.7669682502746582s\n",
      "111/174, train_loss: 15.8142, time taken: 2.105332851409912s\n",
      "112/174, train_loss: 16.5715, time taken: 1.983675241470337s\n",
      "113/174, train_loss: 18.4811, time taken: 1.8030004501342773s\n",
      "114/174, train_loss: 17.1136, time taken: 1.9017329216003418s\n",
      "115/174, train_loss: 19.0464, time taken: 1.9997131824493408s\n",
      "116/174, train_loss: 17.5658, time taken: 2.005894184112549s\n",
      "117/174, train_loss: 15.3151, time taken: 1.8718647956848145s\n",
      "118/174, train_loss: 17.9942, time taken: 1.8247332572937012s\n",
      "119/174, train_loss: 18.9564, time taken: 1.9889745712280273s\n",
      "120/174, train_loss: 17.4722, time taken: 1.8115839958190918s\n",
      "121/174, train_loss: 17.3294, time taken: 1.9746224880218506s\n",
      "122/174, train_loss: 14.9338, time taken: 1.822911024093628s\n",
      "123/174, train_loss: 23.7206, time taken: 1.7065818309783936s\n",
      "124/174, train_loss: 20.5234, time taken: 2.104368209838867s\n",
      "125/174, train_loss: 15.1655, time taken: 1.80094313621521s\n",
      "126/174, train_loss: 18.7548, time taken: 1.798391342163086s\n",
      "127/174, train_loss: 20.5802, time taken: 2.0005199909210205s\n",
      "128/174, train_loss: 13.7038, time taken: 2.0942609310150146s\n",
      "129/174, train_loss: 19.4347, time taken: 1.7061305046081543s\n",
      "130/174, train_loss: 12.5694, time taken: 1.9918792247772217s\n",
      "131/174, train_loss: 15.4041, time taken: 1.9689662456512451s\n",
      "132/174, train_loss: 16.9056, time taken: 1.8009684085845947s\n",
      "133/174, train_loss: 20.2952, time taken: 1.7010066509246826s\n",
      "134/174, train_loss: 15.5075, time taken: 2.1331682205200195s\n",
      "135/174, train_loss: 15.9221, time taken: 1.8192830085754395s\n",
      "136/174, train_loss: 16.9112, time taken: 1.811875343322754s\n",
      "137/174, train_loss: 19.7657, time taken: 1.7815001010894775s\n",
      "138/174, train_loss: 16.3281, time taken: 1.8140299320220947s\n",
      "139/174, train_loss: 18.4269, time taken: 1.9896256923675537s\n",
      "140/174, train_loss: 16.9622, time taken: 2.0984039306640625s\n",
      "141/174, train_loss: 18.4543, time taken: 1.7748451232910156s\n",
      "142/174, train_loss: 14.6956, time taken: 1.9083266258239746s\n",
      "143/174, train_loss: 21.0607, time taken: 1.91778564453125s\n",
      "144/174, train_loss: 22.0786, time taken: 1.6959521770477295s\n",
      "145/174, train_loss: 19.9268, time taken: 2.203834056854248s\n",
      "146/174, train_loss: 18.6332, time taken: 1.8790290355682373s\n",
      "147/174, train_loss: 16.3248, time taken: 1.786799669265747s\n",
      "148/174, train_loss: 13.3606, time taken: 1.7853267192840576s\n",
      "149/174, train_loss: 19.4251, time taken: 1.7975316047668457s\n",
      "150/174, train_loss: 14.9013, time taken: 1.702817440032959s\n",
      "151/174, train_loss: 16.7170, time taken: 1.7023918628692627s\n",
      "152/174, train_loss: 17.9662, time taken: 2.299724578857422s\n",
      "153/174, train_loss: 16.5368, time taken: 1.9141757488250732s\n",
      "154/174, train_loss: 14.6714, time taken: 1.8062481880187988s\n",
      "155/174, train_loss: 16.2343, time taken: 1.936570644378662s\n",
      "156/174, train_loss: 17.9693, time taken: 1.6978964805603027s\n",
      "157/174, train_loss: 13.8219, time taken: 1.8032567501068115s\n",
      "158/174, train_loss: 18.0725, time taken: 1.7693724632263184s\n",
      "159/174, train_loss: 16.4441, time taken: 1.7080211639404297s\n",
      "160/174, train_loss: 18.9071, time taken: 1.7713191509246826s\n",
      "161/174, train_loss: 17.4188, time taken: 1.710871696472168s\n",
      "162/174, train_loss: 19.0874, time taken: 1.8186843395233154s\n",
      "163/174, train_loss: 15.7389, time taken: 1.9923231601715088s\n",
      "164/174, train_loss: 20.4474, time taken: 1.769540786743164s\n",
      "165/174, train_loss: 16.1823, time taken: 1.9297540187835693s\n",
      "166/174, train_loss: 17.9466, time taken: 1.8668949604034424s\n",
      "167/174, train_loss: 17.5108, time taken: 1.792525053024292s\n",
      "168/174, train_loss: 18.1257, time taken: 1.904067039489746s\n",
      "169/174, train_loss: 16.3037, time taken: 1.7145161628723145s\n",
      "170/174, train_loss: 18.4221, time taken: 1.9912559986114502s\n",
      "171/174, train_loss: 14.4656, time taken: 1.7824211120605469s\n",
      "172/174, train_loss: 17.5924, time taken: 1.7333791255950928s\n",
      "173/174, train_loss: 19.8019, time taken: 1.9257526397705078s\n",
      "174/174, train_loss: 15.7364, time taken: 1.898841381072998s\n",
      "175/174, train_loss: 11.4814, time taken: 1.4842298030853271s\n",
      "epoch 36 average loss: 17.2345\n",
      "----------\n",
      "epoch 37/2000\n",
      "1/174, train_loss: 14.4873, time taken: 8.082815170288086s\n",
      "2/174, train_loss: 17.0501, time taken: 2.3792805671691895s\n",
      "3/174, train_loss: 16.3777, time taken: 1.9890618324279785s\n",
      "4/174, train_loss: 16.3658, time taken: 1.989811897277832s\n",
      "5/174, train_loss: 15.2696, time taken: 2.11236310005188s\n",
      "6/174, train_loss: 17.7115, time taken: 2.37463641166687s\n",
      "7/174, train_loss: 11.9995, time taken: 1.8970839977264404s\n",
      "8/174, train_loss: 14.8608, time taken: 2.01918625831604s\n",
      "9/174, train_loss: 16.5653, time taken: 2.098649024963379s\n",
      "10/174, train_loss: 17.2443, time taken: 1.9012081623077393s\n",
      "11/174, train_loss: 17.4368, time taken: 1.9879140853881836s\n",
      "12/174, train_loss: 13.6698, time taken: 2.1003589630126953s\n",
      "13/174, train_loss: 15.4652, time taken: 1.902618169784546s\n",
      "14/174, train_loss: 17.5347, time taken: 2.275663137435913s\n",
      "15/174, train_loss: 12.1542, time taken: 2.4229040145874023s\n",
      "16/174, train_loss: 14.0762, time taken: 2.0750017166137695s\n",
      "17/174, train_loss: 19.1317, time taken: 2.207341432571411s\n",
      "18/174, train_loss: 18.7418, time taken: 2.2017669677734375s\n",
      "19/174, train_loss: 11.7578, time taken: 1.9751572608947754s\n",
      "20/174, train_loss: 15.6689, time taken: 2.0921847820281982s\n",
      "21/174, train_loss: 17.1175, time taken: 2.013469934463501s\n",
      "22/174, train_loss: 16.5284, time taken: 1.8068158626556396s\n",
      "23/174, train_loss: 18.4473, time taken: 1.8913617134094238s\n",
      "24/174, train_loss: 13.3615, time taken: 1.9763767719268799s\n",
      "25/174, train_loss: 16.5775, time taken: 1.8144092559814453s\n",
      "26/174, train_loss: 16.3247, time taken: 1.8910439014434814s\n",
      "27/174, train_loss: 16.4814, time taken: 1.8180289268493652s\n",
      "28/174, train_loss: 16.1019, time taken: 1.8001675605773926s\n",
      "29/174, train_loss: 15.4533, time taken: 1.9054105281829834s\n",
      "30/174, train_loss: 19.1220, time taken: 1.898674726486206s\n",
      "31/174, train_loss: 16.9358, time taken: 1.785079002380371s\n",
      "32/174, train_loss: 21.8520, time taken: 1.8343560695648193s\n",
      "33/174, train_loss: 17.8214, time taken: 1.804062843322754s\n",
      "34/174, train_loss: 19.1218, time taken: 2.000331163406372s\n",
      "35/174, train_loss: 17.0521, time taken: 2.01932954788208s\n",
      "36/174, train_loss: 19.1249, time taken: 1.8006584644317627s\n",
      "37/174, train_loss: 18.3082, time taken: 1.8591818809509277s\n",
      "38/174, train_loss: 15.1233, time taken: 2.0185720920562744s\n",
      "39/174, train_loss: 21.3541, time taken: 1.9075922966003418s\n",
      "40/174, train_loss: 18.8029, time taken: 1.9116315841674805s\n",
      "41/174, train_loss: 21.3509, time taken: 1.7043726444244385s\n",
      "42/174, train_loss: 14.1872, time taken: 1.9924564361572266s\n",
      "43/174, train_loss: 17.5243, time taken: 2.229922294616699s\n",
      "44/174, train_loss: 18.5093, time taken: 1.9771981239318848s\n",
      "45/174, train_loss: 12.9622, time taken: 1.99241304397583s\n",
      "46/174, train_loss: 17.0383, time taken: 1.9982848167419434s\n",
      "47/174, train_loss: 18.5642, time taken: 1.7109959125518799s\n",
      "48/174, train_loss: 19.0810, time taken: 2.0971789360046387s\n",
      "49/174, train_loss: 15.0567, time taken: 2.0871670246124268s\n",
      "50/174, train_loss: 20.5432, time taken: 2.0021328926086426s\n",
      "51/174, train_loss: 17.2067, time taken: 2.10349178314209s\n",
      "52/174, train_loss: 16.7475, time taken: 2.112410068511963s\n",
      "53/174, train_loss: 19.4171, time taken: 1.6990301609039307s\n",
      "54/174, train_loss: 19.6651, time taken: 1.8093700408935547s\n",
      "55/174, train_loss: 20.4166, time taken: 1.8086788654327393s\n",
      "56/174, train_loss: 18.8883, time taken: 1.847038984298706s\n",
      "57/174, train_loss: 12.6657, time taken: 1.904280424118042s\n",
      "58/174, train_loss: 12.6010, time taken: 1.911836862564087s\n",
      "59/174, train_loss: 16.6876, time taken: 1.877504825592041s\n",
      "60/174, train_loss: 11.5740, time taken: 1.9145472049713135s\n",
      "61/174, train_loss: 18.5283, time taken: 1.9149365425109863s\n",
      "62/174, train_loss: 17.4605, time taken: 1.792264699935913s\n",
      "63/174, train_loss: 18.7273, time taken: 1.778097152709961s\n",
      "64/174, train_loss: 15.7987, time taken: 1.9896931648254395s\n",
      "65/174, train_loss: 20.3844, time taken: 2.1042733192443848s\n",
      "66/174, train_loss: 16.7851, time taken: 2.083315372467041s\n",
      "67/174, train_loss: 18.6151, time taken: 1.8224408626556396s\n",
      "68/174, train_loss: 17.7791, time taken: 1.9683012962341309s\n",
      "69/174, train_loss: 12.8556, time taken: 1.7170419692993164s\n",
      "70/174, train_loss: 11.8024, time taken: 2.0760574340820312s\n",
      "71/174, train_loss: 19.2381, time taken: 2.025461196899414s\n",
      "72/174, train_loss: 20.1359, time taken: 1.9808354377746582s\n",
      "73/174, train_loss: 15.5548, time taken: 2.305344343185425s\n",
      "74/174, train_loss: 15.0603, time taken: 1.892259120941162s\n",
      "75/174, train_loss: 20.0944, time taken: 1.8238699436187744s\n",
      "76/174, train_loss: 20.4514, time taken: 1.7183668613433838s\n",
      "77/174, train_loss: 19.4408, time taken: 1.6842803955078125s\n",
      "78/174, train_loss: 16.9259, time taken: 1.7900006771087646s\n",
      "79/174, train_loss: 16.3446, time taken: 1.8149495124816895s\n",
      "80/174, train_loss: 20.2289, time taken: 1.816497564315796s\n",
      "81/174, train_loss: 18.2023, time taken: 2.092033624649048s\n",
      "82/174, train_loss: 18.5486, time taken: 1.8696513175964355s\n",
      "83/174, train_loss: 22.2648, time taken: 2.0010647773742676s\n",
      "84/174, train_loss: 20.5579, time taken: 2.1229870319366455s\n",
      "85/174, train_loss: 14.2044, time taken: 2.148012638092041s\n",
      "86/174, train_loss: 20.3278, time taken: 1.898902177810669s\n",
      "87/174, train_loss: 15.9275, time taken: 2.0380008220672607s\n",
      "88/174, train_loss: 19.4832, time taken: 2.389960289001465s\n",
      "89/174, train_loss: 14.8815, time taken: 1.986192226409912s\n",
      "90/174, train_loss: 14.6206, time taken: 2.1056816577911377s\n",
      "91/174, train_loss: 21.7989, time taken: 2.1685657501220703s\n",
      "92/174, train_loss: 20.1027, time taken: 1.8157951831817627s\n",
      "93/174, train_loss: 17.8190, time taken: 1.8769371509552002s\n",
      "94/174, train_loss: 15.5933, time taken: 1.8281245231628418s\n",
      "95/174, train_loss: 14.8525, time taken: 2.120197057723999s\n",
      "96/174, train_loss: 18.1393, time taken: 2.1225829124450684s\n",
      "97/174, train_loss: 16.7843, time taken: 1.7987279891967773s\n",
      "98/174, train_loss: 16.9849, time taken: 1.835190773010254s\n",
      "99/174, train_loss: 15.9349, time taken: 1.7917108535766602s\n",
      "100/174, train_loss: 17.0030, time taken: 1.8925766944885254s\n",
      "101/174, train_loss: 16.8192, time taken: 1.8140370845794678s\n",
      "102/174, train_loss: 20.2490, time taken: 2.470681667327881s\n",
      "103/174, train_loss: 19.0793, time taken: 1.9989171028137207s\n",
      "104/174, train_loss: 20.1381, time taken: 1.9811768531799316s\n",
      "105/174, train_loss: 17.7111, time taken: 1.7149879932403564s\n",
      "106/174, train_loss: 18.0107, time taken: 1.7911450862884521s\n",
      "107/174, train_loss: 16.5104, time taken: 1.7222580909729004s\n",
      "108/174, train_loss: 19.4982, time taken: 1.8668546676635742s\n",
      "109/174, train_loss: 15.1308, time taken: 1.9219093322753906s\n",
      "110/174, train_loss: 9.4694, time taken: 1.7129511833190918s\n",
      "111/174, train_loss: 15.2058, time taken: 1.8308637142181396s\n",
      "112/174, train_loss: 15.2234, time taken: 1.7721145153045654s\n",
      "113/174, train_loss: 21.8077, time taken: 1.7156336307525635s\n",
      "114/174, train_loss: 16.8867, time taken: 1.7277555465698242s\n",
      "115/174, train_loss: 14.3980, time taken: 2.100470781326294s\n",
      "116/174, train_loss: 18.0298, time taken: 1.7114861011505127s\n",
      "117/174, train_loss: 12.9827, time taken: 3.638543128967285s\n",
      "118/174, train_loss: 20.4374, time taken: 1.8838579654693604s\n",
      "119/174, train_loss: 18.2966, time taken: 2.0855531692504883s\n",
      "120/174, train_loss: 18.6291, time taken: 1.7840895652770996s\n",
      "121/174, train_loss: 17.3814, time taken: 1.8977866172790527s\n",
      "122/174, train_loss: 14.2903, time taken: 2.1059162616729736s\n",
      "123/174, train_loss: 14.7273, time taken: 1.8214731216430664s\n",
      "124/174, train_loss: 16.1450, time taken: 1.9671509265899658s\n",
      "125/174, train_loss: 23.9566, time taken: 1.8915247917175293s\n",
      "126/174, train_loss: 14.8470, time taken: 1.808760166168213s\n",
      "127/174, train_loss: 19.2324, time taken: 1.7787866592407227s\n",
      "128/174, train_loss: 20.3275, time taken: 1.7366409301757812s\n",
      "129/174, train_loss: 16.8850, time taken: 1.9028799533843994s\n",
      "130/174, train_loss: 15.2512, time taken: 1.9405090808868408s\n",
      "131/174, train_loss: 17.6089, time taken: 1.9437482357025146s\n",
      "132/174, train_loss: 14.8805, time taken: 1.8526418209075928s\n",
      "133/174, train_loss: 16.3847, time taken: 1.8032257556915283s\n",
      "134/174, train_loss: 14.4204, time taken: 1.9281370639801025s\n",
      "135/174, train_loss: 19.0628, time taken: 1.8806114196777344s\n",
      "136/174, train_loss: 17.9838, time taken: 2.0086443424224854s\n",
      "137/174, train_loss: 17.9123, time taken: 2.09208607673645s\n",
      "138/174, train_loss: 11.8121, time taken: 2.0653953552246094s\n",
      "139/174, train_loss: 16.2574, time taken: 1.8306529521942139s\n",
      "140/174, train_loss: 15.8612, time taken: 1.825998306274414s\n",
      "141/174, train_loss: 16.1567, time taken: 1.7981178760528564s\n",
      "142/174, train_loss: 18.8787, time taken: 1.8085439205169678s\n",
      "143/174, train_loss: 21.8485, time taken: 1.8804643154144287s\n",
      "144/174, train_loss: 20.6830, time taken: 1.8923413753509521s\n",
      "145/174, train_loss: 20.8317, time taken: 1.7819361686706543s\n",
      "146/174, train_loss: 17.6926, time taken: 1.7273848056793213s\n",
      "147/174, train_loss: 18.3010, time taken: 1.8552279472351074s\n",
      "148/174, train_loss: 17.6576, time taken: 1.809908151626587s\n",
      "149/174, train_loss: 15.6887, time taken: 1.89935302734375s\n",
      "150/174, train_loss: 15.4136, time taken: 1.822861909866333s\n",
      "151/174, train_loss: 19.5907, time taken: 1.636394739151001s\n",
      "152/174, train_loss: 19.8479, time taken: 1.8002917766571045s\n",
      "153/174, train_loss: 16.1980, time taken: 1.6967535018920898s\n",
      "154/174, train_loss: 18.2946, time taken: 1.7944397926330566s\n",
      "155/174, train_loss: 19.5341, time taken: 1.9024560451507568s\n",
      "156/174, train_loss: 18.1801, time taken: 1.8782503604888916s\n",
      "157/174, train_loss: 12.3388, time taken: 1.6986730098724365s\n",
      "158/174, train_loss: 18.0358, time taken: 1.6802806854248047s\n",
      "159/174, train_loss: 15.1843, time taken: 1.7152585983276367s\n",
      "160/174, train_loss: 15.8274, time taken: 1.8829550743103027s\n",
      "161/174, train_loss: 20.1375, time taken: 1.8010406494140625s\n",
      "162/174, train_loss: 13.6497, time taken: 1.7987697124481201s\n",
      "163/174, train_loss: 17.5948, time taken: 1.7922523021697998s\n",
      "164/174, train_loss: 17.8470, time taken: 3.608944892883301s\n",
      "165/174, train_loss: 18.4038, time taken: 1.9657599925994873s\n",
      "166/174, train_loss: 16.9104, time taken: 1.9988796710968018s\n",
      "167/174, train_loss: 18.7693, time taken: 1.823335886001587s\n",
      "168/174, train_loss: 16.8628, time taken: 1.790057897567749s\n",
      "169/174, train_loss: 16.2795, time taken: 1.6820123195648193s\n",
      "170/174, train_loss: 17.0936, time taken: 1.9086408615112305s\n",
      "171/174, train_loss: 15.1115, time taken: 1.6927411556243896s\n",
      "172/174, train_loss: 16.4841, time taken: 1.6170947551727295s\n",
      "173/174, train_loss: 14.3811, time taken: 1.7683732509613037s\n",
      "174/174, train_loss: 17.8319, time taken: 1.713447093963623s\n",
      "175/174, train_loss: 14.7355, time taken: 1.6691675186157227s\n",
      "epoch 37 average loss: 17.1402\n",
      "Entering Validation for epoch: 37\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 37 Validation avg loss: 15.9312, time taken: 0.980201005935669s\n",
      "----------\n",
      "epoch 38/2000\n",
      "1/174, train_loss: 13.1735, time taken: 8.25814938545227s\n",
      "2/174, train_loss: 17.5415, time taken: 2.3038055896759033s\n",
      "3/174, train_loss: 19.7535, time taken: 1.9800360202789307s\n",
      "4/174, train_loss: 14.8243, time taken: 1.8134434223175049s\n",
      "5/174, train_loss: 17.1860, time taken: 1.829244613647461s\n",
      "6/174, train_loss: 20.2465, time taken: 1.9794294834136963s\n",
      "7/174, train_loss: 17.5837, time taken: 2.29524564743042s\n",
      "8/174, train_loss: 11.8452, time taken: 1.9202394485473633s\n",
      "9/174, train_loss: 17.5583, time taken: 2.0714809894561768s\n",
      "10/174, train_loss: 16.0431, time taken: 1.8171653747558594s\n",
      "11/174, train_loss: 15.2552, time taken: 2.1993823051452637s\n",
      "12/174, train_loss: 20.5820, time taken: 1.7960913181304932s\n",
      "13/174, train_loss: 23.3819, time taken: 1.861546277999878s\n",
      "14/174, train_loss: 18.4611, time taken: 1.9998116493225098s\n",
      "15/174, train_loss: 18.5119, time taken: 1.9366047382354736s\n",
      "16/174, train_loss: 17.1136, time taken: 1.8006186485290527s\n",
      "17/174, train_loss: 15.7570, time taken: 1.8960554599761963s\n",
      "18/174, train_loss: 16.5535, time taken: 1.8871662616729736s\n",
      "19/174, train_loss: 15.8294, time taken: 2.0962891578674316s\n",
      "20/174, train_loss: 15.1495, time taken: 1.9843714237213135s\n",
      "21/174, train_loss: 14.5407, time taken: 1.7726240158081055s\n",
      "22/174, train_loss: 16.1540, time taken: 1.7220475673675537s\n",
      "23/174, train_loss: 20.4136, time taken: 1.7944905757904053s\n",
      "24/174, train_loss: 18.8180, time taken: 1.872413158416748s\n",
      "25/174, train_loss: 16.2137, time taken: 1.9020977020263672s\n",
      "26/174, train_loss: 15.2297, time taken: 1.813166618347168s\n",
      "27/174, train_loss: 11.8523, time taken: 1.8155288696289062s\n",
      "28/174, train_loss: 16.6803, time taken: 1.940943956375122s\n",
      "29/174, train_loss: 20.1435, time taken: 1.9738869667053223s\n",
      "30/174, train_loss: 15.3176, time taken: 1.7222235202789307s\n",
      "31/174, train_loss: 21.8625, time taken: 1.7887263298034668s\n",
      "32/174, train_loss: 14.1141, time taken: 2.173471689224243s\n",
      "33/174, train_loss: 17.2396, time taken: 1.91987943649292s\n",
      "34/174, train_loss: 15.5383, time taken: 1.8753032684326172s\n",
      "35/174, train_loss: 13.1382, time taken: 1.7075233459472656s\n",
      "36/174, train_loss: 12.5069, time taken: 2.1013243198394775s\n",
      "37/174, train_loss: 18.3027, time taken: 1.9985482692718506s\n",
      "38/174, train_loss: 15.3882, time taken: 1.8660204410552979s\n",
      "39/174, train_loss: 18.4255, time taken: 1.698815107345581s\n",
      "40/174, train_loss: 14.2266, time taken: 2.019031286239624s\n",
      "41/174, train_loss: 21.2738, time taken: 2.0032193660736084s\n",
      "42/174, train_loss: 19.9826, time taken: 1.8911371231079102s\n",
      "43/174, train_loss: 10.9847, time taken: 1.805588960647583s\n",
      "44/174, train_loss: 16.1283, time taken: 2.0949718952178955s\n",
      "45/174, train_loss: 13.5009, time taken: 1.7873237133026123s\n",
      "46/174, train_loss: 15.9344, time taken: 2.0806407928466797s\n",
      "47/174, train_loss: 14.3612, time taken: 1.890315055847168s\n",
      "48/174, train_loss: 14.1147, time taken: 1.8281595706939697s\n",
      "49/174, train_loss: 14.8855, time taken: 2.0642824172973633s\n",
      "50/174, train_loss: 19.6094, time taken: 2.1074960231781006s\n",
      "51/174, train_loss: 16.0461, time taken: 1.8003337383270264s\n",
      "52/174, train_loss: 18.1757, time taken: 2.003523111343384s\n",
      "53/174, train_loss: 19.5931, time taken: 2.0881075859069824s\n",
      "54/174, train_loss: 19.1403, time taken: 1.8949809074401855s\n",
      "55/174, train_loss: 17.3483, time taken: 1.9841721057891846s\n",
      "56/174, train_loss: 15.8335, time taken: 2.1405413150787354s\n",
      "57/174, train_loss: 17.9669, time taken: 2.119227409362793s\n",
      "58/174, train_loss: 15.0514, time taken: 1.8091399669647217s\n",
      "59/174, train_loss: 17.0975, time taken: 2.0972349643707275s\n",
      "60/174, train_loss: 18.2747, time taken: 2.1731529235839844s\n",
      "61/174, train_loss: 15.1210, time taken: 1.8903417587280273s\n",
      "62/174, train_loss: 16.5511, time taken: 1.9030442237854004s\n",
      "63/174, train_loss: 16.9446, time taken: 1.8924503326416016s\n",
      "64/174, train_loss: 16.2517, time taken: 1.7867190837860107s\n",
      "65/174, train_loss: 16.0268, time taken: 1.7269442081451416s\n",
      "66/174, train_loss: 16.6668, time taken: 1.7958612442016602s\n",
      "67/174, train_loss: 20.0957, time taken: 2.2090742588043213s\n",
      "68/174, train_loss: 19.4572, time taken: 1.8893752098083496s\n",
      "69/174, train_loss: 15.9042, time taken: 1.8706510066986084s\n",
      "70/174, train_loss: 22.3147, time taken: 1.9211537837982178s\n",
      "71/174, train_loss: 15.1670, time taken: 2.0061848163604736s\n",
      "72/174, train_loss: 13.5216, time taken: 2.059384822845459s\n",
      "73/174, train_loss: 21.1952, time taken: 1.8142294883728027s\n",
      "74/174, train_loss: 15.7604, time taken: 1.9747414588928223s\n",
      "75/174, train_loss: 14.8160, time taken: 1.7909040451049805s\n",
      "76/174, train_loss: 17.7620, time taken: 1.7425973415374756s\n",
      "77/174, train_loss: 17.1523, time taken: 1.7715795040130615s\n",
      "78/174, train_loss: 19.8574, time taken: 1.7845206260681152s\n",
      "79/174, train_loss: 16.0857, time taken: 1.913712978363037s\n",
      "80/174, train_loss: 20.6402, time taken: 1.8091225624084473s\n",
      "81/174, train_loss: 18.0368, time taken: 2.075009346008301s\n",
      "82/174, train_loss: 15.7213, time taken: 1.7219312191009521s\n",
      "83/174, train_loss: 20.4854, time taken: 1.861018180847168s\n",
      "84/174, train_loss: 15.5179, time taken: 1.8976781368255615s\n",
      "85/174, train_loss: 18.4112, time taken: 1.704254150390625s\n",
      "86/174, train_loss: 18.0677, time taken: 1.9962892532348633s\n",
      "87/174, train_loss: 18.4167, time taken: 1.7952558994293213s\n",
      "88/174, train_loss: 16.2449, time taken: 1.9032104015350342s\n",
      "89/174, train_loss: 14.8145, time taken: 1.8253066539764404s\n",
      "90/174, train_loss: 17.3235, time taken: 1.8427512645721436s\n",
      "91/174, train_loss: 19.3534, time taken: 2.293954372406006s\n",
      "92/174, train_loss: 16.0911, time taken: 1.7661950588226318s\n",
      "93/174, train_loss: 19.4279, time taken: 2.114534854888916s\n",
      "94/174, train_loss: 22.6940, time taken: 1.8030030727386475s\n",
      "95/174, train_loss: 16.8234, time taken: 1.9742028713226318s\n",
      "96/174, train_loss: 23.1578, time taken: 1.7201261520385742s\n",
      "97/174, train_loss: 17.5698, time taken: 1.9827826023101807s\n",
      "98/174, train_loss: 16.5587, time taken: 1.9945156574249268s\n",
      "99/174, train_loss: 17.2699, time taken: 1.8830347061157227s\n",
      "100/174, train_loss: 18.2419, time taken: 1.8108067512512207s\n",
      "101/174, train_loss: 16.0241, time taken: 1.6896910667419434s\n",
      "102/174, train_loss: 17.3046, time taken: 1.8129267692565918s\n",
      "103/174, train_loss: 15.5870, time taken: 1.887007236480713s\n",
      "104/174, train_loss: 17.7518, time taken: 1.8886544704437256s\n",
      "105/174, train_loss: 19.0521, time taken: 1.9965839385986328s\n",
      "106/174, train_loss: 14.1179, time taken: 1.8190724849700928s\n",
      "107/174, train_loss: 18.7281, time taken: 1.7052063941955566s\n",
      "108/174, train_loss: 13.2187, time taken: 1.8670854568481445s\n",
      "109/174, train_loss: 17.2315, time taken: 1.9061546325683594s\n",
      "110/174, train_loss: 17.1568, time taken: 1.787968397140503s\n",
      "111/174, train_loss: 18.7679, time taken: 1.7988345623016357s\n",
      "112/174, train_loss: 19.0105, time taken: 1.681617021560669s\n",
      "113/174, train_loss: 18.8894, time taken: 1.70674467086792s\n",
      "114/174, train_loss: 14.7765, time taken: 2.234565496444702s\n",
      "115/174, train_loss: 15.7846, time taken: 1.8016326427459717s\n",
      "116/174, train_loss: 21.7122, time taken: 1.8940989971160889s\n",
      "117/174, train_loss: 14.4228, time taken: 1.8061857223510742s\n",
      "118/174, train_loss: 18.1056, time taken: 1.8308072090148926s\n",
      "119/174, train_loss: 15.7336, time taken: 1.7127091884613037s\n",
      "120/174, train_loss: 16.6366, time taken: 1.8945071697235107s\n",
      "121/174, train_loss: 19.6930, time taken: 1.889803171157837s\n",
      "122/174, train_loss: 21.3683, time taken: 1.8190181255340576s\n",
      "123/174, train_loss: 12.7716, time taken: 1.7054383754730225s\n",
      "124/174, train_loss: 19.5744, time taken: 1.9711263179779053s\n",
      "125/174, train_loss: 17.9755, time taken: 3.8064029216766357s\n",
      "126/174, train_loss: 18.4670, time taken: 2.1979663372039795s\n",
      "127/174, train_loss: 15.6111, time taken: 2.1945981979370117s\n",
      "128/174, train_loss: 18.9138, time taken: 1.9724586009979248s\n",
      "129/174, train_loss: 16.2191, time taken: 1.8906915187835693s\n",
      "130/174, train_loss: 20.7436, time taken: 1.7995672225952148s\n",
      "131/174, train_loss: 18.7376, time taken: 2.1146912574768066s\n",
      "132/174, train_loss: 16.8630, time taken: 2.2173867225646973s\n",
      "133/174, train_loss: 15.0652, time taken: 1.8072199821472168s\n",
      "134/174, train_loss: 16.2356, time taken: 1.718916893005371s\n",
      "135/174, train_loss: 20.1095, time taken: 1.7075600624084473s\n",
      "136/174, train_loss: 18.8856, time taken: 1.887737512588501s\n",
      "137/174, train_loss: 16.9239, time taken: 2.1119837760925293s\n",
      "138/174, train_loss: 19.5082, time taken: 1.8142144680023193s\n",
      "139/174, train_loss: 15.7987, time taken: 1.7979674339294434s\n",
      "140/174, train_loss: 15.3083, time taken: 1.690812587738037s\n",
      "141/174, train_loss: 19.1514, time taken: 2.2046079635620117s\n",
      "142/174, train_loss: 16.9985, time taken: 1.7949283123016357s\n",
      "143/174, train_loss: 13.9608, time taken: 2.1092100143432617s\n",
      "144/174, train_loss: 16.7020, time taken: 2.0670320987701416s\n",
      "145/174, train_loss: 19.9977, time taken: 1.8271565437316895s\n",
      "146/174, train_loss: 16.1174, time taken: 1.869105339050293s\n",
      "147/174, train_loss: 17.6291, time taken: 1.893791913986206s\n",
      "148/174, train_loss: 16.1448, time taken: 1.821643352508545s\n",
      "149/174, train_loss: 16.6309, time taken: 1.9801506996154785s\n",
      "150/174, train_loss: 16.4055, time taken: 1.7072842121124268s\n",
      "151/174, train_loss: 16.6640, time taken: 2.004129409790039s\n",
      "152/174, train_loss: 16.8223, time taken: 1.807704210281372s\n",
      "153/174, train_loss: 15.5906, time taken: 1.9667956829071045s\n",
      "154/174, train_loss: 13.8234, time taken: 1.7989733219146729s\n",
      "155/174, train_loss: 21.1514, time taken: 1.8140454292297363s\n",
      "156/174, train_loss: 18.3845, time taken: 1.8673374652862549s\n",
      "157/174, train_loss: 17.2714, time taken: 1.8006763458251953s\n",
      "158/174, train_loss: 19.3846, time taken: 1.9120337963104248s\n",
      "159/174, train_loss: 19.5838, time taken: 2.2181763648986816s\n",
      "160/174, train_loss: 16.4408, time taken: 1.7030212879180908s\n",
      "161/174, train_loss: 17.9369, time taken: 1.9005568027496338s\n",
      "162/174, train_loss: 20.7762, time taken: 2.1029410362243652s\n",
      "163/174, train_loss: 16.4399, time taken: 3.529768228530884s\n",
      "164/174, train_loss: 13.9062, time taken: 2.09548020362854s\n",
      "165/174, train_loss: 17.2217, time taken: 1.895860195159912s\n",
      "166/174, train_loss: 16.6059, time taken: 1.8837885856628418s\n",
      "167/174, train_loss: 15.3280, time taken: 1.8962492942810059s\n",
      "168/174, train_loss: 16.7028, time taken: 1.9925308227539062s\n",
      "169/174, train_loss: 21.2273, time taken: 1.8142459392547607s\n",
      "170/174, train_loss: 19.8414, time taken: 1.7966084480285645s\n",
      "171/174, train_loss: 15.8449, time taken: 2.1793525218963623s\n",
      "172/174, train_loss: 16.9306, time taken: 1.9033706188201904s\n",
      "173/174, train_loss: 15.0279, time taken: 1.7784717082977295s\n",
      "174/174, train_loss: 17.0876, time taken: 2.1253600120544434s\n",
      "175/174, train_loss: 20.5108, time taken: 1.4876465797424316s\n",
      "epoch 38 average loss: 17.2132\n",
      "----------\n",
      "epoch 39/2000\n",
      "1/174, train_loss: 17.1776, time taken: 9.080066204071045s\n",
      "2/174, train_loss: 13.5878, time taken: 2.523589611053467s\n",
      "3/174, train_loss: 15.1633, time taken: 2.0682532787323s\n",
      "4/174, train_loss: 20.8653, time taken: 2.126185894012451s\n",
      "5/174, train_loss: 16.9613, time taken: 2.385754108428955s\n",
      "6/174, train_loss: 22.4160, time taken: 2.1220648288726807s\n",
      "7/174, train_loss: 17.4876, time taken: 1.850696086883545s\n",
      "8/174, train_loss: 21.7994, time taken: 2.107396364212036s\n",
      "9/174, train_loss: 15.9610, time taken: 1.9146144390106201s\n",
      "10/174, train_loss: 18.0201, time taken: 1.8676011562347412s\n",
      "11/174, train_loss: 18.0878, time taken: 1.711010456085205s\n",
      "12/174, train_loss: 16.5440, time taken: 1.9305975437164307s\n",
      "13/174, train_loss: 11.9308, time taken: 2.0029027462005615s\n",
      "14/174, train_loss: 15.7748, time taken: 1.896355152130127s\n",
      "15/174, train_loss: 17.9815, time taken: 1.811631441116333s\n",
      "16/174, train_loss: 19.6154, time taken: 2.0834760665893555s\n",
      "17/174, train_loss: 14.5817, time taken: 1.8182690143585205s\n",
      "18/174, train_loss: 18.6727, time taken: 1.906550407409668s\n",
      "19/174, train_loss: 14.9840, time taken: 1.8799524307250977s\n",
      "20/174, train_loss: 18.4182, time taken: 1.8952593803405762s\n",
      "21/174, train_loss: 13.4753, time taken: 1.879622220993042s\n",
      "22/174, train_loss: 17.3098, time taken: 1.9036920070648193s\n",
      "23/174, train_loss: 17.9130, time taken: 1.9031124114990234s\n",
      "24/174, train_loss: 18.1203, time taken: 1.7223165035247803s\n",
      "25/174, train_loss: 17.4734, time taken: 1.8432016372680664s\n",
      "26/174, train_loss: 16.3969, time taken: 2.0233511924743652s\n",
      "27/174, train_loss: 21.0876, time taken: 1.7891271114349365s\n",
      "28/174, train_loss: 15.1677, time taken: 1.8812086582183838s\n",
      "29/174, train_loss: 18.8814, time taken: 1.993598222732544s\n",
      "30/174, train_loss: 15.1702, time taken: 1.8092761039733887s\n",
      "31/174, train_loss: 16.0900, time taken: 1.7074816226959229s\n",
      "32/174, train_loss: 16.4636, time taken: 1.7932379245758057s\n",
      "33/174, train_loss: 16.4086, time taken: 2.0078628063201904s\n",
      "34/174, train_loss: 11.7784, time taken: 1.8787634372711182s\n",
      "35/174, train_loss: 17.8553, time taken: 1.888791561126709s\n",
      "36/174, train_loss: 16.1847, time taken: 1.8140923976898193s\n",
      "37/174, train_loss: 17.5247, time taken: 1.8935747146606445s\n",
      "38/174, train_loss: 19.1460, time taken: 1.8734493255615234s\n",
      "39/174, train_loss: 18.2913, time taken: 1.6194047927856445s\n",
      "40/174, train_loss: 17.8195, time taken: 1.6792943477630615s\n",
      "41/174, train_loss: 12.0114, time taken: 3.4053854942321777s\n",
      "42/174, train_loss: 17.7487, time taken: 1.7826941013336182s\n",
      "43/174, train_loss: 20.9542, time taken: 1.8389372825622559s\n",
      "44/174, train_loss: 18.4529, time taken: 1.6397089958190918s\n",
      "45/174, train_loss: 17.7600, time taken: 1.864542007446289s\n",
      "46/174, train_loss: 17.9663, time taken: 1.7961320877075195s\n",
      "47/174, train_loss: 14.5638, time taken: 1.9100723266601562s\n",
      "48/174, train_loss: 20.5071, time taken: 1.784907579421997s\n",
      "49/174, train_loss: 15.3575, time taken: 1.9270763397216797s\n",
      "50/174, train_loss: 19.3206, time taken: 1.996324062347412s\n",
      "51/174, train_loss: 12.2233, time taken: 1.9760372638702393s\n",
      "52/174, train_loss: 20.5349, time taken: 1.8805770874023438s\n",
      "53/174, train_loss: 19.7796, time taken: 1.7941713333129883s\n",
      "54/174, train_loss: 17.1220, time taken: 1.794816017150879s\n",
      "55/174, train_loss: 19.0822, time taken: 1.6984608173370361s\n",
      "56/174, train_loss: 16.5562, time taken: 1.91841459274292s\n",
      "57/174, train_loss: 16.0962, time taken: 2.307588577270508s\n",
      "58/174, train_loss: 20.3581, time taken: 1.978733777999878s\n",
      "59/174, train_loss: 16.9682, time taken: 1.9174010753631592s\n",
      "60/174, train_loss: 16.5510, time taken: 2.0680196285247803s\n",
      "61/174, train_loss: 13.6956, time taken: 1.9002115726470947s\n",
      "62/174, train_loss: 14.1483, time taken: 1.9070823192596436s\n",
      "63/174, train_loss: 17.5682, time taken: 2.1115527153015137s\n",
      "64/174, train_loss: 15.0911, time taken: 1.8599119186401367s\n",
      "65/174, train_loss: 20.5738, time taken: 1.9242117404937744s\n",
      "66/174, train_loss: 15.9767, time taken: 2.2070398330688477s\n",
      "67/174, train_loss: 15.0762, time taken: 1.9789516925811768s\n",
      "68/174, train_loss: 16.2184, time taken: 1.79093599319458s\n",
      "69/174, train_loss: 15.5914, time taken: 1.8897919654846191s\n",
      "70/174, train_loss: 16.8911, time taken: 1.9001078605651855s\n",
      "71/174, train_loss: 18.7933, time taken: 1.9189598560333252s\n",
      "72/174, train_loss: 15.1851, time taken: 1.77825927734375s\n",
      "73/174, train_loss: 20.0164, time taken: 1.8108184337615967s\n",
      "74/174, train_loss: 19.2722, time taken: 1.8826680183410645s\n",
      "75/174, train_loss: 18.6646, time taken: 1.7987613677978516s\n",
      "76/174, train_loss: 19.3238, time taken: 1.920581340789795s\n",
      "77/174, train_loss: 13.4461, time taken: 1.8126769065856934s\n",
      "78/174, train_loss: 20.1577, time taken: 2.0927202701568604s\n",
      "79/174, train_loss: 18.3685, time taken: 1.8916575908660889s\n",
      "80/174, train_loss: 15.4769, time taken: 1.8189966678619385s\n",
      "81/174, train_loss: 17.8634, time taken: 3.8921351432800293s\n",
      "82/174, train_loss: 18.5379, time taken: 1.820235013961792s\n",
      "83/174, train_loss: 19.5359, time taken: 2.0562167167663574s\n",
      "84/174, train_loss: 19.2902, time taken: 1.7989702224731445s\n",
      "85/174, train_loss: 16.8770, time taken: 2.0245838165283203s\n",
      "86/174, train_loss: 17.4812, time taken: 1.98172926902771s\n",
      "87/174, train_loss: 16.5623, time taken: 1.9750041961669922s\n",
      "88/174, train_loss: 18.0976, time taken: 1.8108971118927002s\n",
      "89/174, train_loss: 16.4460, time taken: 1.824521541595459s\n",
      "90/174, train_loss: 13.2138, time taken: 2.059093475341797s\n",
      "91/174, train_loss: 15.0480, time taken: 2.2974817752838135s\n",
      "92/174, train_loss: 18.1794, time taken: 1.7864725589752197s\n",
      "93/174, train_loss: 16.0910, time taken: 1.8175461292266846s\n",
      "94/174, train_loss: 18.1550, time taken: 1.9240505695343018s\n",
      "95/174, train_loss: 15.4441, time taken: 1.7893831729888916s\n",
      "96/174, train_loss: 19.3278, time taken: 1.7041513919830322s\n",
      "97/174, train_loss: 14.9102, time taken: 1.706843614578247s\n",
      "98/174, train_loss: 19.6531, time taken: 1.812126874923706s\n",
      "99/174, train_loss: 14.7746, time taken: 2.009089946746826s\n",
      "100/174, train_loss: 13.7749, time taken: 1.7682380676269531s\n",
      "101/174, train_loss: 16.9173, time taken: 1.7849805355072021s\n",
      "102/174, train_loss: 19.7689, time taken: 2.1100850105285645s\n",
      "103/174, train_loss: 16.9539, time taken: 1.8247313499450684s\n",
      "104/174, train_loss: 18.5561, time taken: 1.8182096481323242s\n",
      "105/174, train_loss: 18.3211, time taken: 1.699153184890747s\n",
      "106/174, train_loss: 18.2537, time taken: 2.0154693126678467s\n",
      "107/174, train_loss: 15.9754, time taken: 1.7961702346801758s\n",
      "108/174, train_loss: 16.6121, time taken: 1.8720567226409912s\n",
      "109/174, train_loss: 17.4745, time taken: 1.80484938621521s\n",
      "110/174, train_loss: 18.5673, time taken: 1.8017888069152832s\n",
      "111/174, train_loss: 15.0381, time taken: 2.0975823402404785s\n",
      "112/174, train_loss: 18.1767, time taken: 1.8694312572479248s\n",
      "113/174, train_loss: 18.6278, time taken: 1.7368032932281494s\n",
      "114/174, train_loss: 17.3151, time taken: 1.8703081607818604s\n",
      "115/174, train_loss: 18.1414, time taken: 1.6934022903442383s\n",
      "116/174, train_loss: 18.7511, time taken: 1.8001482486724854s\n",
      "117/174, train_loss: 18.4290, time taken: 1.7284090518951416s\n",
      "118/174, train_loss: 14.0949, time taken: 1.7161281108856201s\n",
      "119/174, train_loss: 15.8512, time taken: 3.7015247344970703s\n",
      "120/174, train_loss: 15.1982, time taken: 1.7088253498077393s\n",
      "121/174, train_loss: 13.8267, time taken: 2.2739126682281494s\n",
      "122/174, train_loss: 18.0778, time taken: 1.8877038955688477s\n",
      "123/174, train_loss: 18.6021, time taken: 2.0181901454925537s\n",
      "124/174, train_loss: 21.0377, time taken: 2.1246590614318848s\n",
      "125/174, train_loss: 12.7105, time taken: 1.8358066082000732s\n",
      "126/174, train_loss: 17.0121, time taken: 2.303130626678467s\n",
      "127/174, train_loss: 10.1277, time taken: 1.8420324325561523s\n",
      "128/174, train_loss: 15.8616, time taken: 1.7454183101654053s\n",
      "129/174, train_loss: 16.7652, time taken: 1.9104743003845215s\n",
      "130/174, train_loss: 17.9126, time taken: 1.9253301620483398s\n",
      "131/174, train_loss: 18.7754, time taken: 1.787580966949463s\n",
      "132/174, train_loss: 20.0899, time taken: 1.792837142944336s\n",
      "133/174, train_loss: 15.2976, time taken: 1.8682506084442139s\n",
      "134/174, train_loss: 18.9602, time taken: 2.2322146892547607s\n",
      "135/174, train_loss: 18.7451, time taken: 1.9937326908111572s\n",
      "136/174, train_loss: 19.1811, time taken: 2.0023744106292725s\n",
      "137/174, train_loss: 17.9669, time taken: 1.9024810791015625s\n",
      "138/174, train_loss: 17.5752, time taken: 1.8806612491607666s\n",
      "139/174, train_loss: 17.7798, time taken: 1.9977474212646484s\n",
      "140/174, train_loss: 14.5523, time taken: 2.0907604694366455s\n",
      "141/174, train_loss: 16.2391, time taken: 1.7125093936920166s\n",
      "142/174, train_loss: 19.4236, time taken: 1.8708069324493408s\n",
      "143/174, train_loss: 18.8428, time taken: 1.8150312900543213s\n",
      "144/174, train_loss: 14.9819, time taken: 1.9943277835845947s\n",
      "145/174, train_loss: 17.0318, time taken: 1.894383430480957s\n",
      "146/174, train_loss: 19.1044, time taken: 1.6827270984649658s\n",
      "147/174, train_loss: 16.5243, time taken: 1.9889819622039795s\n",
      "148/174, train_loss: 17.8286, time taken: 1.903350591659546s\n",
      "149/174, train_loss: 17.1933, time taken: 2.003303050994873s\n",
      "150/174, train_loss: 22.1492, time taken: 1.806230068206787s\n",
      "151/174, train_loss: 19.7356, time taken: 1.8122966289520264s\n",
      "152/174, train_loss: 16.2489, time taken: 1.8920941352844238s\n",
      "153/174, train_loss: 15.8370, time taken: 1.9852399826049805s\n",
      "154/174, train_loss: 16.4104, time taken: 1.887483835220337s\n",
      "155/174, train_loss: 12.3149, time taken: 1.8111097812652588s\n",
      "156/174, train_loss: 20.9522, time taken: 1.8816518783569336s\n",
      "157/174, train_loss: 19.3558, time taken: 1.6946189403533936s\n",
      "158/174, train_loss: 15.9132, time taken: 3.801095485687256s\n",
      "159/174, train_loss: 18.5828, time taken: 1.9096622467041016s\n",
      "160/174, train_loss: 14.8586, time taken: 1.796954870223999s\n",
      "161/174, train_loss: 22.2292, time taken: 1.7712624073028564s\n",
      "162/174, train_loss: 22.0876, time taken: 2.1183841228485107s\n",
      "163/174, train_loss: 17.4406, time taken: 1.8872454166412354s\n",
      "164/174, train_loss: 14.7022, time taken: 1.9027669429779053s\n",
      "165/174, train_loss: 14.1360, time taken: 2.0040128231048584s\n",
      "166/174, train_loss: 18.5656, time taken: 1.7859442234039307s\n",
      "167/174, train_loss: 18.8296, time taken: 1.889136791229248s\n",
      "168/174, train_loss: 15.5830, time taken: 2.092360258102417s\n",
      "169/174, train_loss: 13.4256, time taken: 1.7146897315979004s\n",
      "170/174, train_loss: 14.6664, time taken: 1.7726972103118896s\n",
      "171/174, train_loss: 20.0180, time taken: 1.6101751327514648s\n",
      "172/174, train_loss: 15.5190, time taken: 1.628868579864502s\n",
      "173/174, train_loss: 16.8691, time taken: 1.9069883823394775s\n",
      "174/174, train_loss: 21.3314, time taken: 1.8200480937957764s\n",
      "175/174, train_loss: 13.2712, time taken: 1.5686874389648438s\n",
      "epoch 39 average loss: 17.1965\n",
      "Entering Validation for epoch: 39\n",
      "Input shape: torch.Size([4, 1, 64, 64, 64])\n",
      "epoch 39 Validation avg loss: 12.9226, time taken: 1.1080591678619385s\n",
      "----------\n",
      "epoch 40/2000\n",
      "1/174, train_loss: 19.4823, time taken: 8.665230512619019s\n",
      "2/174, train_loss: 18.8319, time taken: 2.362461566925049s\n",
      "3/174, train_loss: 17.4233, time taken: 2.0214991569519043s\n",
      "4/174, train_loss: 19.2841, time taken: 1.9127137660980225s\n",
      "5/174, train_loss: 20.2849, time taken: 1.960434913635254s\n",
      "6/174, train_loss: 14.2682, time taken: 1.88981294631958s\n",
      "7/174, train_loss: 17.1414, time taken: 1.7227730751037598s\n",
      "8/174, train_loss: 15.7912, time taken: 1.8932530879974365s\n",
      "9/174, train_loss: 17.6891, time taken: 1.7864928245544434s\n",
      "10/174, train_loss: 21.4260, time taken: 1.7992548942565918s\n",
      "11/174, train_loss: 19.1797, time taken: 1.802044153213501s\n",
      "12/174, train_loss: 14.2996, time taken: 1.8957929611206055s\n",
      "13/174, train_loss: 16.7088, time taken: 1.7935090065002441s\n",
      "14/174, train_loss: 13.3765, time taken: 1.7150449752807617s\n",
      "15/174, train_loss: 20.6741, time taken: 1.8325984477996826s\n",
      "16/174, train_loss: 16.0633, time taken: 1.8848352432250977s\n",
      "17/174, train_loss: 22.0273, time taken: 2.012538194656372s\n",
      "18/174, train_loss: 14.8908, time taken: 1.9937055110931396s\n",
      "19/174, train_loss: 18.7942, time taken: 2.172028064727783s\n",
      "20/174, train_loss: 17.5404, time taken: 2.8036017417907715s\n",
      "21/174, train_loss: 24.5226, time taken: 1.7219431400299072s\n",
      "22/174, train_loss: 13.6305, time taken: 1.793081521987915s\n",
      "23/174, train_loss: 23.1480, time taken: 3.2010416984558105s\n",
      "24/174, train_loss: 16.8854, time taken: 2.571560859680176s\n",
      "25/174, train_loss: 19.0669, time taken: 2.195948839187622s\n",
      "26/174, train_loss: 13.6423, time taken: 1.9073448181152344s\n",
      "27/174, train_loss: 14.1034, time taken: 1.891843318939209s\n",
      "28/174, train_loss: 20.5566, time taken: 2.0061607360839844s\n",
      "29/174, train_loss: 16.2029, time taken: 1.9760644435882568s\n",
      "30/174, train_loss: 21.3686, time taken: 1.9124290943145752s\n",
      "31/174, train_loss: 16.3192, time taken: 2.1725432872772217s\n",
      "32/174, train_loss: 15.5444, time taken: 1.8271243572235107s\n",
      "33/174, train_loss: 15.6359, time taken: 1.9666345119476318s\n",
      "34/174, train_loss: 19.9972, time taken: 1.9869768619537354s\n",
      "35/174, train_loss: 18.7069, time taken: 1.7159597873687744s\n",
      "36/174, train_loss: 20.2389, time taken: 1.9804589748382568s\n",
      "37/174, train_loss: 20.0158, time taken: 1.8948276042938232s\n",
      "38/174, train_loss: 19.9149, time taken: 2.0376176834106445s\n",
      "39/174, train_loss: 16.6519, time taken: 1.8210694789886475s\n",
      "40/174, train_loss: 15.3721, time taken: 1.7079541683197021s\n",
      "41/174, train_loss: 17.3762, time taken: 1.8912761211395264s\n",
      "42/174, train_loss: 17.6689, time taken: 1.8112094402313232s\n",
      "43/174, train_loss: 15.4886, time taken: 1.8379266262054443s\n",
      "44/174, train_loss: 17.5464, time taken: 1.8815932273864746s\n",
      "45/174, train_loss: 17.2247, time taken: 1.800490379333496s\n",
      "46/174, train_loss: 13.9631, time taken: 1.7791156768798828s\n",
      "47/174, train_loss: 18.5492, time taken: 1.8204641342163086s\n",
      "48/174, train_loss: 17.4758, time taken: 1.9942996501922607s\n",
      "49/174, train_loss: 16.7916, time taken: 1.7130556106567383s\n",
      "50/174, train_loss: 19.5720, time taken: 1.7219276428222656s\n",
      "51/174, train_loss: 17.5474, time taken: 1.8012328147888184s\n",
      "52/174, train_loss: 19.5628, time taken: 2.074932813644409s\n",
      "53/174, train_loss: 16.8669, time taken: 1.8895044326782227s\n",
      "54/174, train_loss: 15.9410, time taken: 1.7936184406280518s\n",
      "55/174, train_loss: 15.0517, time taken: 2.1965887546539307s\n",
      "56/174, train_loss: 20.3967, time taken: 1.909114122390747s\n",
      "57/174, train_loss: 15.6645, time taken: 1.7072176933288574s\n",
      "58/174, train_loss: 18.0857, time taken: 1.8722360134124756s\n",
      "59/174, train_loss: 19.2538, time taken: 1.8339436054229736s\n",
      "60/174, train_loss: 19.1282, time taken: 2.0991382598876953s\n",
      "61/174, train_loss: 19.1187, time taken: 1.9602158069610596s\n",
      "62/174, train_loss: 18.0980, time taken: 2.114169120788574s\n",
      "63/174, train_loss: 14.5133, time taken: 1.9714417457580566s\n",
      "64/174, train_loss: 17.1397, time taken: 3.809648036956787s\n",
      "65/174, train_loss: 18.8229, time taken: 1.9953949451446533s\n",
      "66/174, train_loss: 18.5857, time taken: 1.8136024475097656s\n",
      "67/174, train_loss: 14.7334, time taken: 1.7884104251861572s\n",
      "68/174, train_loss: 19.8194, time taken: 1.9177360534667969s\n",
      "69/174, train_loss: 18.3187, time taken: 1.962275743484497s\n",
      "70/174, train_loss: 15.4339, time taken: 2.006286859512329s\n",
      "71/174, train_loss: 13.3769, time taken: 1.7877635955810547s\n",
      "72/174, train_loss: 16.3537, time taken: 1.9058947563171387s\n",
      "73/174, train_loss: 19.7788, time taken: 2.0222384929656982s\n",
      "74/174, train_loss: 15.7009, time taken: 1.7872545719146729s\n",
      "75/174, train_loss: 15.7375, time taken: 1.8017313480377197s\n",
      "76/174, train_loss: 16.1352, time taken: 1.8848507404327393s\n",
      "77/174, train_loss: 17.4489, time taken: 1.8935232162475586s\n",
      "78/174, train_loss: 15.4443, time taken: 1.884242057800293s\n",
      "79/174, train_loss: 15.6171, time taken: 1.9348640441894531s\n",
      "80/174, train_loss: 15.2379, time taken: 2.1365482807159424s\n",
      "81/174, train_loss: 18.5254, time taken: 1.9772968292236328s\n",
      "82/174, train_loss: 21.0424, time taken: 1.9957382678985596s\n",
      "83/174, train_loss: 20.0473, time taken: 1.9044294357299805s\n",
      "84/174, train_loss: 15.8882, time taken: 1.80910325050354s\n",
      "85/174, train_loss: 13.8675, time taken: 1.785536289215088s\n",
      "86/174, train_loss: 14.5599, time taken: 2.006080389022827s\n",
      "87/174, train_loss: 17.3202, time taken: 1.7666065692901611s\n",
      "88/174, train_loss: 17.9488, time taken: 1.9943768978118896s\n",
      "89/174, train_loss: 22.9107, time taken: 1.7179226875305176s\n",
      "90/174, train_loss: 13.3306, time taken: 1.8131120204925537s\n",
      "91/174, train_loss: 17.7431, time taken: 2.0721476078033447s\n",
      "92/174, train_loss: 10.7991, time taken: 2.103178024291992s\n",
      "93/174, train_loss: 16.6348, time taken: 1.8795878887176514s\n",
      "94/174, train_loss: 16.6983, time taken: 1.8014318943023682s\n",
      "95/174, train_loss: 16.3145, time taken: 1.9017744064331055s\n",
      "96/174, train_loss: 23.6780, time taken: 1.72922945022583s\n",
      "97/174, train_loss: 19.3024, time taken: 1.8033487796783447s\n",
      "98/174, train_loss: 15.4622, time taken: 1.9057235717773438s\n",
      "99/174, train_loss: 15.5054, time taken: 1.8958768844604492s\n",
      "100/174, train_loss: 15.8027, time taken: 1.9977126121520996s\n",
      "101/174, train_loss: 16.3181, time taken: 1.8222332000732422s\n",
      "102/174, train_loss: 14.2221, time taken: 1.8972597122192383s\n",
      "103/174, train_loss: 15.5764, time taken: 1.9749398231506348s\n",
      "104/174, train_loss: 22.2945, time taken: 1.7990443706512451s\n",
      "105/174, train_loss: 15.2959, time taken: 1.7691729068756104s\n",
      "106/174, train_loss: 18.9598, time taken: 1.7143023014068604s\n",
      "107/174, train_loss: 19.6978, time taken: 1.703190565109253s\n",
      "108/174, train_loss: 12.5058, time taken: 1.694014549255371s\n",
      "109/174, train_loss: 20.7841, time taken: 1.8816790580749512s\n",
      "110/174, train_loss: 18.5121, time taken: 1.714341402053833s\n",
      "111/174, train_loss: 19.4793, time taken: 2.978830337524414s\n",
      "112/174, train_loss: 13.7604, time taken: 2.215975522994995s\n",
      "113/174, train_loss: 18.7098, time taken: 1.9111347198486328s\n",
      "114/174, train_loss: 16.4716, time taken: 1.7655396461486816s\n",
      "115/174, train_loss: 16.7378, time taken: 1.986006736755371s\n",
      "116/174, train_loss: 17.4863, time taken: 2.033681631088257s\n",
      "117/174, train_loss: 16.6745, time taken: 1.8925657272338867s\n",
      "118/174, train_loss: 14.2223, time taken: 1.9711081981658936s\n",
      "119/174, train_loss: 20.7595, time taken: 1.9844613075256348s\n",
      "120/174, train_loss: 20.7347, time taken: 2.428004741668701s\n",
      "121/174, train_loss: 15.2670, time taken: 1.892798900604248s\n",
      "122/174, train_loss: 14.8078, time taken: 2.2770018577575684s\n",
      "123/174, train_loss: 22.8714, time taken: 2.1917495727539062s\n",
      "124/174, train_loss: 14.9275, time taken: 2.0035829544067383s\n",
      "125/174, train_loss: 18.9217, time taken: 1.908064365386963s\n",
      "126/174, train_loss: 14.7514, time taken: 1.8963639736175537s\n",
      "127/174, train_loss: 16.5442, time taken: 1.8118677139282227s\n",
      "128/174, train_loss: 18.2114, time taken: 1.782630443572998s\n",
      "129/174, train_loss: 20.8485, time taken: 2.201993227005005s\n",
      "130/174, train_loss: 17.7207, time taken: 2.07855486869812s\n",
      "131/174, train_loss: 15.7485, time taken: 1.8897364139556885s\n",
      "132/174, train_loss: 20.7307, time taken: 1.9972400665283203s\n",
      "133/174, train_loss: 20.7312, time taken: 1.9011313915252686s\n",
      "134/174, train_loss: 15.3388, time taken: 1.7997698783874512s\n",
      "135/174, train_loss: 19.6292, time taken: 1.9839928150177002s\n",
      "136/174, train_loss: 16.0480, time taken: 1.835371732711792s\n",
      "137/174, train_loss: 19.0513, time taken: 1.8617868423461914s\n",
      "138/174, train_loss: 14.7134, time taken: 1.902968406677246s\n",
      "139/174, train_loss: 15.7324, time taken: 2.020001173019409s\n",
      "140/174, train_loss: 18.2568, time taken: 1.8005366325378418s\n",
      "141/174, train_loss: 14.7602, time taken: 1.8792757987976074s\n",
      "142/174, train_loss: 19.2198, time taken: 1.8883051872253418s\n",
      "143/174, train_loss: 18.0960, time taken: 1.897284746170044s\n",
      "144/174, train_loss: 17.8685, time taken: 1.804436445236206s\n",
      "145/174, train_loss: 13.1238, time taken: 1.8071370124816895s\n",
      "146/174, train_loss: 18.6079, time taken: 1.9066803455352783s\n",
      "147/174, train_loss: 17.6974, time taken: 1.829211950302124s\n",
      "148/174, train_loss: 18.0694, time taken: 1.8973181247711182s\n",
      "149/174, train_loss: 20.8895, time taken: 3.700014352798462s\n",
      "150/174, train_loss: 17.1274, time taken: 1.7878999710083008s\n",
      "151/174, train_loss: 17.0317, time taken: 2.275965690612793s\n",
      "152/174, train_loss: 14.6323, time taken: 1.8058831691741943s\n",
      "153/174, train_loss: 18.1734, time taken: 2.3126606941223145s\n",
      "154/174, train_loss: 18.8969, time taken: 2.003692626953125s\n",
      "155/174, train_loss: 16.6772, time taken: 1.9812135696411133s\n",
      "156/174, train_loss: 11.5599, time taken: 1.804236650466919s\n",
      "157/174, train_loss: 16.2992, time taken: 2.1903977394104004s\n",
      "158/174, train_loss: 16.7195, time taken: 1.886551856994629s\n",
      "159/174, train_loss: 15.3109, time taken: 1.9145009517669678s\n",
      "160/174, train_loss: 14.9959, time taken: 1.9716014862060547s\n",
      "161/174, train_loss: 18.2897, time taken: 1.9018263816833496s\n",
      "162/174, train_loss: 15.3082, time taken: 2.1274936199188232s\n",
      "163/174, train_loss: 13.6421, time taken: 1.8796412944793701s\n",
      "164/174, train_loss: 17.5089, time taken: 1.9960048198699951s\n",
      "165/174, train_loss: 14.9246, time taken: 1.799010992050171s\n",
      "166/174, train_loss: 15.9328, time taken: 1.8054003715515137s\n",
      "167/174, train_loss: 18.2433, time taken: 1.9915642738342285s\n",
      "168/174, train_loss: 22.1231, time taken: 1.7081754207611084s\n",
      "169/174, train_loss: 12.1518, time taken: 1.8674910068511963s\n",
      "170/174, train_loss: 16.9405, time taken: 1.9001412391662598s\n",
      "171/174, train_loss: 17.7289, time taken: 1.8864357471466064s\n",
      "172/174, train_loss: 14.1439, time taken: 1.8099896907806396s\n",
      "173/174, train_loss: 14.0911, time taken: 1.8906216621398926s\n",
      "174/174, train_loss: 15.7783, time taken: 1.715611219406128s\n",
      "175/174, train_loss: 17.2334, time taken: 1.4882218837738037s\n",
      "epoch 40 average loss: 17.2829\n",
      "----------\n",
      "epoch 41/2000\n",
      "1/174, train_loss: 17.1444, time taken: 8.386213779449463s\n",
      "2/174, train_loss: 17.3258, time taken: 2.507305145263672s\n",
      "3/174, train_loss: 18.2740, time taken: 1.9689581394195557s\n",
      "4/174, train_loss: 16.5448, time taken: 1.9090514183044434s\n",
      "5/174, train_loss: 14.8116, time taken: 1.7830381393432617s\n",
      "6/174, train_loss: 17.9174, time taken: 1.9085462093353271s\n",
      "7/174, train_loss: 18.0695, time taken: 1.717620849609375s\n",
      "8/174, train_loss: 18.3025, time taken: 1.7810719013214111s\n",
      "9/174, train_loss: 17.3359, time taken: 1.8284912109375s\n",
      "10/174, train_loss: 14.3418, time taken: 1.7143852710723877s\n",
      "11/174, train_loss: 15.6648, time taken: 1.916168212890625s\n",
      "12/174, train_loss: 19.8057, time taken: 1.8053228855133057s\n",
      "13/174, train_loss: 15.6843, time taken: 1.9558141231536865s\n",
      "14/174, train_loss: 13.1351, time taken: 1.7062392234802246s\n",
      "15/174, train_loss: 16.7005, time taken: 1.8057184219360352s\n",
      "16/174, train_loss: 19.3152, time taken: 1.7978756427764893s\n",
      "17/174, train_loss: 18.1021, time taken: 1.8957016468048096s\n",
      "18/174, train_loss: 15.3707, time taken: 1.8108766078948975s\n",
      "19/174, train_loss: 18.8245, time taken: 1.8014209270477295s\n",
      "20/174, train_loss: 16.7226, time taken: 2.2611141204833984s\n",
      "21/174, train_loss: 18.5365, time taken: 1.9244053363800049s\n",
      "22/174, train_loss: 17.8893, time taken: 1.8711323738098145s\n",
      "23/174, train_loss: 18.4359, time taken: 2.1024837493896484s\n",
      "24/174, train_loss: 16.5909, time taken: 1.9129581451416016s\n",
      "25/174, train_loss: 15.2745, time taken: 2.072451591491699s\n",
      "26/174, train_loss: 18.6455, time taken: 1.719834327697754s\n",
      "27/174, train_loss: 15.6208, time taken: 1.8679938316345215s\n",
      "28/174, train_loss: 15.9706, time taken: 1.8305895328521729s\n",
      "29/174, train_loss: 17.3642, time taken: 1.8943686485290527s\n",
      "30/174, train_loss: 14.0623, time taken: 2.495084762573242s\n",
      "31/174, train_loss: 18.5851, time taken: 2.1941895484924316s\n",
      "32/174, train_loss: 14.2128, time taken: 1.8614790439605713s\n",
      "33/174, train_loss: 21.2860, time taken: 2.133833646774292s\n",
      "34/174, train_loss: 13.5687, time taken: 2.1888692378997803s\n",
      "35/174, train_loss: 15.2623, time taken: 1.9931721687316895s\n",
      "36/174, train_loss: 18.4935, time taken: 1.893392562866211s\n",
      "37/174, train_loss: 16.5396, time taken: 2.288789749145508s\n",
      "38/174, train_loss: 14.8868, time taken: 1.8240704536437988s\n",
      "39/174, train_loss: 17.9338, time taken: 2.1549479961395264s\n",
      "40/174, train_loss: 15.0658, time taken: 2.0024712085723877s\n",
      "41/174, train_loss: 14.0910, time taken: 1.90085768699646s\n",
      "42/174, train_loss: 15.8632, time taken: 1.6319689750671387s\n",
      "43/174, train_loss: 15.2454, time taken: 1.7689592838287354s\n",
      "44/174, train_loss: 15.1566, time taken: 1.8183636665344238s\n",
      "45/174, train_loss: 16.5004, time taken: 1.968388319015503s\n",
      "46/174, train_loss: 16.8189, time taken: 1.8081908226013184s\n",
      "47/174, train_loss: 18.4926, time taken: 1.8915002346038818s\n",
      "48/174, train_loss: 19.4026, time taken: 1.890699863433838s\n",
      "49/174, train_loss: 16.8786, time taken: 1.804870843887329s\n",
      "50/174, train_loss: 16.4839, time taken: 1.7214791774749756s\n",
      "51/174, train_loss: 15.0444, time taken: 1.8144245147705078s\n",
      "52/174, train_loss: 18.0046, time taken: 1.8097002506256104s\n",
      "53/174, train_loss: 16.1689, time taken: 1.9762847423553467s\n",
      "54/174, train_loss: 14.0997, time taken: 2.0081851482391357s\n",
      "55/174, train_loss: 11.0788, time taken: 2.1002585887908936s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_cl_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        inputs, inputs_2, gt_input = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"image_2\"].to(device),\n",
    "            batch_data[\"gt_image\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs_v1, hidden_v1 = model(inputs)\n",
    "        outputs_v2, hidden_v2 = model(inputs_2)\n",
    "\n",
    "        flat_out_v1 = outputs_v1.flatten(start_dim=1, end_dim=4)\n",
    "        flat_out_v2 = outputs_v2.flatten(start_dim=1, end_dim=4)\n",
    "\n",
    "        r_loss = recon_loss(outputs_v1, gt_input)\n",
    "        cl_loss = contrastive_loss(flat_out_v1, flat_out_v2)\n",
    "\n",
    "        # Adjust the CL loss by Recon Loss\n",
    "        total_loss = r_loss + cl_loss * r_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += total_loss.item()\n",
    "        step_loss_values.append(total_loss.item())\n",
    "\n",
    "        # CL & Recon Loss Storage of Value\n",
    "        epoch_cl_loss += cl_loss.item()\n",
    "        epoch_recon_loss += r_loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {total_loss.item():.4f}, \"\n",
    "            f\"time taken: {end_time-start_time}s\"\n",
    "        )\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_cl_loss /= step\n",
    "    epoch_recon_loss /= step\n",
    "\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    epoch_cl_loss_values.append(epoch_cl_loss)\n",
    "    epoch_recon_loss_values.append(epoch_recon_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if epoch % val_interval == 0:\n",
    "        print(\"Entering Validation for epoch: {}\".format(epoch + 1))\n",
    "        total_val_loss = 0\n",
    "        val_step = 0\n",
    "        model.eval()\n",
    "        for val_batch in val_loader:\n",
    "            val_step += 1\n",
    "            start_time = time.time()\n",
    "            inputs, gt_input = (\n",
    "                val_batch[\"image\"].to(device),\n",
    "                val_batch[\"gt_image\"].to(device),\n",
    "            )\n",
    "            print(\"Input shape: {}\".format(inputs.shape))\n",
    "            outputs, outputs_v2 = model(inputs)\n",
    "            val_loss = recon_loss(outputs, gt_input)\n",
    "            total_val_loss += val_loss.item()\n",
    "            end_time = time.time()\n",
    "\n",
    "        total_val_loss /= val_step\n",
    "        val_loss_values.append(total_val_loss)\n",
    "        print(\n",
    "            f\"epoch {epoch + 1} Validation avg loss: {total_val_loss:.4f}, \" f\"time taken: {end_time-start_time}s\")\n",
    "\n",
    "        if total_val_loss < best_val_loss:\n",
    "            print(\n",
    "                f\"Saving new model based on validation loss {total_val_loss:.4f}\")\n",
    "            best_val_loss = total_val_loss\n",
    "            checkpoint = {\"epoch\": max_epochs, \"state_dict\": model.state_dict(\n",
    "            ), \"optimizer\": optimizer.state_dict()}\n",
    "            torch.save(checkpoint, os.path.join(logdir_path, experiment_name + \".pth\"))\n",
    "\n",
    "        plt.figure(1, figsize=(8, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(epoch_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(val_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Validation Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(epoch_cl_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Contrastive Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(epoch_recon_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Recon Loss\")\n",
    "\n",
    "        plt.savefig(os.path.join(logdir_path, experiment_name + \".png\"))\n",
    "        plt.close(1)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSLUnet",
   "language": "python",
   "name": "sslunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
