{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.0\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.2.2+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
      "MONAI __file__: /home/<username>/.conda/envs/unetSSL/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: 1.12.0\n",
      "Pillow version: 10.3.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.66.2\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.8\n",
      "pandas version: 2.2.1\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import L1Loss\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.networks.nets import ViTAutoEnc\n",
    "from monai.losses import ContrastiveLoss\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    CopyItemsd,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Spacingd,\n",
    "    OneOf,\n",
    "    ScaleIntensityRanged,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCoarseDropoutd,\n",
    "    RandCoarseShuffled,\n",
    ")\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': './Synth3DValTrain/im0.nii.gz', 'label': './Synth3DValTrain/seg0.nii.gz'}, {'image': './Synth3DValTrain/im1.nii.gz', 'label': './Synth3DValTrain/seg1.nii.gz'}, {'image': './Synth3DValTrain/im2.nii.gz', 'label': './Synth3DValTrain/seg2.nii.gz'}, {'image': './Synth3DValTrain/im3.nii.gz', 'label': './Synth3DValTrain/seg3.nii.gz'}, {'image': './Synth3DValTrain/im4.nii.gz', 'label': './Synth3DValTrain/seg4.nii.gz'}, {'image': './Synth3DValTrain/im5.nii.gz', 'label': './Synth3DValTrain/seg5.nii.gz'}, {'image': './Synth3DValTrain/im6.nii.gz', 'label': './Synth3DValTrain/seg6.nii.gz'}, {'image': './Synth3DValTrain/im7.nii.gz', 'label': './Synth3DValTrain/seg7.nii.gz'}, {'image': './Synth3DValTrain/im8.nii.gz', 'label': './Synth3DValTrain/seg8.nii.gz'}, {'image': './Synth3DValTrain/im9.nii.gz', 'label': './Synth3DValTrain/seg9.nii.gz'}] [{'image': './Synth3DValTrain/im0.nii.gz', 'label': './Synth3DValTrain/seg0.nii.gz'}, {'image': './Synth3DValTrain/im1.nii.gz', 'label': './Synth3DValTrain/seg1.nii.gz'}, {'image': './Synth3DValTrain/im2.nii.gz', 'label': './Synth3DValTrain/seg2.nii.gz'}, {'image': './Synth3DValTrain/im3.nii.gz', 'label': './Synth3DValTrain/seg3.nii.gz'}, {'image': './Synth3DValTrain/im4.nii.gz', 'label': './Synth3DValTrain/seg4.nii.gz'}, {'image': './Synth3DValTrain/im5.nii.gz', 'label': './Synth3DValTrain/seg5.nii.gz'}, {'image': './Synth3DValTrain/im6.nii.gz', 'label': './Synth3DValTrain/seg6.nii.gz'}, {'image': './Synth3DValTrain/im7.nii.gz', 'label': './Synth3DValTrain/seg7.nii.gz'}, {'image': './Synth3DValTrain/im8.nii.gz', 'label': './Synth3DValTrain/seg8.nii.gz'}, {'image': './Synth3DValTrain/im9.nii.gz', 'label': './Synth3DValTrain/seg9.nii.gz'}]\n"
     ]
    }
   ],
   "source": [
    "logdir_path = os.path.normpath(\"./logs/\")\n",
    "\n",
    "#Convert the train and validation images into a list with locations\n",
    "train_dir = \"./Synth3DTrain\"\n",
    "val_dir = \"./Synth3DValTrain\"\n",
    "\n",
    "#train image file\n",
    "timage_filenames = sorted([os.path.join(train_dir, f) for f in os.listdir(train_dir) if f.startswith(\"im\")])\n",
    "tlabel_filenames = sorted([os.path.join(train_dir, f) for f in os.listdir(train_dir) if f.startswith(\"seg\")])\n",
    "\n",
    "#validation image files\n",
    "vimage_filenames = sorted([os.path.join(val_dir, f) for f in os.listdir(val_dir) if f.startswith(\"im\")])\n",
    "vlabel_filenames = sorted([os.path.join(val_dir, f) for f in os.listdir(val_dir) if f.startswith(\"seg\")])\n",
    "\n",
    "# Create a list of dictionaries containing the file paths\n",
    "train_datalist = [{\"image\": img, \"label\": lbl} for img, lbl in zip(vimage_filenames, vlabel_filenames)]\n",
    "validation_datalist = [{\"image\": img, \"label\": lbl} for img, lbl in zip(vimage_filenames, vlabel_filenames)]\n",
    "\n",
    "# Print the datalist to verify\n",
    "print(train_datalist, validation_datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([32, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erattakulangara/.conda/envs/unetSSL/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "# Define Training Transforms\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        # Spacingd(keys=[\"image\"], pixdim=(2.0, 2.0, 2.0), mode=(\"bilinear\")),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-157,\n",
    "            a_max=256,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "        SpatialPadd(keys=[\"image\"], spatial_size=(32, 32, 32)),\n",
    "        RandSpatialCropSamplesd(keys=[\"image\"], roi_size=(\n",
    "            32, 32, 32), random_size=False, num_samples=2),\n",
    "        CopyItemsd(keys=[\"image\"], times=2, names=[\n",
    "                   \"gt_image\", \"image_2\"], allow_missing_keys=False),\n",
    "        OneOf(\n",
    "            transforms=[\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=6, spatial_size=5, dropout_holes=True, max_spatial_size=32\n",
    "                ),\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=6, spatial_size=20, dropout_holes=False, max_spatial_size=64\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        RandCoarseShuffled(keys=[\"image\"], prob=0.8, holes=10, spatial_size=8),\n",
    "        # Please note that that if image, image_2 are called via the same transform call because of the determinism\n",
    "        # they will get augmented the exact same way which is not the required case here, hence two calls are made\n",
    "        OneOf(\n",
    "            transforms=[\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image_2\"], prob=1.0, holes=6, spatial_size=5, dropout_holes=True, max_spatial_size=32\n",
    "                ),\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image_2\"], prob=1.0, holes=6, spatial_size=20, dropout_holes=False, max_spatial_size=64\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        RandCoarseShuffled(keys=[\"image_2\"], prob=0.8,\n",
    "                           holes=10, spatial_size=8),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "check_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image = check_data[\"image\"][0][0]\n",
    "print(f\"image shape: {image.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config\n",
    "\n",
    "# Define Network ViT backbone & Loss & Optimizer\n",
    "device = torch.device(\"cpu\")\n",
    "model = ViTAutoEnc(\n",
    "    in_channels=1,\n",
    "    img_size=(32, 32, 32),\n",
    "    patch_size=(16, 16, 16),\n",
    "    proj_type=\"conv\",\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Hyper-paramters for training loop\n",
    "max_epochs = 500\n",
    "val_interval = 2\n",
    "batch_size = 4\n",
    "lr = 1e-4\n",
    "epoch_loss_values = []\n",
    "step_loss_values = []\n",
    "epoch_cl_loss_values = []\n",
    "epoch_recon_loss_values = []\n",
    "val_loss_values = []\n",
    "best_val_loss = 1000.0\n",
    "\n",
    "recon_loss = L1Loss()\n",
    "contrastive_loss = ContrastiveLoss(temperature=0.05)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Define DataLoader using MONAI, CacheDataset needs to be used\n",
    "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = Dataset(data=validation_datalist, transform=train_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/500\n",
      "1/2, train_loss: 3.1335, time taken: 3.6087486743927s\n",
      "2/2, train_loss: 2.4974, time taken: 1.179062843322754s\n",
      "3/2, train_loss: 1.9204, time taken: 0.964601993560791s\n",
      "epoch 1 average loss: 2.5171\n",
      "Entering Validation for epoch: 1\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 1 Validation avg loss: 0.7492, time taken: 0.4864940643310547s\n",
      "Saving new model based on validation loss 0.7492\n",
      "----------\n",
      "epoch 2/500\n",
      "1/2, train_loss: 2.3736, time taken: 13.566534996032715s\n",
      "2/2, train_loss: 2.8517, time taken: 2.8003063201904297s\n",
      "3/2, train_loss: 2.2988, time taken: 1.5801873207092285s\n",
      "epoch 2 average loss: 2.5080\n",
      "----------\n",
      "epoch 3/500\n",
      "1/2, train_loss: 2.7715, time taken: 9.56297516822815s\n",
      "2/2, train_loss: 2.7706, time taken: 1.468001365661621s\n",
      "3/2, train_loss: 1.9593, time taken: 1.4327049255371094s\n",
      "epoch 3 average loss: 2.5005\n",
      "Entering Validation for epoch: 3\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 3 Validation avg loss: 0.7539, time taken: 0.3864130973815918s\n",
      "----------\n",
      "epoch 4/500\n",
      "1/2, train_loss: 2.5121, time taken: 10.62238073348999s\n",
      "2/2, train_loss: 2.6993, time taken: 3.55513596534729s\n",
      "3/2, train_loss: 2.1791, time taken: 1.2989475727081299s\n",
      "epoch 4 average loss: 2.4635\n",
      "----------\n",
      "epoch 5/500\n",
      "1/2, train_loss: 2.6965, time taken: 10.0161874294281s\n",
      "2/2, train_loss: 2.4654, time taken: 1.626746416091919s\n",
      "3/2, train_loss: 2.2336, time taken: 1.3791980743408203s\n",
      "epoch 5 average loss: 2.4651\n",
      "Entering Validation for epoch: 5\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 5 Validation avg loss: 0.6921, time taken: 0.29292893409729004s\n",
      "Saving new model based on validation loss 0.6921\n",
      "----------\n",
      "epoch 6/500\n",
      "1/2, train_loss: 2.5532, time taken: 9.86319375038147s\n",
      "2/2, train_loss: 2.3375, time taken: 1.4807391166687012s\n",
      "3/2, train_loss: 1.9844, time taken: 1.3496809005737305s\n",
      "epoch 6 average loss: 2.2917\n",
      "----------\n",
      "epoch 7/500\n",
      "1/2, train_loss: 2.2163, time taken: 10.153630495071411s\n",
      "2/2, train_loss: 2.6008, time taken: 1.6601507663726807s\n",
      "3/2, train_loss: 1.8876, time taken: 1.3021008968353271s\n",
      "epoch 7 average loss: 2.2349\n",
      "Entering Validation for epoch: 7\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 7 Validation avg loss: 0.6320, time taken: 0.3753640651702881s\n",
      "Saving new model based on validation loss 0.6320\n",
      "----------\n",
      "epoch 8/500\n",
      "1/2, train_loss: 2.5030, time taken: 9.7730073928833s\n",
      "2/2, train_loss: 2.5047, time taken: 1.5506815910339355s\n",
      "3/2, train_loss: 1.8686, time taken: 1.5089349746704102s\n",
      "epoch 8 average loss: 2.2921\n",
      "----------\n",
      "epoch 9/500\n",
      "1/2, train_loss: 2.2970, time taken: 10.571315288543701s\n",
      "2/2, train_loss: 2.3733, time taken: 2.1406850814819336s\n",
      "3/2, train_loss: 1.8888, time taken: 1.4179425239562988s\n",
      "epoch 9 average loss: 2.1864\n",
      "Entering Validation for epoch: 9\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 9 Validation avg loss: 0.6263, time taken: 0.4589362144470215s\n",
      "Saving new model based on validation loss 0.6263\n",
      "----------\n",
      "epoch 10/500\n",
      "1/2, train_loss: 2.2265, time taken: 10.604245662689209s\n",
      "2/2, train_loss: 2.3423, time taken: 4.9903953075408936s\n",
      "3/2, train_loss: 1.8350, time taken: 1.6678009033203125s\n",
      "epoch 10 average loss: 2.1346\n",
      "----------\n",
      "epoch 11/500\n",
      "1/2, train_loss: 2.0747, time taken: 10.852741479873657s\n",
      "2/2, train_loss: 2.2782, time taken: 4.879258632659912s\n",
      "3/2, train_loss: 1.8794, time taken: 1.3123455047607422s\n",
      "epoch 11 average loss: 2.0774\n",
      "Entering Validation for epoch: 11\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 11 Validation avg loss: 0.5864, time taken: 0.4053812026977539s\n",
      "Saving new model based on validation loss 0.5864\n",
      "----------\n",
      "epoch 12/500\n",
      "1/2, train_loss: 2.0912, time taken: 9.911598443984985s\n",
      "2/2, train_loss: 2.1900, time taken: 1.5263075828552246s\n",
      "3/2, train_loss: 1.6692, time taken: 1.4724578857421875s\n",
      "epoch 12 average loss: 1.9835\n",
      "----------\n",
      "epoch 13/500\n",
      "1/2, train_loss: 1.9417, time taken: 10.175806999206543s\n",
      "2/2, train_loss: 2.2439, time taken: 2.3859238624572754s\n",
      "3/2, train_loss: 1.6629, time taken: 1.3718078136444092s\n",
      "epoch 13 average loss: 1.9495\n",
      "Entering Validation for epoch: 13\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 13 Validation avg loss: 0.5539, time taken: 0.47428345680236816s\n",
      "Saving new model based on validation loss 0.5539\n",
      "----------\n",
      "epoch 14/500\n",
      "1/2, train_loss: 2.0944, time taken: 10.556824684143066s\n",
      "2/2, train_loss: 2.0689, time taken: 1.7854382991790771s\n",
      "3/2, train_loss: 1.4302, time taken: 1.784555196762085s\n",
      "epoch 14 average loss: 1.8645\n",
      "----------\n",
      "epoch 15/500\n",
      "1/2, train_loss: 1.8622, time taken: 10.2850980758667s\n",
      "2/2, train_loss: 1.9776, time taken: 1.5979905128479004s\n",
      "3/2, train_loss: 1.6329, time taken: 1.2886924743652344s\n",
      "epoch 15 average loss: 1.8242\n",
      "Entering Validation for epoch: 15\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 15 Validation avg loss: 0.5255, time taken: 0.5448155403137207s\n",
      "Saving new model based on validation loss 0.5255\n",
      "----------\n",
      "epoch 16/500\n",
      "1/2, train_loss: 1.8866, time taken: 10.202551364898682s\n",
      "2/2, train_loss: 2.0211, time taken: 1.5227265357971191s\n",
      "3/2, train_loss: 1.4118, time taken: 1.3414185047149658s\n",
      "epoch 16 average loss: 1.7732\n",
      "----------\n",
      "epoch 17/500\n",
      "1/2, train_loss: 1.7590, time taken: 10.649348974227905s\n",
      "2/2, train_loss: 1.9957, time taken: 1.7884142398834229s\n",
      "3/2, train_loss: 1.5179, time taken: 1.5191264152526855s\n",
      "epoch 17 average loss: 1.7575\n",
      "Entering Validation for epoch: 17\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 17 Validation avg loss: 0.5073, time taken: 0.5618062019348145s\n",
      "Saving new model based on validation loss 0.5073\n",
      "----------\n",
      "epoch 18/500\n",
      "1/2, train_loss: 1.7456, time taken: 10.527908563613892s\n",
      "2/2, train_loss: 1.9343, time taken: 1.783315896987915s\n",
      "3/2, train_loss: 1.4159, time taken: 1.404388666152954s\n",
      "epoch 18 average loss: 1.6986\n",
      "----------\n",
      "epoch 19/500\n",
      "1/2, train_loss: 1.7011, time taken: 10.487518787384033s\n",
      "2/2, train_loss: 1.8406, time taken: 1.8120670318603516s\n",
      "3/2, train_loss: 1.5434, time taken: 1.406067132949829s\n",
      "epoch 19 average loss: 1.6950\n",
      "Entering Validation for epoch: 19\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 19 Validation avg loss: 0.4839, time taken: 0.3915836811065674s\n",
      "Saving new model based on validation loss 0.4839\n",
      "----------\n",
      "epoch 20/500\n",
      "1/2, train_loss: 1.8489, time taken: 10.488097906112671s\n",
      "2/2, train_loss: 1.6666, time taken: 1.5277090072631836s\n",
      "3/2, train_loss: 1.4504, time taken: 1.4951207637786865s\n",
      "epoch 20 average loss: 1.6553\n",
      "----------\n",
      "epoch 21/500\n",
      "1/2, train_loss: 1.7487, time taken: 11.104011297225952s\n",
      "2/2, train_loss: 1.7524, time taken: 1.7122471332550049s\n",
      "3/2, train_loss: 1.2695, time taken: 1.294600486755371s\n",
      "epoch 21 average loss: 1.5902\n",
      "Entering Validation for epoch: 21\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 21 Validation avg loss: 0.4544, time taken: 0.3825099468231201s\n",
      "Saving new model based on validation loss 0.4544\n",
      "----------\n",
      "epoch 22/500\n",
      "1/2, train_loss: 1.5373, time taken: 10.178855895996094s\n",
      "2/2, train_loss: 1.7819, time taken: 1.9521355628967285s\n",
      "3/2, train_loss: 1.3926, time taken: 1.3806419372558594s\n",
      "epoch 22 average loss: 1.5706\n",
      "----------\n",
      "epoch 23/500\n",
      "1/2, train_loss: 1.5622, time taken: 10.13349199295044s\n",
      "2/2, train_loss: 1.6862, time taken: 1.55527663230896s\n",
      "3/2, train_loss: 1.3723, time taken: 1.396662712097168s\n",
      "epoch 23 average loss: 1.5402\n",
      "Entering Validation for epoch: 23\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 23 Validation avg loss: 0.4393, time taken: 0.5516915321350098s\n",
      "Saving new model based on validation loss 0.4393\n",
      "----------\n",
      "epoch 24/500\n",
      "1/2, train_loss: 1.6435, time taken: 10.438752889633179s\n",
      "2/2, train_loss: 1.5206, time taken: 1.632143259048462s\n",
      "3/2, train_loss: 1.3484, time taken: 1.5636515617370605s\n",
      "epoch 24 average loss: 1.5042\n",
      "----------\n",
      "epoch 25/500\n",
      "1/2, train_loss: 1.5850, time taken: 10.587425947189331s\n",
      "2/2, train_loss: 1.5141, time taken: 5.21036171913147s\n",
      "3/2, train_loss: 1.3071, time taken: 1.499302864074707s\n",
      "epoch 25 average loss: 1.4687\n",
      "Entering Validation for epoch: 25\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 25 Validation avg loss: 0.4219, time taken: 0.5615692138671875s\n",
      "Saving new model based on validation loss 0.4219\n",
      "----------\n",
      "epoch 26/500\n",
      "1/2, train_loss: 1.5527, time taken: 10.448431730270386s\n",
      "2/2, train_loss: 1.5084, time taken: 2.6510746479034424s\n",
      "3/2, train_loss: 1.2918, time taken: 1.528761386871338s\n",
      "epoch 26 average loss: 1.4510\n",
      "----------\n",
      "epoch 27/500\n",
      "1/2, train_loss: 1.4959, time taken: 10.62601900100708s\n",
      "2/2, train_loss: 1.5487, time taken: 2.802450180053711s\n",
      "3/2, train_loss: 1.1287, time taken: 1.40191650390625s\n",
      "epoch 27 average loss: 1.3911\n",
      "Entering Validation for epoch: 27\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 27 Validation avg loss: 0.4134, time taken: 0.628293514251709s\n",
      "Saving new model based on validation loss 0.4134\n",
      "----------\n",
      "epoch 28/500\n",
      "1/2, train_loss: 1.3987, time taken: 10.289355278015137s\n",
      "2/2, train_loss: 1.5546, time taken: 1.6479370594024658s\n",
      "3/2, train_loss: 1.1868, time taken: 1.415628433227539s\n",
      "epoch 28 average loss: 1.3800\n",
      "----------\n",
      "epoch 29/500\n",
      "1/2, train_loss: 1.4844, time taken: 10.392976522445679s\n",
      "2/2, train_loss: 1.4967, time taken: 1.7828211784362793s\n",
      "3/2, train_loss: 1.1126, time taken: 1.4376046657562256s\n",
      "epoch 29 average loss: 1.3645\n",
      "Entering Validation for epoch: 29\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 29 Validation avg loss: 0.3865, time taken: 0.559887170791626s\n",
      "Saving new model based on validation loss 0.3865\n",
      "----------\n",
      "epoch 30/500\n",
      "1/2, train_loss: 1.4255, time taken: 10.20534896850586s\n",
      "2/2, train_loss: 1.4611, time taken: 1.5146734714508057s\n",
      "3/2, train_loss: 1.0406, time taken: 1.852823257446289s\n",
      "epoch 30 average loss: 1.3091\n",
      "----------\n",
      "epoch 31/500\n",
      "1/2, train_loss: 1.4382, time taken: 10.193684339523315s\n",
      "2/2, train_loss: 1.4133, time taken: 1.5592541694641113s\n",
      "3/2, train_loss: 1.1489, time taken: 1.2281427383422852s\n",
      "epoch 31 average loss: 1.3335\n",
      "Entering Validation for epoch: 31\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([8, 1, 32, 32, 32])\n",
      "Input shape: torch.Size([4, 1, 32, 32, 32])\n",
      "epoch 31 Validation avg loss: 0.3650, time taken: 0.5214748382568359s\n",
      "Saving new model based on validation loss 0.3650\n",
      "----------\n",
      "epoch 32/500\n",
      "1/2, train_loss: 1.4514, time taken: 10.555242776870728s\n",
      "2/2, train_loss: 1.2997, time taken: 1.9937183856964111s\n",
      "3/2, train_loss: 1.1194, time taken: 1.316807508468628s\n",
      "epoch 32 average loss: 1.2902\n",
      "----------\n",
      "epoch 33/500\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_cl_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        inputs, inputs_2, gt_input = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"image_2\"].to(device),\n",
    "            batch_data[\"gt_image\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs_v1, hidden_v1 = model(inputs)\n",
    "        outputs_v2, hidden_v2 = model(inputs_2)\n",
    "\n",
    "        flat_out_v1 = outputs_v1.flatten(start_dim=1, end_dim=4)\n",
    "        flat_out_v2 = outputs_v2.flatten(start_dim=1, end_dim=4)\n",
    "\n",
    "        r_loss = recon_loss(outputs_v1, gt_input)\n",
    "        cl_loss = contrastive_loss(flat_out_v1, flat_out_v2)\n",
    "\n",
    "        # Adjust the CL loss by Recon Loss\n",
    "        total_loss = r_loss + cl_loss * r_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += total_loss.item()\n",
    "        step_loss_values.append(total_loss.item())\n",
    "\n",
    "        # CL & Recon Loss Storage of Value\n",
    "        epoch_cl_loss += cl_loss.item()\n",
    "        epoch_recon_loss += r_loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {total_loss.item():.4f}, \"\n",
    "            f\"time taken: {end_time-start_time}s\"\n",
    "        )\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_cl_loss /= step\n",
    "    epoch_recon_loss /= step\n",
    "\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    epoch_cl_loss_values.append(epoch_cl_loss)\n",
    "    epoch_recon_loss_values.append(epoch_recon_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if epoch % val_interval == 0:\n",
    "        print(\"Entering Validation for epoch: {}\".format(epoch + 1))\n",
    "        total_val_loss = 0\n",
    "        val_step = 0\n",
    "        model.eval()\n",
    "        for val_batch in val_loader:\n",
    "            val_step += 1\n",
    "            start_time = time.time()\n",
    "            inputs, gt_input = (\n",
    "                val_batch[\"image\"].to(device),\n",
    "                val_batch[\"gt_image\"].to(device),\n",
    "            )\n",
    "            print(\"Input shape: {}\".format(inputs.shape))\n",
    "            outputs, outputs_v2 = model(inputs)\n",
    "            val_loss = recon_loss(outputs, gt_input)\n",
    "            total_val_loss += val_loss.item()\n",
    "            end_time = time.time()\n",
    "\n",
    "        total_val_loss /= val_step\n",
    "        val_loss_values.append(total_val_loss)\n",
    "        print(\n",
    "            f\"epoch {epoch + 1} Validation avg loss: {total_val_loss:.4f}, \" f\"time taken: {end_time-start_time}s\")\n",
    "\n",
    "        if total_val_loss < best_val_loss:\n",
    "            print(\n",
    "                f\"Saving new model based on validation loss {total_val_loss:.4f}\")\n",
    "            best_val_loss = total_val_loss\n",
    "            checkpoint = {\"epoch\": max_epochs, \"state_dict\": model.state_dict(\n",
    "            ), \"optimizer\": optimizer.state_dict()}\n",
    "            torch.save(checkpoint, os.path.join(logdir_path, \"best_model_Synth_500.pt\"))\n",
    "\n",
    "        plt.figure(1, figsize=(8, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(epoch_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(val_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Validation Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(epoch_cl_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Contrastive Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(epoch_recon_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Recon Loss\")\n",
    "\n",
    "        plt.savefig(os.path.join(logdir_path, \"loss_plots.png\"))\n",
    "        plt.close(1)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
